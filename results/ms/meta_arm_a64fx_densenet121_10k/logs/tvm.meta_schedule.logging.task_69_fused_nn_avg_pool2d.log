2024-04-28 20:48:05 [INFO] [task_scheduler.cc:160] Initializing Task #69: "fused_nn_avg_pool2d"
2024-04-28 20:48:05 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(32), T.int64(56), T.int64(56), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)))
        for ax0, ax1, ax2, ax3, ax4, rv0, rv1 in T.grid(T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4), T.int64(2), T.int64(2)):
            with T.block("pool_sum"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, v_rv0, v_rv1 = T.axis.remap("SSSSSRR", [ax0, ax1, ax2, ax3, ax4, rv0, rv1])
                T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1, v_ax4])
                T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                with T.init():
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1, v_ax4]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)):
            with T.block("pool_avg"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T.block_attr({"schedule_rule": "meta_schedule.pool_avg"})
                pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
2024-04-28 20:48:05 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-28 20:48:05 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(32), T.int64(56), T.int64(56), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 64, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)))
            pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4), T.int64(1)))
            for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(28), T.int64(28)):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4, ax5, ax6 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("pool_sum_rf"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("SS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(32), ax1 + ax2_1)
                        v_ax2 = T.axis.spatial(T.int64(28), ax2 + ax3_1)
                        v_ax3 = T.axis.spatial(T.int64(28), ax3 + ax4)
                        v_ax4, vrv0_rv1_fused_1 = T.axis.remap("SR", [ax5, ax6])
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) % T.int64(2), v_ax4])
                        T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        with T.init():
                            pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = T.float32(0)
                        pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) % T.int64(2), v_ax4]
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4, ax5 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                    with T.block("pool_sum"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("RS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(32), ax1 + ax2_1)
                        v_ax2 = T.axis.spatial(T.int64(28), ax2 + ax3_1)
                        v_ax3 = T.axis.spatial(T.int64(28), ax3 + ax4)
                        v_ax4 = T.axis.spatial(T.int64(4), ax5)
                        T.reads(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0]
                for ax4 in range(T.int64(4)):
                    with T.block("pool_avg"):
                        v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 4])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=3)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
2024-04-28 20:48:06 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(32), T.int64(56), T.int64(56), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 512, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)))
            pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4), T.int64(4)))
            for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(28), T.int64(28)):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4, ax5, ax6 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                    with T.block("pool_sum_rf"):
                        vrv0_rv1_fused_1, v_ax0 = T.axis.remap("SS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(32), ax1 + ax2_1)
                        v_ax2 = T.axis.spatial(T.int64(28), ax2 + ax3_1)
                        v_ax3 = T.axis.spatial(T.int64(28), ax3 + ax4)
                        v_ax4, vrv0_rv1_fused_0 = T.axis.remap("SR", [ax5, ax6])
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) % T.int64(2), v_ax4])
                        T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                        with T.init():
                            pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = T.float32(0)
                        pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) % T.int64(2), v_ax4]
                for ax4, rv0_rv1_fused_1 in T.grid(T.int64(4), T.int64(4)):
                    with T.block("pool_sum"):
                        vrv0_rv1_fused_1, v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("RSSSSS", [rv0_rv1_fused_1, ax0, ax1, ax2, ax3, ax4])
                        T.reads(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)):
                with T.block("pool_avg"):
                    v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 4])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=3)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
2024-04-28 20:48:06 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(32), T.int64(56), T.int64(56), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 64, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)))
            for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(28), T.int64(28)):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4, ax5, ax6 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(2)):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0_1)
                        v_ax1 = T.axis.spatial(T.int64(32), ax1 + ax1_1)
                        v_ax2 = T.axis.spatial(T.int64(28), ax2 + ax2_1)
                        v_ax3 = T.axis.spatial(T.int64(28), ax3 + ax3_1)
                        v_ax4, v_rv0, v_rv1 = T.axis.remap("SRR", [ax4, ax5, ax6])
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1, v_ax4])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1, v_ax4]
                for ax4 in range(T.int64(4)):
                    with T.block("pool_avg"):
                        v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l4, preserve_unit_loops=True, index=-1)
2024-04-28 22:29:43 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-28 22:29:43 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-28 22:29:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x6f8cda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xb6ab1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x70d7218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x6b797e8)]: 0 failure(s)
2024-04-28 22:29:46 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-04-28 22:29:49 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x6f8cda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xb6ab1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x70d7218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x6b797e8)]: 0 failure(s)
2024-04-28 22:29:53 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x6f8cda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xb6ab1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x70d7218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x6b797e8)]: 0 failure(s)
2024-04-28 22:29:57 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x6f8cda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xb6ab1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x70d7218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x6b797e8)]: 0 failure(s)
2024-04-28 22:30:01 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x6f8cda8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xb6ab1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x70d7218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x6b797e8)]: 0 failure(s)
2024-04-28 22:30:03 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9988  0.9966  0.9937  0.9931  0.9926  0.9894  0.9844  0.9839  0.9760  0.9754  0.9735  0.9726  0.9716  0.9708  0.9688  0.9673
[17 : 32]:	0.9652  0.9647  0.9642  0.9619  0.9594  0.9573  0.9567  0.9566  0.9563  0.9545  0.9536  0.9519  0.9517  0.9479  0.9422  0.9382
[33 : 48]:	0.9377  0.9305  0.9292  0.9285  0.9285  0.9283  0.9254  0.9244  0.9209  0.9153  0.9144  0.9131  0.9054  0.9032  0.8999  0.8984
[49 : 64]:	0.8976  0.8954  0.8944  0.8931  0.8922  0.8882  0.8859  0.8849  0.8825  0.8808  0.8803  0.8764  0.8730  0.8729  0.8721  0.8714
2024-04-28 22:30:03 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-28 22:30:03 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #1: GFLOPs: 25.5954. Time: 98.0175 us. Best GFLOPs: 25.5954
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #2: GFLOPs: 115.7396. Time: 21.6762 us. Best GFLOPs: 115.7396
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #3: GFLOPs: 51.5095. Time: 48.7055 us. Best GFLOPs: 115.7396
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #4: GFLOPs: 51.4382. Time: 48.7731 us. Best GFLOPs: 115.7396
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #5: GFLOPs: 76.6416. Time: 32.7342 us. Best GFLOPs: 115.7396
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #6: GFLOPs: 34.0496. Time: 73.6809 us. Best GFLOPs: 115.7396
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #7: GFLOPs: 39.3246. Time: 63.7973 us. Best GFLOPs: 115.7396
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #8: GFLOPs: 103.9776. Time: 24.1283 us. Best GFLOPs: 115.7396
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:121] [Task #69: fused_nn_avg_pool2d] Trial #9: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(32), T.int64(56), T.int64(56), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)))
        pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4), T.int64(1)))
        for ax0_ax1_fused in T.parallel(T.int64(32), annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax3_init, ax4_init, ax0_init, ax1_init, ax2_init, ax3_init_1, ax4_init_1, ax5_init in T.grid(T.int64(28), T.int64(28), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                with T.block("pool_sum_rf_init"):
                    vrv0_rv1_fused_1, v_ax0 = T.axis.remap("SS", [ax0_init, ax1_init])
                    v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused + ax2_init)
                    v_ax2 = T.axis.spatial(T.int64(28), ax3_init + ax3_init_1)
                    v_ax3 = T.axis.spatial(T.int64(28), ax4_init + ax4_init_1)
                    v_ax4 = T.axis.spatial(T.int64(4), ax5_init)
                    T.reads()
                    T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                    pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = T.float32(0)
            for ax1_init, ax2_init, ax3_init, ax4_init in T.grid(T.int64(1), T.int64(1), T.int64(28), T.int64(28)):
                for ax5_fused_init in T.vectorized(T.int64(4)):
                    with T.block("pool_sum_init"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax1_init)
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused + ax2_init)
                        v_ax2, v_ax3, v_ax4 = T.axis.remap("SSS", [ax3_init, ax4_init, ax5_fused_init])
                        T.reads()
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
            for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(28), T.int64(28)):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, ax5, ax6 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("pool_sum_rf_update"):
                        vrv0_rv1_fused_1, v_ax0 = T.axis.remap("SS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused + ax2_1)
                        v_ax2 = T.axis.spatial(T.int64(28), ax3 + ax3_1)
                        v_ax3 = T.axis.spatial(T.int64(28), ax4 + ax4_1)
                        v_ax4, vrv0_rv1_fused_0 = T.axis.remap("SR", [ax5, ax6])
                        T.reads(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) % T.int64(2), v_ax4])
                        T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                        pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) % T.int64(2), v_ax4]
                for ax5_fused in T.vectorized(T.int64(4)):
                    with T.block("pool_sum_update"):
                        vrv0_rv1_fused_1, v_ax0 = T.axis.remap("RS", [ax0, ax1])
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused + ax2)
                        v_ax2, v_ax3, v_ax4 = T.axis.remap("SSS", [ax3, ax4, ax5_fused])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1]
            for ax2, ax3 in T.grid(T.int64(28), T.int64(28)):
                for ax4_fused in T.vectorized(T.int64(4)):
                    with T.block("pool_avg"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSS", [ax0_ax1_fused, ax2, ax3, ax4_fused])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[4, 1])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=6)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
sch.enter_postproc()
b20 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b20, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b20, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b20, ann_key="meta_schedule.unroll_explicit")
b21, b22, b23 = sch.get_child_blocks(b20)
l24, l25, l26, l27, l28, l29, l30, l31, l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b21)
l38 = sch.fuse(l24, l25, preserve_unit_iters=True)
sch.parallel(loop=l38)
sch.annotate(block_or_loop=l38, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l38, ann_key="pragma_unroll_explicit", ann_val=1)
l39, l40, l41, l42, l43, l44, l45 = sch.get_loops(block=b22)
l46 = sch.fuse(l45, preserve_unit_iters=True)
sch.vectorize(loop=l46)
sch.annotate(block_or_loop=l39, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l39, ann_key="pragma_unroll_explicit", ann_val=1)
l47, l48, l49, l50 = sch.get_loops(block=b23)
l51 = sch.fuse(l50, preserve_unit_iters=True)
sch.vectorize(loop=l51)
b52 = sch.get_block(name="pool_sum_rf", func_name="main")
l53, l54, l55, l56, l57, l58, l59, l60, l61, l62, l63, l64, l65 = sch.get_loops(block=b52)
b66 = sch.decompose_reduction(block=b52, loop=l54)
b67 = sch.get_block(name="pool_sum", func_name="main")
l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
b75 = sch.decompose_reduction(block=b67, loop=l69)
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #10: GFLOPs: 172.3109. Time: 14.5597 us. Best GFLOPs: 172.3109
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #11: GFLOPs: 37.3487. Time: 67.1724 us. Best GFLOPs: 172.3109
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #12: GFLOPs: 51.8629. Time: 48.3737 us. Best GFLOPs: 172.3109
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #13: GFLOPs: 147.8601. Time: 16.9674 us. Best GFLOPs: 172.3109
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #14: GFLOPs: 38.8164. Time: 64.6325 us. Best GFLOPs: 172.3109
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #15: GFLOPs: 186.1565. Time: 13.4768 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #16: GFLOPs: 52.9264. Time: 47.4016 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #17: GFLOPs: 33.1788. Time: 75.6144 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #18: GFLOPs: 141.1665. Time: 17.7719 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #19: GFLOPs: 38.9636. Time: 64.3883 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #20: GFLOPs: 156.9561. Time: 15.9841 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #21: GFLOPs: 51.3017. Time: 48.9028 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #22: GFLOPs: 46.0165. Time: 54.5196 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #23: GFLOPs: 40.0519. Time: 62.6387 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #24: GFLOPs: 76.8132. Time: 32.6610 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #25: GFLOPs: 96.1801. Time: 26.0844 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #26: GFLOPs: 57.0457. Time: 43.9788 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #27: GFLOPs: 138.4498. Time: 18.1206 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #28: GFLOPs: 53.0395. Time: 47.3006 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #29: GFLOPs: 171.3402. Time: 14.6422 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #30: GFLOPs: 92.8069. Time: 27.0325 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #31: GFLOPs: 54.0782. Time: 46.3921 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #32: GFLOPs: 106.4465. Time: 23.5686 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #33: GFLOPs: 20.6189. Time: 121.6745 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #34: GFLOPs: 116.4281. Time: 21.5481 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #35: GFLOPs: 51.0163. Time: 49.1765 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #36: GFLOPs: 175.3451. Time: 14.3078 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #37: GFLOPs: 32.6985. Time: 76.7252 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #38: GFLOPs: 36.5364. Time: 68.6658 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #39: GFLOPs: 88.1320. Time: 28.4664 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:121] [Task #69: fused_nn_avg_pool2d] Trial #40: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(32), T.int64(56), T.int64(56), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4)))
        pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28), T.int64(4), T.int64(1)))
        for ax0_ax1_fused in T.parallel(T.int64(32), annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax3_init, ax4_init, ax0_init, ax1_init, ax2_init, ax3_init_1, ax4_init_1, ax5_init in T.grid(T.int64(28), T.int64(28), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                with T.block("pool_sum_rf_init"):
                    vrv0_rv1_fused_0, v_ax0 = T.axis.remap("SS", [ax0_init, ax1_init])
                    v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused + ax2_init)
                    v_ax2 = T.axis.spatial(T.int64(28), ax3_init + ax3_init_1)
                    v_ax3 = T.axis.spatial(T.int64(28), ax4_init + ax4_init_1)
                    v_ax4 = T.axis.spatial(T.int64(4), ax5_init)
                    T.reads()
                    T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                    pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = T.float32(0)
            for ax1_init, ax2_init, ax3_init, ax4_init in T.grid(T.int64(1), T.int64(1), T.int64(28), T.int64(28)):
                for ax5_fused_init in T.vectorized(T.int64(4)):
                    with T.block("pool_sum_init"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax1_init)
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused + ax2_init)
                        v_ax2, v_ax3, v_ax4 = T.axis.remap("SSS", [ax3_init, ax4_init, ax5_fused_init])
                        T.reads()
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
            for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(28), T.int64(28)):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, ax5, ax6 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("pool_sum_rf_update"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("SS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused + ax2_1)
                        v_ax2 = T.axis.spatial(T.int64(28), ax3 + ax3_1)
                        v_ax3 = T.axis.spatial(T.int64(28), ax4 + ax4_1)
                        v_ax4, vrv0_rv1_fused_1 = T.axis.remap("SR", [ax5, ax6])
                        T.reads(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) % T.int64(2), v_ax4])
                        T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(4) + vrv0_rv1_fused_1) % T.int64(2), v_ax4]
                for ax5_fused in T.vectorized(T.int64(4)):
                    with T.block("pool_sum_update"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("RS", [ax0, ax1])
                        v_ax1 = T.axis.spatial(T.int64(32), ax0_ax1_fused + ax2)
                        v_ax2, v_ax3, v_ax4 = T.axis.remap("SSS", [ax3, ax4, ax5_fused])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4], pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0]
            for ax2, ax3 in T.grid(T.int64(28), T.int64(28)):
                for ax4_fused in T.vectorized(T.int64(4)):
                    with T.block("pool_avg"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSS", [ax0_ax1_fused, ax2, ax3, ax4_fused])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(55)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 4])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=6)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
sch.enter_postproc()
b20 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b20, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b20, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b20, ann_key="meta_schedule.unroll_explicit")
b21, b22, b23 = sch.get_child_blocks(b20)
l24, l25, l26, l27, l28, l29, l30, l31, l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b21)
l38 = sch.fuse(l24, l25, preserve_unit_iters=True)
sch.parallel(loop=l38)
sch.annotate(block_or_loop=l38, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l38, ann_key="pragma_unroll_explicit", ann_val=1)
l39, l40, l41, l42, l43, l44, l45 = sch.get_loops(block=b22)
l46 = sch.fuse(l45, preserve_unit_iters=True)
sch.vectorize(loop=l46)
sch.annotate(block_or_loop=l39, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l39, ann_key="pragma_unroll_explicit", ann_val=1)
l47, l48, l49, l50 = sch.get_loops(block=b23)
l51 = sch.fuse(l50, preserve_unit_iters=True)
sch.vectorize(loop=l51)
b52 = sch.get_block(name="pool_sum_rf", func_name="main")
l53, l54, l55, l56, l57, l58, l59, l60, l61, l62, l63, l64, l65 = sch.get_loops(block=b52)
b66 = sch.decompose_reduction(block=b52, loop=l54)
b67 = sch.get_block(name="pool_sum", func_name="main")
l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
b75 = sch.decompose_reduction(block=b67, loop=l69)
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #41: GFLOPs: 176.4296. Time: 14.2198 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #42: GFLOPs: 45.6756. Time: 54.9265 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #43: GFLOPs: 58.3932. Time: 42.9639 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #44: GFLOPs: 128.6356. Time: 19.5032 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #45: GFLOPs: 131.8448. Time: 19.0284 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #46: GFLOPs: 56.5307. Time: 44.3795 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #47: GFLOPs: 172.4217. Time: 14.5504 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #48: GFLOPs: 56.9448. Time: 44.0567 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #49: GFLOPs: 174.8113. Time: 14.3515 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #50: GFLOPs: 66.9550. Time: 37.4700 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #51: GFLOPs: 136.5982. Time: 18.3663 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #52: GFLOPs: 18.5923. Time: 134.9379 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #53: GFLOPs: 30.1392. Time: 83.2405 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #54: GFLOPs: 65.0306. Time: 38.5787 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #55: GFLOPs: 121.7016. Time: 20.6143 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #56: GFLOPs: 66.5167. Time: 37.7169 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #57: GFLOPs: 172.6014. Time: 14.5352 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #58: GFLOPs: 52.1487. Time: 48.1086 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #59: GFLOPs: 132.1437. Time: 18.9854 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #60: GFLOPs: 82.4194. Time: 30.4394 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #61: GFLOPs: 53.9149. Time: 46.5326 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #62: GFLOPs: 162.0083. Time: 15.4856 us. Best GFLOPs: 186.1565
2024-04-28 22:48:10 [INFO] [task_scheduler.cc:131] [Task #69: fused_nn_avg_pool2d] Trial #63: GFLOPs: 37.0444. Time: 67.7241 us. Best GFLOPs: 186.1565
