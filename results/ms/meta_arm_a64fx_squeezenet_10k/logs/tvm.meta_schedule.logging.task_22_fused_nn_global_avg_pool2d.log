2024-04-29 01:55:28 [INFO] [task_scheduler.cc:160] Initializing Task #22: "fused_nn_global_avg_pool2d"
2024-04-29 01:55:28 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(250), T.int64(13), T.int64(13), T.int64(4)), "float32"), adaptive_pool_avg: T.Buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        adaptive_pool_sum = T.alloc_buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)))
        for ax0, ax1, ax2, ax3, ax4, rv0, rv1 in T.grid(T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4), T.int64(13), T.int64(13)):
            with T.block("adaptive_pool_sum"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, v_rv0, v_rv1 = T.axis.remap("SSSSSRR", [ax0, ax1, ax2, ax3, ax4, rv0, rv1])
                T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(13) + v_rv0, v_ax3 * T.int64(13) + v_rv1, v_ax4])
                T.writes(adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                with T.init():
                    adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + p0[v_ax0, v_ax1, v_ax2 * T.int64(13) + v_rv0, v_ax3 * T.int64(13) + v_rv1, v_ax4]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)):
            with T.block("adaptive_pool_avg"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T.writes(adaptive_pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T.block_attr({"schedule_rule": "meta_schedule.adaptive_pool_avg"})
                adaptive_pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] * T.float32(0.0059171597633136093)
2024-04-29 01:55:28 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-29 01:55:28 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(250), T.int64(13), T.int64(13), T.int64(4)), "float32"), adaptive_pool_avg: T.Buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 512, "meta_schedule.vectorize": 64})
            adaptive_pool_sum = T.alloc_buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)))
            adaptive_pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4), T.int64(169)))
            for ax0, ax1, ax2, ax3, ax4, rv0_rv1_fused_0, rv0_rv1_fused_1 in T.grid(T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4), T.int64(169), T.int64(1)):
                with T.block("adaptive_pool_sum_rf"):
                    vrv0_rv1_fused_0, v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1 = T.axis.remap("SSSSSSR", [rv0_rv1_fused_0, ax0, ax1, ax2, ax3, ax4, rv0_rv1_fused_1])
                    T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(13) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) // T.int64(13), v_ax3 * T.int64(13) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) % T.int64(13), v_ax4])
                    T.writes(adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                    with T.init():
                        adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = T.float32(0)
                    adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] + p0[v_ax0, v_ax1, v_ax2 * T.int64(13) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) // T.int64(13), v_ax3 * T.int64(13) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) % T.int64(13), v_ax4]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(250)):
                for ax0_1, ax1_1, ax2, ax3, ax4, ax5 in T.grid(T.int64(169), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                    with T.block("adaptive_pool_sum"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("RS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(250), ax1 + ax2)
                        v_ax2, v_ax3, v_ax4 = T.axis.remap("SSS", [ax3, ax4, ax5])
                        T.reads(adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        T.writes(adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0]
                for ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                    with T.block("adaptive_pool_avg"):
                        v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(adaptive_pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        adaptive_pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] * T.float32(0.0059171597633136093)
b0 = sch.get_block(name="adaptive_pool_sum", func_name="main")
b1 = sch.get_block(name="adaptive_pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[169, 1])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=-1)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
2024-04-29 01:55:28 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(250), T.int64(13), T.int64(13), T.int64(4)), "float32"), adaptive_pool_avg: T.Buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 0, "meta_schedule.vectorize": 64})
            adaptive_pool_sum = T.alloc_buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)))
            adaptive_pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4), T.int64(1)))
            for ax0, ax1, ax2, ax3, ax4, rv0_rv1_fused_0, rv0_rv1_fused_1 in T.grid(T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4), T.int64(169), T.int64(1)):
                with T.block("adaptive_pool_sum_rf"):
                    vrv0_rv1_fused_1, v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0 = T.axis.remap("SSSSSSR", [rv0_rv1_fused_1, ax0, ax1, ax2, ax3, ax4, rv0_rv1_fused_0])
                    T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(13) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) // T.int64(13), v_ax3 * T.int64(13) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) % T.int64(13), v_ax4])
                    T.writes(adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                    with T.init():
                        adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = T.float32(0)
                    adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] + p0[v_ax0, v_ax1, v_ax2 * T.int64(13) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) // T.int64(13), v_ax3 * T.int64(13) + (vrv0_rv1_fused_0 + vrv0_rv1_fused_1) % T.int64(13), v_ax4]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(250)):
                for ax0_1, ax1_1, ax2, ax3, ax4, ax5 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                    with T.block("adaptive_pool_sum"):
                        vrv0_rv1_fused_1, v_ax0 = T.axis.remap("RS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(250), ax1 + ax2)
                        v_ax2, v_ax3, v_ax4 = T.axis.remap("SSS", [ax3, ax4, ax5])
                        T.reads(adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                        T.writes(adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + adaptive_pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1]
                for ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                    with T.block("adaptive_pool_avg"):
                        v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(adaptive_pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        adaptive_pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] * T.float32(0.0059171597633136093)
b0 = sch.get_block(name="adaptive_pool_sum", func_name="main")
b1 = sch.get_block(name="adaptive_pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[169, 1])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=-1)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
2024-04-29 01:55:28 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(250), T.int64(13), T.int64(13), T.int64(4)), "float32"), adaptive_pool_avg: T.Buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 16, "meta_schedule.vectorize": 64})
            adaptive_pool_sum = T.alloc_buffer((T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)))
            for ax0, ax1, ax2, ax3, ax4, rv0, rv1 in T.grid(T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4), T.int64(13), T.int64(13)):
                with T.block("adaptive_pool_sum"):
                    v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, v_rv0, v_rv1 = T.axis.remap("SSSSSRR", [ax0, ax1, ax2, ax3, ax4, rv0, rv1])
                    T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(13) + v_rv0, v_ax3 * T.int64(13) + v_rv1, v_ax4])
                    T.writes(adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    with T.init():
                        adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                    adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + p0[v_ax0, v_ax1, v_ax2 * T.int64(13) + v_rv0, v_ax3 * T.int64(13) + v_rv1, v_ax4]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(250), T.int64(1), T.int64(1), T.int64(4)):
                with T.block("adaptive_pool_avg"):
                    v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                    T.reads(adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    T.writes(adaptive_pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    adaptive_pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = adaptive_pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] * T.float32(0.0059171597633136093)
b0 = sch.get_block(name="adaptive_pool_sum", func_name="main")
b1 = sch.get_block(name="adaptive_pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l4, preserve_unit_loops=True, index=-1)
2024-04-29 02:29:17 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 02:29:17 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-29 02:29:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 02:29:19 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-04-29 02:29:21 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 02:29:24 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 02:29:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 02:29:29 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 02:29:30 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9990  0.9978  0.9922  0.9915  0.9908  0.9899  0.9873  0.9830  0.9729  0.9715  0.9670  0.9667  0.9646  0.9595  0.9581  0.9581
[17 : 32]:	0.9552  0.9448  0.9394  0.9334  0.9296  0.9289  0.9171  0.9113  0.9063  0.9028  0.8911  0.8839  0.8713  0.8661  0.8617  0.8612
[33 : 48]:	0.8600  0.8510  0.8466  0.8426  0.8421  0.8398  0.8310  0.8303  0.8263  0.8236  0.8151  0.8148  0.8087  0.8066  0.8058  0.7915
[49 : 64]:	0.7855  0.7828  0.7823  0.7778  0.7752  0.7716  0.7688  0.7662  0.7586  0.7531  0.7486  0.7466  0.7421  0.7409  0.7403  0.7403
2024-04-29 02:29:30 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 02:29:30 [INFO] [evolutionary_search.cc:730] Sending 62 candidates(s) for measurement
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #1: GFLOPs: 2.8972. Time: 58.6774 us. Best GFLOPs: 2.8972
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #2: GFLOPs: 3.3626. Time: 50.5562 us. Best GFLOPs: 3.3626
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #3: GFLOPs: 4.4016. Time: 38.6221 us. Best GFLOPs: 4.4016
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #4: GFLOPs: 5.5252. Time: 30.7679 us. Best GFLOPs: 5.5252
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #5: GFLOPs: 4.6806. Time: 36.3202 us. Best GFLOPs: 5.5252
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #6: GFLOPs: 11.4922. Time: 14.7927 us. Best GFLOPs: 11.4922
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #7: GFLOPs: 11.7003. Time: 14.5295 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #8: GFLOPs: 6.5207. Time: 26.0709 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #9: GFLOPs: 4.5160. Time: 37.6440 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #10: GFLOPs: 7.8106. Time: 21.7652 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #11: GFLOPs: 5.8005. Time: 29.3076 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #12: GFLOPs: 6.1440. Time: 27.6693 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #13: GFLOPs: 4.2682. Time: 39.8294 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #14: GFLOPs: 4.9498. Time: 34.3448 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #15: GFLOPs: 6.5156. Time: 26.0912 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #16: GFLOPs: 2.5100. Time: 67.7291 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #17: GFLOPs: 3.0248. Time: 56.2016 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #18: GFLOPs: 6.6294. Time: 25.6434 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #19: GFLOPs: 6.1391. Time: 27.6911 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #20: GFLOPs: 8.5001. Time: 19.9998 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #21: GFLOPs: 4.2599. Time: 39.9068 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #22: GFLOPs: 3.2505. Time: 52.2995 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #23: GFLOPs: 4.3013. Time: 39.5226 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #24: GFLOPs: 2.9010. Time: 58.6012 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #25: GFLOPs: 10.5715. Time: 16.0809 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #26: GFLOPs: 3.0929. Time: 54.9638 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #27: GFLOPs: 1.8876. Time: 90.0593 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #28: GFLOPs: 6.4688. Time: 26.2801 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #29: GFLOPs: 6.4044. Time: 26.5444 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #30: GFLOPs: 4.2561. Time: 39.9429 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #31: GFLOPs: 8.1189. Time: 20.9388 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #32: GFLOPs: 6.4280. Time: 26.4467 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #33: GFLOPs: 3.7538. Time: 45.2875 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #34: GFLOPs: 10.4896. Time: 16.2065 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #35: GFLOPs: 1.8950. Time: 89.7112 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #36: GFLOPs: 5.0754. Time: 33.4948 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #37: GFLOPs: 4.0195. Time: 42.2937 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #38: GFLOPs: 3.4671. Time: 49.0317 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #39: GFLOPs: 5.9826. Time: 28.4156 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #40: GFLOPs: 11.3744. Time: 14.9459 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #41: GFLOPs: 2.6155. Time: 64.9977 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #42: GFLOPs: 5.4842. Time: 30.9982 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #43: GFLOPs: 4.2397. Time: 40.0970 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #44: GFLOPs: 10.2042. Time: 16.6598 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #45: GFLOPs: 6.1832. Time: 27.4940 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #46: GFLOPs: 3.9598. Time: 42.9316 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #47: GFLOPs: 4.2688. Time: 39.8235 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #48: GFLOPs: 6.2487. Time: 27.2057 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #49: GFLOPs: 10.9393. Time: 15.5402 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #50: GFLOPs: 6.0042. Time: 28.3136 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #51: GFLOPs: 5.9657. Time: 28.4961 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #52: GFLOPs: 2.5675. Time: 66.2124 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #53: GFLOPs: 4.7907. Time: 35.4854 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #54: GFLOPs: 4.0768. Time: 41.6989 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #55: GFLOPs: 4.3657. Time: 38.9399 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #56: GFLOPs: 1.8995. Time: 89.4956 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #57: GFLOPs: 6.3252. Time: 26.8768 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #58: GFLOPs: 7.4823. Time: 22.7201 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #59: GFLOPs: 7.0505. Time: 24.1118 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #60: GFLOPs: 11.0006. Time: 15.4538 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #61: GFLOPs: 6.0047. Time: 28.3112 us. Best GFLOPs: 11.7003
2024-04-29 02:32:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #62: GFLOPs: 6.3774. Time: 26.6568 us. Best GFLOPs: 11.7003
2024-04-29 06:54:31 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 06:54:31 [INFO] [evolutionary_search.cc:715] Picked top 62 candidate(s) from database
2024-04-29 06:54:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 06:54:32 [INFO] [evolutionary_search.cc:723] Sampled 450 candidate(s)
2024-04-29 06:54:36 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 06:54:41 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 06:54:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 06:54:50 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x39c6778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x3c3ced8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x3261608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0xb109cb8)]: 0 failure(s)
2024-04-29 06:54:52 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0555  1.0296  0.9925  0.9586  0.9503  0.9495  0.9084  0.8842  0.8827  0.8784  0.8532  0.8470  0.8309  0.7979  0.7956  0.7943
[17 : 32]:	0.7629  0.7623  0.7321  0.7303  0.7142  0.7023  0.7017  0.7005  0.6988  0.6948  0.6936  0.6915  0.6828  0.6761  0.6687  0.6596
[33 : 48]:	0.6502  0.6459  0.6233  0.6219  0.5956  0.5795  0.5788  0.5776  0.5746  0.5728  0.5626  0.5557  0.5477  0.5402  0.5393  0.5389
[49 : 64]:	0.5334  0.5291  0.5283  0.5226  0.5215  0.5178  0.5162  0.5090  0.5082  0.5068  0.5051  0.4946  0.4932  0.4815  0.4765  0.4718
2024-04-29 06:54:53 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 06:54:53 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #63: GFLOPs: 5.2476. Time: 32.3956 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #64: GFLOPs: 4.7095. Time: 36.0973 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #65: GFLOPs: 4.4928. Time: 37.8384 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #66: GFLOPs: 11.2765. Time: 15.0756 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #67: GFLOPs: 3.5888. Time: 47.3696 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #68: GFLOPs: 10.9835. Time: 15.4778 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #69: GFLOPs: 10.2175. Time: 16.6381 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #70: GFLOPs: 10.5760. Time: 16.0742 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #71: GFLOPs: 8.6816. Time: 19.5816 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #72: GFLOPs: 9.5197. Time: 17.8577 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #73: GFLOPs: 11.6567. Time: 14.5839 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #74: GFLOPs: 11.1431. Time: 15.2561 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #75: GFLOPs: 10.3953. Time: 16.3535 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #76: GFLOPs: 9.9013. Time: 17.1695 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #77: GFLOPs: 11.1422. Time: 15.2573 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #78: GFLOPs: 11.0607. Time: 15.3697 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #79: GFLOPs: 11.3533. Time: 14.9736 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #80: GFLOPs: 11.3427. Time: 14.9876 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #81: GFLOPs: 10.8054. Time: 15.7328 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #82: GFLOPs: 9.1303. Time: 18.6194 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #83: GFLOPs: 10.7809. Time: 15.7686 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #84: GFLOPs: 7.1073. Time: 23.9191 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #85: GFLOPs: 10.5410. Time: 16.1275 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #86: GFLOPs: 8.7072. Time: 19.5240 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #87: GFLOPs: 9.8694. Time: 17.2250 us. Best GFLOPs: 11.7003
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #88: GFLOPs: 11.7896. Time: 14.4195 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #89: GFLOPs: 6.0905. Time: 27.9122 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #90: GFLOPs: 11.4189. Time: 14.8876 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #91: GFLOPs: 11.1532. Time: 15.2423 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #92: GFLOPs: 10.4013. Time: 16.3441 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #93: GFLOPs: 8.0144. Time: 21.2119 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #94: GFLOPs: 6.9223. Time: 24.5584 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #95: GFLOPs: 4.1402. Time: 41.0607 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #96: GFLOPs: 7.8761. Time: 21.5844 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #97: GFLOPs: 5.9190. Time: 28.7212 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #98: GFLOPs: 10.3261. Time: 16.4632 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #99: GFLOPs: 6.0547. Time: 28.0775 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #100: GFLOPs: 6.5623. Time: 25.9056 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #101: GFLOPs: 2.1768. Time: 78.0971 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #102: GFLOPs: 3.8180. Time: 44.5263 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #103: GFLOPs: 11.7527. Time: 14.4648 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #104: GFLOPs: 5.6072. Time: 30.3180 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #105: GFLOPs: 11.0813. Time: 15.3412 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #106: GFLOPs: 5.7088. Time: 29.7787 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #107: GFLOPs: 5.4302. Time: 31.3066 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #108: GFLOPs: 6.5175. Time: 26.0837 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #109: GFLOPs: 5.6551. Time: 30.0612 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #110: GFLOPs: 6.0751. Time: 27.9830 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #111: GFLOPs: 5.6838. Time: 29.9097 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #112: GFLOPs: 7.5064. Time: 22.6472 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #113: GFLOPs: 5.4248. Time: 31.3378 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #114: GFLOPs: 6.2156. Time: 27.3507 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #115: GFLOPs: 6.7107. Time: 25.3327 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #116: GFLOPs: 6.4355. Time: 26.4159 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #117: GFLOPs: 6.4113. Time: 26.5155 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #118: GFLOPs: 3.7290. Time: 45.5891 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #119: GFLOPs: 10.9801. Time: 15.4826 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #120: GFLOPs: 6.0586. Time: 28.0593 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #121: GFLOPs: 5.9708. Time: 28.4720 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #122: GFLOPs: 7.0737. Time: 24.0326 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #123: GFLOPs: 1.9070. Time: 89.1473 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #124: GFLOPs: 4.3704. Time: 38.8980 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #125: GFLOPs: 4.4259. Time: 38.4101 us. Best GFLOPs: 11.7896
2024-04-29 06:56:09 [INFO] [task_scheduler.cc:131] [Task #22: fused_nn_global_avg_pool2d] Trial #126: GFLOPs: 6.2362. Time: 27.2602 us. Best GFLOPs: 11.7896
