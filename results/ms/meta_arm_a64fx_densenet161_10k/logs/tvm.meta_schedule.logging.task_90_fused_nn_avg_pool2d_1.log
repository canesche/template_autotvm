2024-04-29 02:33:59 [INFO] [task_scheduler.cc:160] Initializing Task #90: "fused_nn_avg_pool2d_1"
2024-04-29 02:33:59 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(96), T.int64(28), T.int64(28), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)))
        for ax0, ax1, ax2, ax3, ax4, rv0, rv1 in T.grid(T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4), T.int64(2), T.int64(2)):
            with T.block("pool_sum"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, v_rv0, v_rv1 = T.axis.remap("SSSSSRR", [ax0, ax1, ax2, ax3, ax4, rv0, rv1])
                T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1, v_ax4])
                T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                with T.init():
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1, v_ax4]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)):
            with T.block("pool_avg"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T.block_attr({"schedule_rule": "meta_schedule.pool_avg"})
                pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(27)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(27)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
2024-04-29 02:33:59 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-29 02:33:59 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(96), T.int64(28), T.int64(28), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 64, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)))
            pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4), T.int64(2)))
            for ax0, ax1 in T.grid(T.int64(1), T.int64(96)):
                for ax0_1, ax1_1, ax2, ax3, ax4, ax5, ax6 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(14), T.int64(14), T.int64(4), T.int64(2)):
                    with T.block("pool_sum_rf"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("SS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(96), ax1 + ax2)
                        v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1 = T.axis.remap("SSSR", [ax3, ax4, ax5, ax6])
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(2) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(2) + vrv0_rv1_fused_1) % T.int64(2), v_ax4])
                        T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        with T.init():
                            pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = T.float32(0)
                        pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(2) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(2) + vrv0_rv1_fused_1) % T.int64(2), v_ax4]
                for ax0_1, ax1_1, ax2, ax3, ax4, ax5 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(14), T.int64(14), T.int64(4)):
                    with T.block("pool_sum"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("RS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(96), ax1 + ax2)
                        v_ax2, v_ax3, v_ax4 = T.axis.remap("SSS", [ax3, ax4, ax5])
                        T.reads(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0]
                for ax2, ax3, ax4 in T.grid(T.int64(14), T.int64(14), T.int64(4)):
                    with T.block("pool_avg"):
                        v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(27)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(27)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[2, 2])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=1)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
2024-04-29 02:33:59 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(96), T.int64(28), T.int64(28), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 64, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)))
            pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4), T.int64(2)))
            for ax0, ax1 in T.grid(T.int64(1), T.int64(96)):
                for ax0_1 in range(T.int64(2)):
                    for ax0_2, ax1_1, ax2, ax3, ax4, ax5, ax6 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(14), T.int64(14), T.int64(4), T.int64(2)):
                        with T.block("pool_sum_rf"):
                            vrv0_rv1_fused_1 = T.axis.spatial(T.int64(2), ax0_1 + ax0_2)
                            v_ax0 = T.axis.spatial(T.int64(1), ax1_1)
                            v_ax1 = T.axis.spatial(T.int64(96), ax1 + ax2)
                            v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0 = T.axis.remap("SSSR", [ax3, ax4, ax5, ax6])
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(2) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(2) + vrv0_rv1_fused_1) % T.int64(2), v_ax4])
                            T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                            with T.init():
                                pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = T.float32(0)
                            pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(2) + vrv0_rv1_fused_1) // T.int64(2), v_ax3 * T.int64(2) + (vrv0_rv1_fused_0 * T.int64(2) + vrv0_rv1_fused_1) % T.int64(2), v_ax4]
                    for ax1_1, ax2, ax3, ax4, ax5 in T.grid(T.int64(1), T.int64(1), T.int64(14), T.int64(14), T.int64(4)):
                        with T.block("pool_sum"):
                            vrv0_rv1_fused_1, v_ax0 = T.axis.remap("RS", [ax0_1, ax1_1])
                            v_ax1 = T.axis.spatial(T.int64(96), ax1 + ax2)
                            v_ax2, v_ax3, v_ax4 = T.axis.remap("SSS", [ax3, ax4, ax5])
                            T.reads(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                            T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                            with T.init():
                                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1]
                for ax2, ax3, ax4 in T.grid(T.int64(14), T.int64(14), T.int64(4)):
                    with T.block("pool_avg"):
                        v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(27)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(27)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[2, 2])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=2)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
2024-04-29 02:33:59 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(96), T.int64(28), T.int64(28), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 512, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(14), T.int64(14), T.int64(4)))
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(96), T.int64(14)):
                for ax0_1, ax1_1, ax2_1, ax3, ax4, ax5, ax6 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(14), T.int64(4), T.int64(2), T.int64(2)):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0_1)
                        v_ax1 = T.axis.spatial(T.int64(96), ax1 + ax1_1)
                        v_ax2 = T.axis.spatial(T.int64(14), ax2 + ax2_1)
                        v_ax3, v_ax4, v_rv0, v_rv1 = T.axis.remap("SSRR", [ax3, ax4, ax5, ax6])
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1, v_ax4])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1, v_ax4]
                for ax3, ax4 in T.grid(T.int64(14), T.int64(4)):
                    with T.block("pool_avg"):
                        v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(v_ax2, T.int64(0)) * T.int64(2) + T.min(v_ax2 * T.int64(2) + T.int64(1), T.int64(27)) + T.int64(1) - v_ax2 * T.int64(2)) * (T.min(v_ax3, T.int64(0)) * T.int64(2) + T.min(v_ax3 * T.int64(2) + T.int64(1), T.int64(27)) + T.int64(1) - v_ax3 * T.int64(2)), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l4, preserve_unit_loops=True, index=-1)
2024-04-29 04:53:36 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 04:53:36 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-29 04:53:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x148d95d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xe967628)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x6d6b4d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x12f6f7f8)]: 0 failure(s)
2024-04-29 04:53:39 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-04-29 04:53:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x148d95d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xe967628)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x6d6b4d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x12f6f7f8)]: 0 failure(s)
2024-04-29 04:53:46 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x148d95d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xe967628)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x6d6b4d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x12f6f7f8)]: 0 failure(s)
2024-04-29 04:53:50 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x148d95d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xe967628)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x6d6b4d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x12f6f7f8)]: 0 failure(s)
2024-04-29 04:53:54 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x148d95d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xe967628)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x6d6b4d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x12f6f7f8)]: 0 failure(s)
2024-04-29 04:53:55 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0000  0.9965  0.9942  0.9937  0.9903  0.9882  0.9880  0.9876  0.9800  0.9796  0.9795  0.9785  0.9769  0.9767  0.9763  0.9746
[17 : 32]:	0.9745  0.9729  0.9712  0.9677  0.9674  0.9670  0.9662  0.9653  0.9652  0.9596  0.9565  0.9537  0.9477  0.9461  0.9460  0.9440
[33 : 48]:	0.9391  0.9347  0.9336  0.9331  0.9317  0.9317  0.9300  0.9292  0.9253  0.9238  0.9219  0.9205  0.9100  0.9095  0.9075  0.9018
[49 : 64]:	0.9016  0.8966  0.8960  0.8942  0.8921  0.8873  0.8811  0.8788  0.8776  0.8744  0.8740  0.8738  0.8734  0.8695  0.8686  0.8670
2024-04-29 04:53:55 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 04:53:55 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #1: GFLOPs: 38.4696. Time: 48.9114 us. Best GFLOPs: 38.4696
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #2: GFLOPs: 14.4103. Time: 130.5729 us. Best GFLOPs: 38.4696
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #3: GFLOPs: 31.4328. Time: 59.8611 us. Best GFLOPs: 38.4696
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #4: GFLOPs: 55.2699. Time: 34.0439 us. Best GFLOPs: 55.2699
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #5: GFLOPs: 134.1564. Time: 14.0254 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #6: GFLOPs: 44.2050. Time: 42.5654 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #7: GFLOPs: 110.7214. Time: 16.9940 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #8: GFLOPs: 104.8454. Time: 17.9464 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #9: GFLOPs: 35.1682. Time: 53.5029 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #10: GFLOPs: 49.6173. Time: 37.9223 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #11: GFLOPs: 42.8877. Time: 43.8728 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #12: GFLOPs: 40.2889. Time: 46.7026 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #13: GFLOPs: 73.7968. Time: 25.4970 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #14: GFLOPs: 117.3021. Time: 16.0406 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #15: GFLOPs: 100.8718. Time: 18.6534 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #16: GFLOPs: 45.9986. Time: 40.9056 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #17: GFLOPs: 58.6844. Time: 32.0630 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #18: GFLOPs: 28.8956. Time: 65.1173 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #19: GFLOPs: 40.9222. Time: 45.9799 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #20: GFLOPs: 65.2341. Time: 28.8438 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #21: GFLOPs: 62.1197. Time: 30.2899 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #22: GFLOPs: 97.7475. Time: 19.2496 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #23: GFLOPs: 53.9971. Time: 34.8463 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #24: GFLOPs: 84.5100. Time: 22.2648 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #25: GFLOPs: 123.8440. Time: 15.1933 us. Best GFLOPs: 134.1564
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #26: GFLOPs: 149.0487. Time: 12.6241 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #27: GFLOPs: 45.4445. Time: 41.4044 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #28: GFLOPs: 115.6852. Time: 16.2648 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #29: GFLOPs: 45.7639. Time: 41.1154 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #30: GFLOPs: 73.1884. Time: 25.7090 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #31: GFLOPs: 71.8644. Time: 26.1826 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #32: GFLOPs: 46.6015. Time: 40.3764 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #33: GFLOPs: 71.6910. Time: 26.2460 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #34: GFLOPs: 45.3434. Time: 41.4966 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #35: GFLOPs: 99.4711. Time: 18.9161 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #36: GFLOPs: 81.8236. Time: 22.9958 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #37: GFLOPs: 77.1465. Time: 24.3900 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #38: GFLOPs: 66.6621. Time: 28.2259 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #39: GFLOPs: 43.1931. Time: 43.5625 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #40: GFLOPs: 100.0517. Time: 18.8063 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #41: GFLOPs: 45.8353. Time: 41.0513 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #42: GFLOPs: 118.1222. Time: 15.9293 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #43: GFLOPs: 139.9161. Time: 13.4481 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #44: GFLOPs: 34.2829. Time: 54.8846 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #45: GFLOPs: 44.8757. Time: 41.9292 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #46: GFLOPs: 42.2236. Time: 44.5627 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #47: GFLOPs: 54.9805. Time: 34.2230 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #48: GFLOPs: 95.6737. Time: 19.6668 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #49: GFLOPs: 68.2151. Time: 27.5833 us. Best GFLOPs: 149.0487
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #50: GFLOPs: 150.8582. Time: 12.4726 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #51: GFLOPs: 47.2289. Time: 39.8400 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #52: GFLOPs: 59.1465. Time: 31.8125 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #53: GFLOPs: 70.5674. Time: 26.6639 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #54: GFLOPs: 82.3618. Time: 22.8456 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #55: GFLOPs: 110.7948. Time: 16.9827 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #56: GFLOPs: 134.6230. Time: 13.9768 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #57: GFLOPs: 67.7027. Time: 27.7921 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #58: GFLOPs: 55.9531. Time: 33.6281 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #59: GFLOPs: 84.1230. Time: 22.3672 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #60: GFLOPs: 119.7438. Time: 15.7135 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #61: GFLOPs: 108.7701. Time: 17.2989 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #62: GFLOPs: 119.9828. Time: 15.6822 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #63: GFLOPs: 96.1128. Time: 19.5770 us. Best GFLOPs: 150.8582
2024-04-29 05:13:04 [INFO] [task_scheduler.cc:131] [Task #90: fused_nn_avg_pool2d_1] Trial #64: GFLOPs: 45.2873. Time: 41.5481 us. Best GFLOPs: 150.8582
