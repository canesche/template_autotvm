2024-04-29 07:43:07 [INFO] [task_scheduler.cc:160] Initializing Task #35: "fused_nn_avg_pool2d_4"
2024-04-29 07:43:07 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(27), T.int64(27), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)))
        for ax0, ax1, ax2, ax3, ax4, rv0, rv1 in T.grid(T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4), T.int64(3), T.int64(3)):
            with T.block("pool_sum"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, v_rv0, v_rv1 = T.axis.remap("SSSSSRR", [ax0, ax1, ax2, ax3, ax4, rv0, rv1])
                T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1, v_ax4])
                T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                with T.init():
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1, v_ax4]
        for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)):
            with T.block("pool_avg"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                T.block_attr({"schedule_rule": "meta_schedule.pool_avg"})
                pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
2024-04-29 07:43:07 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-29 07:43:07 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(27), T.int64(27), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 512, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)))
            pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4), T.int64(3)))
            for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, ax5, ax6 in T.grid(T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3)):
                    with T.block("pool_sum_rf"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("SS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(64), ax1 + ax2_1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax2 + ax3_1)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3 + ax4_1)
                        v_ax4 = T.axis.spatial(T.int64(4), ax4 + ax5)
                        vrv0_rv1_fused_1 = T.axis.reduce(T.int64(3), ax6)
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + (vrv0_rv1_fused_0 * T.int64(3) + vrv0_rv1_fused_1) // T.int64(3), v_ax3 + (vrv0_rv1_fused_0 * T.int64(3) + vrv0_rv1_fused_1) % T.int64(3), v_ax4])
                        T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        with T.init():
                            pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = T.float32(0)
                        pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] = pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0] + p0[v_ax0, v_ax1, v_ax2 + (vrv0_rv1_fused_0 * T.int64(3) + vrv0_rv1_fused_1) // T.int64(3), v_ax3 + (vrv0_rv1_fused_0 * T.int64(3) + vrv0_rv1_fused_1) % T.int64(3), v_ax4]
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, ax5 in T.grid(T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                    with T.block("pool_sum"):
                        vrv0_rv1_fused_0, v_ax0 = T.axis.remap("RS", [ax0_1, ax1_1])
                        v_ax1 = T.axis.spatial(T.int64(64), ax1 + ax2_1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax2 + ax3_1)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3 + ax4_1)
                        v_ax4 = T.axis.spatial(T.int64(4), ax4 + ax5)
                        T.reads(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_0]
                with T.block("pool_avg"):
                    v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 3])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=4)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
2024-04-29 07:43:07 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(27), T.int64(27), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 64, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)))
            pool_sum_rf = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4), T.int64(3)))
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(64), T.int64(25)):
                for ax0_1 in range(T.int64(3)):
                    for ax0_2, ax1_1, ax2_1, ax3, ax4, ax5, ax6 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(4), T.int64(3)):
                        with T.block("pool_sum_rf"):
                            vrv0_rv1_fused_1 = T.axis.spatial(T.int64(3), ax0_1 + ax0_2)
                            v_ax0 = T.axis.spatial(T.int64(1), ax1_1)
                            v_ax1 = T.axis.spatial(T.int64(64), ax1 + ax2_1)
                            v_ax2 = T.axis.spatial(T.int64(25), ax2 + ax3)
                            v_ax3, v_ax4, vrv0_rv1_fused_0 = T.axis.remap("SSR", [ax4, ax5, ax6])
                            T.reads(p0[v_ax0, v_ax1, v_ax2 + (vrv0_rv1_fused_0 * T.int64(3) + vrv0_rv1_fused_1) // T.int64(3), v_ax3 + (vrv0_rv1_fused_0 * T.int64(3) + vrv0_rv1_fused_1) % T.int64(3), v_ax4])
                            T.writes(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                            with T.init():
                                pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = T.float32(0)
                            pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] = pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1] + p0[v_ax0, v_ax1, v_ax2 + (vrv0_rv1_fused_0 * T.int64(3) + vrv0_rv1_fused_1) // T.int64(3), v_ax3 + (vrv0_rv1_fused_0 * T.int64(3) + vrv0_rv1_fused_1) % T.int64(3), v_ax4]
                    for ax1_1, ax2_1, ax3, ax4, ax5 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(4)):
                        with T.block("pool_sum"):
                            vrv0_rv1_fused_1, v_ax0 = T.axis.remap("RS", [ax0_1, ax1_1])
                            v_ax1 = T.axis.spatial(T.int64(64), ax1 + ax2_1)
                            v_ax2 = T.axis.spatial(T.int64(25), ax2 + ax3)
                            v_ax3, v_ax4 = T.axis.remap("SS", [ax4, ax5])
                            T.reads(pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1])
                            T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                            with T.init():
                                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + pool_sum_rf[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4, vrv0_rv1_fused_1]
                for ax3, ax4 in T.grid(T.int64(25), T.int64(4)):
                    with T.block("pool_avg"):
                        v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 3])
l13, l14 = sch.split(loop=l10, factors=[v11, v12], preserve_unit_iters=True)
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l18, preserve_unit_loops=True, index=-1)
l19 = sch.sample_compute_location(block=b17, decision=3)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True, index=-1)
2024-04-29 07:43:07 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(27), T.int64(27), T.int64(4)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 768, "meta_schedule.unroll_explicit": 512, "meta_schedule.vectorize": 64})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)))
            for ax0, ax1, ax2, ax3, ax4 in T.grid(T.int64(1), T.int64(64), T.int64(25), T.int64(25), T.int64(4)):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4_1, ax5, ax6 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(3)):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0_1)
                        v_ax1 = T.axis.spatial(T.int64(64), ax1 + ax1_1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax2 + ax2_1)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3 + ax3_1)
                        v_ax4 = T.axis.spatial(T.int64(4), ax4 + ax4_1)
                        v_rv0, v_rv1 = T.axis.remap("RR", [ax5, ax6])
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1, v_ax4])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                        with T.init():
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = T.float32(0)
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1, v_ax4]
                with T.block("pool_avg"):
                    v_ax0, v_ax1, v_ax2, v_ax3, v_ax4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3, v_ax4] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=768)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l4, preserve_unit_loops=True, index=-1)
2024-04-29 08:33:45 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 08:33:45 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-29 08:33:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa4b20d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xa4b2e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd8239c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x981b638)]: 0 failure(s)
2024-04-29 08:33:48 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-04-29 08:33:51 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa4b20d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xa4b2e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd8239c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x981b638)]: 0 failure(s)
2024-04-29 08:33:55 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa4b20d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xa4b2e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd8239c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x981b638)]: 0 failure(s)
2024-04-29 08:33:59 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa4b20d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xa4b2e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd8239c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x981b638)]: 0 failure(s)
2024-04-29 08:34:03 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa4b20d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0xa4b2e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd8239c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x981b638)]: 0 failure(s)
2024-04-29 08:34:04 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9996  0.9976  0.9965  0.9932  0.9896  0.9843  0.9842  0.9838  0.9834  0.9827  0.9776  0.9775  0.9698  0.9688  0.9666  0.9639
[17 : 32]:	0.9619  0.9589  0.9580  0.9577  0.9545  0.9534  0.9527  0.9472  0.9456  0.9428  0.9426  0.9422  0.9363  0.9355  0.9344  0.9336
[33 : 48]:	0.9317  0.9306  0.9293  0.9289  0.9280  0.9264  0.9256  0.9229  0.9156  0.9147  0.9142  0.9142  0.9107  0.9104  0.9068  0.9037
[49 : 64]:	0.9032  0.8983  0.8979  0.8970  0.8965  0.8914  0.8908  0.8904  0.8854  0.8851  0.8842  0.8829  0.8828  0.8803  0.8738  0.8728
2024-04-29 08:34:04 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 08:34:05 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #1: GFLOPs: 24.4406. Time: 157.1159 us. Best GFLOPs: 24.4406
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #2: GFLOPs: 21.8564. Time: 175.6923 us. Best GFLOPs: 24.4406
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #3: GFLOPs: 160.5214. Time: 23.9220 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #4: GFLOPs: 91.1621. Time: 42.1228 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #5: GFLOPs: 21.0653. Time: 182.2900 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #6: GFLOPs: 42.9409. Time: 89.4252 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #7: GFLOPs: 23.4547. Time: 163.7201 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #8: GFLOPs: 30.8473. Time: 124.4840 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #9: GFLOPs: 143.5789. Time: 26.7449 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #10: GFLOPs: 26.7895. Time: 143.3397 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #11: GFLOPs: 26.0764. Time: 147.2594 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #12: GFLOPs: 38.8289. Time: 98.8954 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #13: GFLOPs: 60.9533. Time: 62.9990 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #14: GFLOPs: 54.7296. Time: 70.1631 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #15: GFLOPs: 93.2780. Time: 41.1673 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #16: GFLOPs: 55.4562. Time: 69.2438 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #17: GFLOPs: 61.8428. Time: 62.0929 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #18: GFLOPs: 55.8914. Time: 68.7046 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #19: GFLOPs: 135.9266. Time: 28.2505 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #20: GFLOPs: 24.1331. Time: 159.1177 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #21: GFLOPs: 104.1419. Time: 36.8728 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #22: GFLOPs: 11.9265. Time: 321.9720 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #23: GFLOPs: 80.7623. Time: 47.5469 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #24: GFLOPs: 43.1468. Time: 88.9984 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #25: GFLOPs: 92.2422. Time: 41.6295 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #26: GFLOPs: 108.9236. Time: 35.2541 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #27: GFLOPs: 158.4015. Time: 24.2422 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #28: GFLOPs: 48.9185. Time: 78.4979 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #29: GFLOPs: 71.1513. Time: 53.9695 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #30: GFLOPs: 136.9202. Time: 28.0455 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #31: GFLOPs: 60.7396. Time: 63.2207 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #32: GFLOPs: 70.8115. Time: 54.2285 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #33: GFLOPs: 35.6305. Time: 107.7727 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #34: GFLOPs: 64.3846. Time: 59.6416 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #35: GFLOPs: 106.2130. Time: 36.1538 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #36: GFLOPs: 60.1984. Time: 63.7890 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #37: GFLOPs: 128.7920. Time: 29.8155 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #38: GFLOPs: 80.2152. Time: 47.8712 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #39: GFLOPs: 128.8334. Time: 29.8059 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #40: GFLOPs: 132.1459. Time: 29.0588 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #41: GFLOPs: 68.6946. Time: 55.8996 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #42: GFLOPs: 74.0343. Time: 51.8679 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #43: GFLOPs: 39.8392. Time: 96.3874 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #44: GFLOPs: 56.4599. Time: 68.0129 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #45: GFLOPs: 38.3026. Time: 100.2544 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #46: GFLOPs: 71.2661. Time: 53.8825 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #47: GFLOPs: 83.1794. Time: 46.1653 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #48: GFLOPs: 80.6412. Time: 47.6183 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #49: GFLOPs: 35.6787. Time: 107.6272 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #50: GFLOPs: 13.5672. Time: 283.0349 us. Best GFLOPs: 160.5214
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #51: GFLOPs: 181.7856. Time: 21.1238 us. Best GFLOPs: 181.7856
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #52: GFLOPs: 39.1060. Time: 98.1947 us. Best GFLOPs: 181.7856
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #53: GFLOPs: 17.3687. Time: 221.0880 us. Best GFLOPs: 181.7856
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #54: GFLOPs: 69.8894. Time: 54.9439 us. Best GFLOPs: 181.7856
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #55: GFLOPs: 20.9758. Time: 183.0677 us. Best GFLOPs: 181.7856
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #56: GFLOPs: 200.6377. Time: 19.1390 us. Best GFLOPs: 200.6377
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #57: GFLOPs: 59.6328. Time: 64.3941 us. Best GFLOPs: 200.6377
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #58: GFLOPs: 61.9075. Time: 62.0281 us. Best GFLOPs: 200.6377
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #59: GFLOPs: 84.1010. Time: 45.6594 us. Best GFLOPs: 200.6377
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #60: GFLOPs: 94.4191. Time: 40.6697 us. Best GFLOPs: 200.6377
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #61: GFLOPs: 135.3875. Time: 28.3630 us. Best GFLOPs: 200.6377
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #62: GFLOPs: 52.1651. Time: 73.6124 us. Best GFLOPs: 200.6377
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #63: GFLOPs: 148.1005. Time: 25.9283 us. Best GFLOPs: 200.6377
2024-04-29 09:17:36 [INFO] [task_scheduler.cc:131] [Task #35: fused_nn_avg_pool2d_4] Trial #64: GFLOPs: 127.3717. Time: 30.1480 us. Best GFLOPs: 200.6377
