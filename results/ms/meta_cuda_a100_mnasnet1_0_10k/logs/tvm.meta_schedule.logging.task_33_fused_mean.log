2024-04-28 19:52:52 [INFO] [task_scheduler.cc:160] Initializing Task #33: "fused_mean"
2024-04-28 19:52:52 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0, ax1, k2, k3 in T.grid(T.int64(1), T.int64(1280), T.int64(7), T.int64(7)):
            with T.block("p0_red"):
                v_ax0, v_ax1, v_k2, v_k3 = T.axis.remap("SSRR", [ax0, ax1, k2, k3])
                T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                T.writes(p0_red[v_ax0, v_ax1])
                with T.init():
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1280)):
            with T.block("T_divide"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(p0_red[v_ax0, v_ax1])
                T.writes(T_divide[v_ax0, v_ax1])
                T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
2024-04-28 19:52:52 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2024-04-28 19:52:52 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
            for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                    for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                        with T.block("p0_red"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                            v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                            T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                            T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                            T.writes(p0_red[v_ax0, v_ax1])
                            with T.init():
                                p0_red[v_ax0, v_ax1] = T.float32(0)
                            p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                    with T.block("T_divide"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                        T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1])
                        T.writes(T_divide[v_ax0, v_ax1])
                        T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
2024-04-28 19:52:52 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
            for ax0_ax1_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x"):
                for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(8), T.int64(7)):
                    for ax2_ax3_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                        with T.block("p0_red"):
                            v_ax0 = T.axis.spatial(T.int64(1), ax0)
                            v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(8) + ax1)
                            v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(8) + ax2_ax3_fused_1) // T.int64(7))
                            v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(8) + ax2_ax3_fused_1) % T.int64(7))
                            T.where(ax2_ax3_fused_0 * T.int64(8) + ax2_ax3_fused_1 < T.int64(49))
                            T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                            T.writes(p0_red_shared[v_ax0, v_ax1])
                            with T.init():
                                p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                            p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
                for ax1_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("T_divide"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(8) + ax1_1)
                        T.reads(p0_red_shared[v_ax0, v_ax1])
                        T.writes(T_divide[v_ax0, v_ax1])
                        T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
2024-04-28 21:13:37 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-28 21:13:37 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-28 21:13:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x3458888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe596d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x4206e88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4212938)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x33531c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe3fee28)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe648a88)]: 0 failure(s)
2024-04-28 21:13:38 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-04-28 21:13:38 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x3458888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe596d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x4206e88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4212938)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x33531c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe3fee28)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe648a88)]: 0 failure(s)
2024-04-28 21:13:39 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x3458888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe596d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x4206e88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4212938)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x33531c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe3fee28)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe648a88)]: 0 failure(s)
2024-04-28 21:13:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x3458888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe596d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x4206e88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4212938)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x33531c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe3fee28)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe648a88)]: 0 failure(s)
2024-04-28 21:13:39 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x3458888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe596d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x4206e88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4212938)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x33531c8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe3fee28)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe648a88)]: 0 failure(s)
2024-04-28 21:13:39 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9981  0.9901  0.9821  0.9799  0.9763  0.9728  0.9715  0.9611  0.9586  0.9571  0.9564  0.9486  0.9432  0.9381  0.9372  0.9241
[17 : 32]:	0.9212  0.9164  0.9058  0.8988  0.8937  0.8935  0.8866  0.8842  0.8805  0.8794  0.8784  0.8763  0.8753  0.8644  0.8613  0.8588
[33 : 48]:	0.8545  0.8474  0.8463  0.8445  0.8270  0.8249  0.8179  0.8169  0.8136  0.8112  0.8095  0.8063  0.8056  0.8032  0.8012  0.8012
[49 : 64]:	0.7974  0.7950  0.7939  0.7923  0.7871  0.7860  0.7822  0.7808  0.7763  0.7729  0.7685  0.7674  0.7652  0.7637  0.7614  0.7554
2024-04-28 21:13:39 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-28 21:13:39 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #1: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(64), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(64) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(64) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #2: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #3: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #4: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #5: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #6: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(512), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(512) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(512) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(512) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax0_ax1_0_fused % T.int64(3) * T.int64(512) + ax1 < T.int64(1280) and ax2_ax3_fused_0 * T.int64(512) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(512) + ax1_1)
                    T.where(ax0_ax1_0_fused % T.int64(3) * T.int64(512) + ax1_1 < T.int64(1280))
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #7: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(256), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(256) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(256) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #8: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #9: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #10: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(256), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(256) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(256) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #11: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #12: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #13: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #14: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #15: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #16: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #17: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #18: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #19: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(256), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(256) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(256) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #20: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(32), T.int64(2)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(32) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(32) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(32) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(32) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(32) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #21: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #22: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #23: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #24: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #25: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #26: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #27: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(128), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(128) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(128) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(128) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(128) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(128) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #28: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #29: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #30: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #31: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #32: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(160), thread="blockIdx.x"):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(8), T.int64(7)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(8) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(8) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(8) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(8) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(8) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #33: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #34: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #35: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #36: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #37: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #38: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(4), T.int64(13)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(4) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(4) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(4) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(4) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(4) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #39: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #40: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #41: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #42: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(128), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(128) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(128) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(128) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(128) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(128) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #43: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #44: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #45: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #46: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(256), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(256) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(256) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #47: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #48: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #49: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #50: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #51: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #52: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #53: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #54: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red_shared = T.alloc_buffer((T.int64(1), T.int64(1280)), scope="shared")
        for ax0_ax1_0_fused in T.thread_binding(T.int64(320), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2_ax3_fused_0 in T.grid(T.int64(1), T.int64(4), T.int64(13)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("p0_red"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(4) + ax1)
                        v_k2 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(4) + ax2_ax3_fused_1) // T.int64(7))
                        v_k3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(4) + ax2_ax3_fused_1) % T.int64(7))
                        T.where(ax2_ax3_fused_0 * T.int64(4) + ax2_ax3_fused_1 < T.int64(49))
                        T.reads(p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red_shared[v_ax0, v_ax1])
                        with T.init():
                            p0_red_shared[v_ax0, v_ax1] = T.float32(0)
                        p0_red_shared[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
            for ax1_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_0_fused * T.int64(4) + ax1_1)
                    T.reads(p0_red_shared[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red_shared[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
b3, = sch.get_consumers(block=b0)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l7, l8 = sch.split(loop=l5, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b0)
l15 = sch.fuse(l13, l14, preserve_unit_iters=True)
l16, l17 = sch.split(loop=l15, factors=[None, v6], preserve_unit_iters=True)
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
l19, l20, l21 = sch.get_loops(block=b1)
l22 = sch.fuse(l19, l20, preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.enter_postproc()
b23 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b23, ann_key="meta_schedule.unroll_explicit")
b24, b25 = sch.get_child_blocks(b23)
l26, l27, l28, l29, l30 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b25)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #55: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #56: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #57: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #58: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #59: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(10), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #60: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #61: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #62: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
2024-04-28 21:20:36 [INFO] [task_scheduler.cc:121] [Task #33: fused_mean] Trial #63: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1280), T.int64(7), T.int64(7)), "float32"), T_divide: T.Buffer((T.int64(1), T.int64(1280)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        p0_red = T.alloc_buffer((T.int64(1), T.int64(1280)))
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("p0_red_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                    T.reads()
                    T.writes(p0_red[v_ax0, v_ax1])
                    p0_red[v_ax0, v_ax1] = T.float32(0)
                for k2, k3 in T.grid(T.int64(7), T.int64(7)):
                    with T.block("p0_red_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1)
                        v_k2, v_k3 = T.axis.remap("RR", [k2, k3])
                        T.reads(p0_red[v_ax0, v_ax1], p0[v_ax0, v_ax1, v_k2, v_k3])
                        T.writes(p0_red[v_ax0, v_ax1])
                        p0_red[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] + p0[v_ax0, v_ax1, v_k2, v_k3]
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(1280), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1)
                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1280))
                    T.reads(p0_red[v_ax0, v_ax1])
                    T.writes(T_divide[v_ax0, v_ax1])
                    T_divide[v_ax0, v_ax1] = p0_red[v_ax0, v_ax1] * T.float32(0.020408163265306121)
b0 = sch.get_block(name="p0_red", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5 = sch.get_loops(block=b1)
l6 = sch.fuse(l4, l5, preserve_unit_iters=True)
v7 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7], preserve_unit_iters=True)
sch.bind(loop=l8, thread_axis="blockIdx.x")
sch.bind(loop=l9, thread_axis="threadIdx.x")
l10, l11, l12, l13 = sch.get_loops(block=b0)
l14 = sch.fuse(l10, l11, preserve_unit_iters=True)
v15 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l16, l17 = sch.split(loop=l14, factors=[None, v15], preserve_unit_iters=True)
sch.bind(loop=l16, thread_axis="blockIdx.x")
sch.bind(loop=l17, thread_axis="threadIdx.x")
sch.enter_postproc()
b18 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b18, ann_key="meta_schedule.unroll_explicit")
b19, b20 = sch.get_child_blocks(b18)
l21, l22, l23, l24 = sch.get_loops(block=b19)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b20)
b27 = sch.get_block(name="p0_red", func_name="main")
l28, l29, l30, l31 = sch.get_loops(block=b27)
b32 = sch.decompose_reduction(block=b27, loop=l30)
