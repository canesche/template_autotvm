2024-04-28 19:52:45 [INFO] [task_scheduler.cc:160] Initializing Task #7: "fused_nn_conv2d_add_add"
2024-04-28 19:52:45 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(72), T.int64(56), T.int64(56)), "float32"), p1: T.Buffer((T.int64(24), T.int64(72), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(24), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(72), T.int64(56), T.int64(56)))
        conv2d_nchw = T.alloc_buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)))
        T_add_1 = T.alloc_buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(72), T.int64(56), T.int64(56)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(24), T.int64(56), T.int64(56), T.int64(72), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1[v_ff, v_rc, v_ry, v_rx])
                T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(24), T.int64(56), T.int64(56)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add_1[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(24), T.int64(56), T.int64(56)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3], p3[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = T_add_1[v_ax0, v_ax1, v_ax2, v_ax3] + p3[v_ax0, v_ax1, v_ax2, v_ax3]
2024-04-28 19:52:45 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-28 19:52:45 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(72), T.int64(56), T.int64(56)), "float32"), p1: T.Buffer((T.int64(24), T.int64(72), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(24), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 1024})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(72), T.int64(56), T.int64(56)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(24), T.int64(72), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(42), thread="threadIdx.x"):
                        for rc_0, ry_0, rx_0 in T.grid(T.int64(36), T.int64(1), T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1568)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(72), rc_0 * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(784))
                                    v2 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(784) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(28))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(24)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + ax0_ax1_ax2_ax3_fused // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(72), rc_0 * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(2))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 + ff_4)
                                    v_yy = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(4) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + xx_3 * T.int64(14) + xx_4)
                                    v_rc = T.axis.reduce(T.int64(72), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(28)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(4) + ax2)
                                v3 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 6, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[36, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
2024-04-28 19:52:45 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(72), T.int64(56), T.int64(56)), "float32"), p1: T.Buffer((T.int64(24), T.int64(72), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(24), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(72), T.int64(56), T.int64(56)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(24), T.int64(72), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(42), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(36), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1568)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(72), rc_0_ry_0_rx_0_fused * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(784))
                                    v2 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(784) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(28))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(24)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + ax0_ax1_ax2_ax3_fused // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(72), rc_0_ry_0_rx_0_fused * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(2))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 + ff_4)
                                    v_yy = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(4) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + xx_3 * T.int64(14) + xx_4)
                                    v_rc = T.axis.reduce(T.int64(72), rc_0_ry_0_rx_0_fused * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(28)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(4) + ax2)
                                v3 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 6, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[36, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
2024-04-28 19:52:45 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(72), T.int64(56), T.int64(56)), "float32"), p1: T.Buffer((T.int64(24), T.int64(72), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(24), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 1024})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(24), T.int64(56), T.int64(56)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(72), T.int64(56), T.int64(56)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(24), T.int64(72), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(42), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(36), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1568)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(72), rc_0_ry_0_rx_0_fused * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(784))
                                    v2 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(784) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(28))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(24)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + ax0_ax1_ax2_ax3_fused // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(72), rc_0_ry_0_rx_0_fused * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(2))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(14)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 + ff_4)
                                    v_yy = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(4) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + xx_3 * T.int64(14) + xx_4)
                                    v_rc = T.axis.reduce(T.int64(72), rc_0_ry_0_rx_0_fused * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(28)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(24), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(12) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) // T.int64(2) * T.int64(28) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(4) + ax2)
                                v3 = T.axis.spatial(T.int64(56), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(28) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 6, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[36, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
l100 = sch.fuse(l55, l61, l67, preserve_unit_iters=True)
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l100, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
2024-04-28 20:10:42 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-28 20:10:42 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-28 20:10:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 490 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 20:10:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 977 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 20:10:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 1468 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 20:10:49 [INFO] [evolutionary_search.cc:723] Sampled 68 candidate(s)
2024-04-28 20:10:52 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 20:10:54 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 109 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 20:10:57 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 99 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 20:10:59 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 20:10:59 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9992  0.9989  0.9986  0.9967  0.9963  0.9958  0.9957  0.9949  0.9935  0.9929  0.9927  0.9916  0.9915  0.9911  0.9902  0.9888
[17 : 32]:	0.9886  0.9880  0.9869  0.9853  0.9851  0.9843  0.9841  0.9817  0.9809  0.9801  0.9790  0.9789  0.9785  0.9781  0.9764  0.9763
[33 : 48]:	0.9761  0.9756  0.9755  0.9754  0.9737  0.9736  0.9733  0.9730  0.9726  0.9714  0.9712  0.9704  0.9697  0.9681  0.9677  0.9672
[49 : 64]:	0.9662  0.9661  0.9647  0.9645  0.9640  0.9621  0.9619  0.9613  0.9605  0.9599  0.9595  0.9590  0.9588  0.9579  0.9573  0.9572
2024-04-28 20:10:59 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-28 20:10:59 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #1: GFLOPs: 812.4764. Time: 13.5248 us. Best GFLOPs: 812.4764
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #2: GFLOPs: 173.2008. Time: 63.4440 us. Best GFLOPs: 812.4764
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #3: GFLOPs: 102.0605. Time: 107.6670 us. Best GFLOPs: 812.4764
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #4: GFLOPs: 1666.8435. Time: 6.5924 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #5: GFLOPs: 1577.7400. Time: 6.9647 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #6: GFLOPs: 660.7800. Time: 16.6297 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #7: GFLOPs: 984.2783. Time: 11.1641 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #8: GFLOPs: 712.2308. Time: 15.4283 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #9: GFLOPs: 254.6183. Time: 43.1569 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #10: GFLOPs: 17.1231. Time: 641.7367 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #11: GFLOPs: 47.4780. Time: 231.4449 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #12: GFLOPs: 811.8484. Time: 13.5352 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #13: GFLOPs: 167.4944. Time: 65.6055 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #14: GFLOPs: 14.1958. Time: 774.0701 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #15: GFLOPs: 123.0188. Time: 89.3241 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #16: GFLOPs: 1052.5933. Time: 10.4395 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #17: GFLOPs: 379.5198. Time: 28.9538 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #18: GFLOPs: 1220.4998. Time: 9.0033 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #19: GFLOPs: 504.5197. Time: 21.7802 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #20: GFLOPs: 290.5923. Time: 37.8143 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #21: GFLOPs: 1356.3212. Time: 8.1017 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #22: GFLOPs: 337.1819. Time: 32.5894 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #23: GFLOPs: 810.0621. Time: 13.5651 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #24: GFLOPs: 139.2888. Time: 78.8903 us. Best GFLOPs: 1666.8435
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #25: GFLOPs: 1727.8377. Time: 6.3597 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #26: GFLOPs: 1137.3243. Time: 9.6618 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #27: GFLOPs: 215.2253. Time: 51.0560 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #28: GFLOPs: 667.6043. Time: 16.4597 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #29: GFLOPs: 722.9152. Time: 15.2003 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #30: GFLOPs: 359.3807. Time: 30.5763 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #31: GFLOPs: 704.6000. Time: 15.5954 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #32: GFLOPs: 869.4722. Time: 12.6382 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #33: GFLOPs: 672.7453. Time: 16.3339 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #34: GFLOPs: 670.3236. Time: 16.3929 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #35: GFLOPs: 869.8332. Time: 12.6329 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #36: GFLOPs: 738.5598. Time: 14.8783 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #37: GFLOPs: 1225.8828. Time: 8.9638 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #38: GFLOPs: 1017.8805. Time: 10.7955 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #39: GFLOPs: 449.4574. Time: 24.4485 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #40: GFLOPs: 369.7531. Time: 29.7186 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #41: GFLOPs: 1005.7899. Time: 10.9253 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #42: GFLOPs: 11.7892. Time: 932.0865 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #43: GFLOPs: 12.8939. Time: 852.2281 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #44: GFLOPs: 424.5476. Time: 25.8829 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #45: GFLOPs: 650.9718. Time: 16.8802 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #46: GFLOPs: 229.1533. Time: 47.9528 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #47: GFLOPs: 55.3883. Time: 198.3910 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #48: GFLOPs: 1.3718. Time: 8010.5156 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #49: GFLOPs: 714.0193. Time: 15.3897 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #50: GFLOPs: 877.2176. Time: 12.5266 us. Best GFLOPs: 1727.8377
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #51: GFLOPs: 1728.5925. Time: 6.3569 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #52: GFLOPs: 4.5323. Time: 2424.5151 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #53: GFLOPs: 389.3353. Time: 28.2239 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #54: GFLOPs: 91.5949. Time: 119.9689 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #55: GFLOPs: 869.3011. Time: 12.6407 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #56: GFLOPs: 136.9573. Time: 80.2333 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #57: GFLOPs: 281.7701. Time: 38.9983 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #58: GFLOPs: 510.4685. Time: 21.5264 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #59: GFLOPs: 119.7415. Time: 91.7689 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #60: GFLOPs: 388.4856. Time: 28.2856 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #61: GFLOPs: 549.3882. Time: 20.0014 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #62: GFLOPs: 855.3137. Time: 12.8474 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #63: GFLOPs: 45.6470. Time: 240.7290 us. Best GFLOPs: 1728.5925
2024-04-28 21:19:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #64: GFLOPs: 116.7677. Time: 94.1060 us. Best GFLOPs: 1728.5925
2024-04-28 22:04:31 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-28 22:04:31 [INFO] [evolutionary_search.cc:715] Picked top 64 candidate(s) from database
2024-04-28 22:04:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 433 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 22:04:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 867 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 22:04:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 1289 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 22:04:36 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-04-28 22:04:40 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 111 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 22:04:43 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 22:04:47 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 22:04:50 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 22:04:51 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0255  1.0149  1.0091  1.0072  1.0069  1.0039  1.0008  0.9989  0.9948  0.9914  0.9909  0.9883  0.9883  0.9879  0.9809  0.9743
[17 : 32]:	0.9624  0.9550  0.9473  0.9461  0.9448  0.9436  0.9395  0.9377  0.9375  0.9367  0.9338  0.9325  0.9323  0.9323  0.9320  0.9309
[33 : 48]:	0.9283  0.9283  0.9273  0.9260  0.9235  0.9219  0.9200  0.9195  0.9182  0.9182  0.9175  0.9156  0.9153  0.9131  0.9131  0.9125
[49 : 64]:	0.9119  0.9095  0.9086  0.9081  0.9077  0.9070  0.9050  0.9028  0.9028  0.9027  0.9012  0.9003  0.8984  0.8974  0.8948  0.8947
2024-04-28 22:04:51 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-28 22:04:51 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #65: GFLOPs: 1578.7103. Time: 6.9605 us. Best GFLOPs: 1728.5925
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #66: GFLOPs: 1756.2999. Time: 6.2566 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #67: GFLOPs: 1688.3623. Time: 6.5084 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #68: GFLOPs: 1690.2772. Time: 6.5010 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #69: GFLOPs: 1689.6463. Time: 6.5035 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #70: GFLOPs: 1685.1684. Time: 6.5207 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #71: GFLOPs: 1694.7598. Time: 6.4838 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #72: GFLOPs: 1371.7945. Time: 8.0103 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #73: GFLOPs: 1369.0917. Time: 8.0262 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #74: GFLOPs: 1673.7372. Time: 6.5653 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #75: GFLOPs: 1754.5279. Time: 6.2630 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #76: GFLOPs: 1694.0879. Time: 6.4864 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #77: GFLOPs: 1693.9989. Time: 6.4867 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #78: GFLOPs: 1367.1151. Time: 8.0378 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #79: GFLOPs: 1689.2803. Time: 6.5049 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #80: GFLOPs: 1685.5870. Time: 6.5191 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #81: GFLOPs: 1687.5556. Time: 6.5115 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #82: GFLOPs: 1633.9452. Time: 6.7252 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #83: GFLOPs: 1634.7280. Time: 6.7219 us. Best GFLOPs: 1756.2999
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #84: GFLOPs: 1915.4340. Time: 5.7368 us. Best GFLOPs: 1915.4340
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #85: GFLOPs: 1688.5783. Time: 6.5076 us. Best GFLOPs: 1915.4340
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #86: GFLOPs: 1977.2568. Time: 5.5575 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #87: GFLOPs: 1180.4579. Time: 9.3087 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #88: GFLOPs: 1545.8580. Time: 7.1084 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #89: GFLOPs: 1445.1737. Time: 7.6036 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #90: GFLOPs: 1917.9527. Time: 5.7293 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #91: GFLOPs: 1897.4082. Time: 5.7913 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #92: GFLOPs: 1844.9189. Time: 5.9561 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #93: GFLOPs: 1916.9636. Time: 5.7323 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #94: GFLOPs: 1683.8343. Time: 6.5259 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #95: GFLOPs: 1317.6066. Time: 8.3398 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #96: GFLOPs: 1683.3870. Time: 6.5276 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #97: GFLOPs: 1694.6225. Time: 6.4844 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #98: GFLOPs: 1915.5992. Time: 5.7363 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #99: GFLOPs: 1685.1514. Time: 6.5208 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #100: GFLOPs: 1684.0192. Time: 6.5252 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #101: GFLOPs: 1414.3012. Time: 7.7696 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #102: GFLOPs: 1151.1045. Time: 9.5461 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #103: GFLOPs: 1918.4082. Time: 5.7279 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #104: GFLOPs: 1434.8243. Time: 7.6585 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #105: GFLOPs: 1579.5259. Time: 6.9569 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #106: GFLOPs: 1458.5135. Time: 7.5341 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #107: GFLOPs: 1977.2549. Time: 5.5575 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #108: GFLOPs: 1694.1822. Time: 6.4860 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #109: GFLOPs: 1684.7767. Time: 6.5223 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #110: GFLOPs: 1345.5102. Time: 8.1668 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #111: GFLOPs: 1345.4003. Time: 8.1675 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #112: GFLOPs: 1918.8898. Time: 5.7265 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #113: GFLOPs: 1579.9379. Time: 6.9550 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #114: GFLOPs: 1688.6254. Time: 6.5074 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #115: GFLOPs: 1529.5605. Time: 7.1841 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #116: GFLOPs: 1571.6701. Time: 6.9916 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #117: GFLOPs: 1691.7045. Time: 6.4955 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #118: GFLOPs: 1368.2490. Time: 8.0311 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #119: GFLOPs: 1361.1353. Time: 8.0731 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #120: GFLOPs: 1687.6058. Time: 6.5113 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #121: GFLOPs: 1687.6842. Time: 6.5110 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #122: GFLOPs: 1561.1345. Time: 7.0388 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #123: GFLOPs: 1686.0475. Time: 6.5173 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #124: GFLOPs: 1391.8524. Time: 7.8949 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #125: GFLOPs: 1932.2519. Time: 5.6869 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #126: GFLOPs: 542.2568. Time: 20.2645 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #127: GFLOPs: 272.2082. Time: 40.3682 us. Best GFLOPs: 1977.2568
2024-04-28 22:07:19 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #128: GFLOPs: 277.3651. Time: 39.6176 us. Best GFLOPs: 1977.2568
2024-04-28 23:10:58 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-28 23:10:59 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-28 23:11:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 23:11:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 794 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 23:11:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 1188 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 23:11:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 1577 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 23:11:06 [INFO] [evolutionary_search.cc:723] Sampled 63 candidate(s)
2024-04-28 23:11:09 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 23:11:13 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 23:11:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 94 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 23:11:19 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-28 23:11:20 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0106  0.9953  0.9949  0.9923  0.9913  0.9883  0.9792  0.9733  0.9653  0.9649  0.9646  0.9600  0.9577  0.9560  0.9553  0.9542
[17 : 32]:	0.9541  0.9531  0.9505  0.9500  0.9467  0.9424  0.9396  0.9393  0.9390  0.9354  0.9343  0.9332  0.9329  0.9310  0.9281  0.9266
[33 : 48]:	0.9261  0.9254  0.9247  0.9237  0.9232  0.9217  0.9208  0.9202  0.9188  0.9184  0.9182  0.9173  0.9159  0.9158  0.9155  0.9151
[49 : 64]:	0.9142  0.9137  0.9134  0.9130  0.9121  0.9120  0.9119  0.9118  0.9095  0.9091  0.9080  0.9068  0.9061  0.9058  0.9052  0.9042
2024-04-28 23:11:20 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-28 23:11:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #129: GFLOPs: 1786.8716. Time: 6.1496 us. Best GFLOPs: 1977.2568
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #130: GFLOPs: 1981.8117. Time: 5.5447 us. Best GFLOPs: 1981.8117
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #131: GFLOPs: 1980.6449. Time: 5.5480 us. Best GFLOPs: 1981.8117
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #132: GFLOPs: 1979.9483. Time: 5.5499 us. Best GFLOPs: 1981.8117
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #133: GFLOPs: 1989.1999. Time: 5.5241 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #134: GFLOPs: 1981.3484. Time: 5.5460 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #135: GFLOPs: 1909.1571. Time: 5.7557 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #136: GFLOPs: 1906.7380. Time: 5.7630 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #137: GFLOPs: 1550.0966. Time: 7.0889 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #138: GFLOPs: 1985.2778. Time: 5.5350 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #139: GFLOPs: 1962.0181. Time: 5.6006 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #140: GFLOPs: 1937.9614. Time: 5.6702 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #141: GFLOPs: 1904.9216. Time: 5.7685 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #142: GFLOPs: 1945.8250. Time: 5.6472 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #143: GFLOPs: 1915.4397. Time: 5.7368 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #144: GFLOPs: 1986.6412. Time: 5.5312 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #145: GFLOPs: 1979.7663. Time: 5.5504 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #146: GFLOPs: 1944.0635. Time: 5.6524 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #147: GFLOPs: 1904.3552. Time: 5.7702 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #148: GFLOPs: 1913.1517. Time: 5.7437 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #149: GFLOPs: 1925.4795. Time: 5.7069 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #150: GFLOPs: 1933.5888. Time: 5.6830 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #151: GFLOPs: 1851.6472. Time: 5.9345 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #152: GFLOPs: 1919.2378. Time: 5.7255 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #153: GFLOPs: 1918.6748. Time: 5.7272 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #154: GFLOPs: 1763.8018. Time: 6.2300 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #155: GFLOPs: 1977.1476. Time: 5.5578 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #156: GFLOPs: 1914.3967. Time: 5.7400 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #157: GFLOPs: 1781.6274. Time: 6.1677 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #158: GFLOPs: 1781.4705. Time: 6.1682 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #159: GFLOPs: 1813.8681. Time: 6.0581 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #160: GFLOPs: 1928.8464. Time: 5.6970 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #161: GFLOPs: 1840.9122. Time: 5.9691 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #162: GFLOPs: 1835.1978. Time: 5.9877 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #163: GFLOPs: 1985.1315. Time: 5.5354 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #164: GFLOPs: 1903.5261. Time: 5.7727 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #165: GFLOPs: 1860.4182. Time: 5.9065 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #166: GFLOPs: 1777.7302. Time: 6.1812 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #167: GFLOPs: 1776.9029. Time: 6.1841 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #168: GFLOPs: 1930.6886. Time: 5.6915 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #169: GFLOPs: 1915.6525. Time: 5.7362 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #170: GFLOPs: 1919.1116. Time: 5.7258 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #171: GFLOPs: 1891.1071. Time: 5.8106 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #172: GFLOPs: 1804.4491. Time: 6.0897 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #173: GFLOPs: 1834.5465. Time: 5.9898 us. Best GFLOPs: 1989.1999
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #174: GFLOPs: 1990.5064. Time: 5.5205 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #175: GFLOPs: 1155.5157. Time: 9.5096 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #176: GFLOPs: 1816.8615. Time: 6.0481 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #177: GFLOPs: 1537.8418. Time: 7.1454 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #178: GFLOPs: 1939.5420. Time: 5.6655 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #179: GFLOPs: 1909.7001. Time: 5.7541 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #180: GFLOPs: 1934.6188. Time: 5.6800 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #181: GFLOPs: 1902.1587. Time: 5.7769 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #182: GFLOPs: 1980.3332. Time: 5.5488 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #183: GFLOPs: 1898.6594. Time: 5.7875 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #184: GFLOPs: 1839.1212. Time: 5.9749 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #185: GFLOPs: 1766.4752. Time: 6.2206 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #186: GFLOPs: 1707.8391. Time: 6.4342 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #187: GFLOPs: 1835.8400. Time: 5.9856 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #188: GFLOPs: 1982.5903. Time: 5.5425 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #189: GFLOPs: 1915.6349. Time: 5.7362 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #190: GFLOPs: 257.4333. Time: 42.6850 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #191: GFLOPs: 1087.3787. Time: 10.1055 us. Best GFLOPs: 1990.5064
2024-04-28 23:12:56 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #192: GFLOPs: 1514.0280. Time: 7.2578 us. Best GFLOPs: 1990.5064
2024-04-29 00:28:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 00:28:35 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 00:28:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 387 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 00:28:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 775 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 00:28:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 1161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 00:28:39 [INFO] [evolutionary_search.cc:723] Sampled 69 candidate(s)
2024-04-29 00:28:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 80 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 00:28:45 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 76 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 00:28:48 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 80 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 00:28:51 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 61 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 00:28:53 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0064  1.0032  0.9980  0.9971  0.9959  0.9959  0.9913  0.9902  0.9901  0.9886  0.9883  0.9879  0.9873  0.9855  0.9853  0.9847
[17 : 32]:	0.9847  0.9843  0.9840  0.9840  0.9839  0.9835  0.9830  0.9830  0.9829  0.9829  0.9821  0.9817  0.9815  0.9815  0.9807  0.9807
[33 : 48]:	0.9805  0.9802  0.9798  0.9792  0.9790  0.9790  0.9790  0.9787  0.9786  0.9785  0.9785  0.9783  0.9783  0.9783  0.9781  0.9780
[49 : 64]:	0.9780  0.9775  0.9775  0.9774  0.9774  0.9772  0.9769  0.9764  0.9763  0.9756  0.9755  0.9753  0.9750  0.9748  0.9746  0.9745
2024-04-29 00:28:53 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 00:28:53 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #193: GFLOPs: 1852.1529. Time: 5.9328 us. Best GFLOPs: 1990.5064
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #194: GFLOPs: 2023.1139. Time: 5.4315 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #195: GFLOPs: 2005.3070. Time: 5.4797 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #196: GFLOPs: 1966.4387. Time: 5.5880 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #197: GFLOPs: 1971.5088. Time: 5.5737 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #198: GFLOPs: 1892.3074. Time: 5.8070 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #199: GFLOPs: 1966.3890. Time: 5.5882 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #200: GFLOPs: 1885.1932. Time: 5.8289 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #201: GFLOPs: 1755.2793. Time: 6.2603 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #202: GFLOPs: 1970.7601. Time: 5.5758 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #203: GFLOPs: 1970.4211. Time: 5.5767 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #204: GFLOPs: 1896.0229. Time: 5.7956 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #205: GFLOPs: 1972.7994. Time: 5.5700 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #206: GFLOPs: 1977.6488. Time: 5.5564 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #207: GFLOPs: 1918.0586. Time: 5.7290 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #208: GFLOPs: 1975.4239. Time: 5.5626 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #209: GFLOPs: 1955.2640. Time: 5.6200 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #210: GFLOPs: 1975.7802. Time: 5.5616 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #211: GFLOPs: 1966.6212. Time: 5.5875 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #212: GFLOPs: 1964.5267. Time: 5.5935 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #213: GFLOPs: 1959.6641. Time: 5.6074 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #214: GFLOPs: 1966.5148. Time: 5.5878 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #215: GFLOPs: 1966.8882. Time: 5.5868 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #216: GFLOPs: 1965.5662. Time: 5.5905 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #217: GFLOPs: 1835.2157. Time: 5.9876 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #218: GFLOPs: 1966.3975. Time: 5.5882 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #219: GFLOPs: 1894.7181. Time: 5.7996 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #220: GFLOPs: 1972.0933. Time: 5.5720 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #221: GFLOPs: 1984.6899. Time: 5.5367 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #222: GFLOPs: 1975.4887. Time: 5.5624 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #223: GFLOPs: 1975.1172. Time: 5.5635 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #224: GFLOPs: 1966.8940. Time: 5.5867 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #225: GFLOPs: 1964.4455. Time: 5.5937 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #226: GFLOPs: 1972.7440. Time: 5.5702 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #227: GFLOPs: 1971.2295. Time: 5.5745 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #228: GFLOPs: 1971.3914. Time: 5.5740 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #229: GFLOPs: 1978.0953. Time: 5.5551 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #230: GFLOPs: 1971.0722. Time: 5.5749 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #231: GFLOPs: 1976.6434. Time: 5.5592 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #232: GFLOPs: 1969.8962. Time: 5.5782 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #233: GFLOPs: 1972.2817. Time: 5.5715 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #234: GFLOPs: 1829.9959. Time: 6.0047 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #235: GFLOPs: 1981.0794. Time: 5.5467 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #236: GFLOPs: 1971.5342. Time: 5.5736 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #237: GFLOPs: 1971.6989. Time: 5.5731 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #238: GFLOPs: 1972.0303. Time: 5.5722 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #239: GFLOPs: 1972.6015. Time: 5.5706 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #240: GFLOPs: 1967.1066. Time: 5.5861 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #241: GFLOPs: 1961.3107. Time: 5.6027 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #242: GFLOPs: 1980.0936. Time: 5.5495 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #243: GFLOPs: 1980.1172. Time: 5.5494 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #244: GFLOPs: 1977.8114. Time: 5.5559 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #245: GFLOPs: 1973.0290. Time: 5.5694 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #246: GFLOPs: 1974.9989. Time: 5.5638 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #247: GFLOPs: 1912.8850. Time: 5.7445 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #248: GFLOPs: 1971.4177. Time: 5.5739 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #249: GFLOPs: 1971.4200. Time: 5.5739 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #250: GFLOPs: 1987.3819. Time: 5.5292 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #251: GFLOPs: 1841.4276. Time: 5.9674 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #252: GFLOPs: 1961.9741. Time: 5.6008 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #253: GFLOPs: 1901.5313. Time: 5.7788 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #254: GFLOPs: 445.4377. Time: 24.6691 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #255: GFLOPs: 452.8797. Time: 24.2637 us. Best GFLOPs: 2023.1139
2024-04-29 00:30:33 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #256: GFLOPs: 518.6774. Time: 21.1857 us. Best GFLOPs: 2023.1139
2024-04-29 01:19:41 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 01:19:42 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 01:19:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 391 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 01:19:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 784 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 01:19:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 1180 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 01:19:48 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-04-29 01:19:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 74 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 01:19:54 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 66 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 01:19:57 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 76 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 01:20:00 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x4e2f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x429eda8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x420c0e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4196fc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4211868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x4268728)]: 73 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x4296cc8)]: 0 failure(s)
2024-04-29 01:20:01 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9809  0.9787  0.9787  0.9785  0.9782  0.9775  0.9771  0.9768  0.9768  0.9765  0.9756  0.9756  0.9748  0.9746  0.9746  0.9745
[17 : 32]:	0.9745  0.9740  0.9738  0.9738  0.9737  0.9736  0.9734  0.9732  0.9732  0.9732  0.9732  0.9730  0.9730  0.9730  0.9729  0.9727
[33 : 48]:	0.9725  0.9725  0.9723  0.9722  0.9722  0.9720  0.9718  0.9717  0.9717  0.9716  0.9715  0.9714  0.9713  0.9712  0.9711  0.9709
[49 : 64]:	0.9707  0.9704  0.9703  0.9700  0.9695  0.9693  0.9693  0.9692  0.9692  0.9690  0.9688  0.9688  0.9686  0.9684  0.9683  0.9682
2024-04-29 01:20:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 01:20:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #257: GFLOPs: 1968.9833. Time: 5.5808 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #258: GFLOPs: 1983.2667. Time: 5.5406 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #259: GFLOPs: 1974.2804. Time: 5.5658 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #260: GFLOPs: 1964.5609. Time: 5.5934 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #261: GFLOPs: 1975.4281. Time: 5.5626 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #262: GFLOPs: 1965.6048. Time: 5.5904 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #263: GFLOPs: 1978.4451. Time: 5.5541 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #264: GFLOPs: 1974.0337. Time: 5.5665 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #265: GFLOPs: 1974.4035. Time: 5.5655 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #266: GFLOPs: 1980.7447. Time: 5.5477 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #267: GFLOPs: 1965.2739. Time: 5.5914 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #268: GFLOPs: 1965.8818. Time: 5.5896 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #269: GFLOPs: 1982.3120. Time: 5.5433 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #270: GFLOPs: 1978.6851. Time: 5.5535 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #271: GFLOPs: 1983.1838. Time: 5.5409 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #272: GFLOPs: 1821.3663. Time: 6.0331 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #273: GFLOPs: 1978.9722. Time: 5.5527 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #274: GFLOPs: 1971.3089. Time: 5.5742 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #275: GFLOPs: 1964.9030. Time: 5.5924 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #276: GFLOPs: 1962.0763. Time: 5.6005 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #277: GFLOPs: 1887.1236. Time: 5.8229 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #278: GFLOPs: 1982.4633. Time: 5.5429 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #279: GFLOPs: 1970.7707. Time: 5.5758 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #280: GFLOPs: 1961.6808. Time: 5.6016 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #281: GFLOPs: 1961.6241. Time: 5.6018 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #282: GFLOPs: 1887.4476. Time: 5.8219 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #283: GFLOPs: 1982.3541. Time: 5.5432 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #284: GFLOPs: 1887.4246. Time: 5.8220 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #285: GFLOPs: 1983.4991. Time: 5.5400 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #286: GFLOPs: 1982.2432. Time: 5.5435 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #287: GFLOPs: 1985.1092. Time: 5.5355 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #288: GFLOPs: 1960.0224. Time: 5.6063 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #289: GFLOPs: 1964.5725. Time: 5.5934 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:47 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #290: GFLOPs: 1899.7245. Time: 5.7843 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #291: GFLOPs: 1981.0458. Time: 5.5468 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #292: GFLOPs: 1982.4363. Time: 5.5429 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #293: GFLOPs: 1968.8245. Time: 5.5813 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #294: GFLOPs: 1968.4326. Time: 5.5824 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #295: GFLOPs: 1978.6906. Time: 5.5534 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #296: GFLOPs: 1975.0549. Time: 5.5637 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #297: GFLOPs: 1961.4358. Time: 5.6023 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #298: GFLOPs: 1908.7654. Time: 5.7569 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #299: GFLOPs: 1988.0704. Time: 5.5272 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #300: GFLOPs: 1970.9378. Time: 5.5753 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #301: GFLOPs: 1973.4887. Time: 5.5681 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #302: GFLOPs: 1973.4732. Time: 5.5681 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #303: GFLOPs: 1974.9675. Time: 5.5639 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #304: GFLOPs: 1973.3427. Time: 5.5685 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #305: GFLOPs: 1898.4253. Time: 5.7882 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #306: GFLOPs: 1982.7787. Time: 5.5420 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #307: GFLOPs: 1916.6870. Time: 5.7331 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #308: GFLOPs: 1895.8916. Time: 5.7960 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #309: GFLOPs: 1906.1127. Time: 5.7649 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #310: GFLOPs: 1980.7925. Time: 5.5475 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #311: GFLOPs: 1968.2569. Time: 5.5829 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #312: GFLOPs: 1970.9266. Time: 5.5753 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #313: GFLOPs: 1962.1992. Time: 5.6001 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #314: GFLOPs: 1975.0362. Time: 5.5637 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #315: GFLOPs: 1973.6574. Time: 5.5676 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #316: GFLOPs: 1902.5437. Time: 5.7757 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #317: GFLOPs: 1973.5062. Time: 5.5680 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #318: GFLOPs: 1503.7920. Time: 7.3072 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #319: GFLOPs: 829.9394. Time: 13.2402 us. Best GFLOPs: 2023.1139
2024-04-29 01:21:48 [INFO] [task_scheduler.cc:131] [Task #7: fused_nn_conv2d_add_add] Trial #320: GFLOPs: 323.1870. Time: 34.0006 us. Best GFLOPs: 2023.1139
