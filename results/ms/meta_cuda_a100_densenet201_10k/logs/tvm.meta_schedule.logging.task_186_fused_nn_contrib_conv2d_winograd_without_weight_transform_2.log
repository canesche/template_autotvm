2024-04-29 16:32:28 [INFO] [task_scheduler.cc:160] Initializing Task #186: "fused_nn_contrib_conv2d_winograd_without_weight_transform_2"
2024-04-29 16:32:28 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        data_pad = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(30), T.int64(30)))
        input_tile = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)))
        B = T.alloc_buffer((T.int64(4), T.int64(4)))
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
        A = T.alloc_buffer((T.int64(4), T.int64(2)))
        inverse = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(128), T.int64(30), T.int64(30)):
            with T.block("data_pad"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(data_pad[v_i0, v_i1, v_i2, v_i3])
                data_pad[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(29) and T.int64(1) <= v_i3 and v_i3 < T.int64(29), p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for ci, p, eps, nu in T.grid(T.int64(128), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("input_tile"):
                v_ci, v_p, v_eps, v_nu = T.axis.remap("SSSS", [ci, p, eps, nu])
                T.reads(data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu])
                T.writes(input_tile[v_ci, v_p, v_eps, v_nu])
                T.block_attr({"schedule_rule": "None"})
                input_tile[v_ci, v_p, v_eps, v_nu] = data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu]
        for i, j in T.grid(T.int64(4), T.int64(4)):
            with T.block("B"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(B[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                B[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
        for eps, nu, ci, p, r_a, r_b in T.grid(T.int64(4), T.int64(4), T.int64(128), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("data_pack"):
                v_eps, v_nu, v_ci, v_p, v_r_a, v_r_b = T.axis.remap("SSSSRR", [eps, nu, ci, p, r_a, r_b])
                T.reads(input_tile[v_ci, v_p, v_r_a, v_r_b], B[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_eps, v_nu):T.min(v_eps, v_nu) + (T.max(v_eps, v_nu) + T.int64(1) - T.min(v_eps, v_nu))])
                T.writes(data_pack[v_eps, v_nu, v_ci, v_p])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                with T.init():
                    data_pack[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                data_pack[v_eps, v_nu, v_ci, v_p] = data_pack[v_eps, v_nu, v_ci, v_p] + input_tile[v_ci, v_p, v_r_a, v_r_b] * B[v_r_a, v_eps] * B[v_r_b, v_nu]
        for eps, nu, co, p, ci in T.grid(T.int64(4), T.int64(4), T.int64(32), T.int64(196), T.int64(128)):
            with T.block("bgemm"):
                v_eps, v_nu, v_co, v_p, v_ci = T.axis.remap("SSSSR", [eps, nu, co, p, ci])
                T.reads(data_pack[v_eps, v_nu, v_ci, v_p], p1[v_eps, v_nu, v_ci, v_co])
                T.writes(bgemm[v_eps, v_nu, v_co, v_p])
                with T.init():
                    bgemm[v_eps, v_nu, v_co, v_p] = T.float32(0)
                bgemm[v_eps, v_nu, v_co, v_p] = bgemm[v_eps, v_nu, v_co, v_p] + data_pack[v_eps, v_nu, v_ci, v_p] * p1[v_eps, v_nu, v_ci, v_co]
        for i, j in T.grid(T.int64(4), T.int64(2)):
            with T.block("A"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(A[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                A[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
        for co, p, vh, vw, r_a, r_b in T.grid(T.int64(32), T.int64(196), T.int64(2), T.int64(2), T.int64(4), T.int64(4)):
            with T.block("inverse"):
                v_co, v_p, v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSSSRR", [co, p, vh, vw, r_a, r_b])
                T.reads(bgemm[v_r_a, v_r_b, v_co, v_p], A[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_vh, v_vw):T.min(v_vh, v_vw) + (T.max(v_vh, v_vw) + T.int64(1) - T.min(v_vh, v_vw))])
                T.writes(inverse[v_co, v_p, v_vh, v_vw])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                with T.init():
                    inverse[v_co, v_p, v_vh, v_vw] = T.float32(0)
                inverse[v_co, v_p, v_vh, v_vw] = inverse[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * A[v_r_a, v_vh] * A[v_r_b, v_vw]
        for n, co, h, w in T.grid(T.int64(1), T.int64(32), T.int64(28), T.int64(28)):
            with T.block("conv2d_winograd"):
                v_n, v_co, v_h, v_w = T.axis.remap("SSSS", [n, co, h, w])
                T.reads(inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                conv2d_winograd[v_n, v_co, v_h, v_w] = inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
2024-04-29 16:32:28 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-29 16:32:28 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                        for ci_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(7168)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1792))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(1792) // T.int64(448))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(448) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(4), T.int64(2), T.int64(14), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused * T.int64(2) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + p_3 * T.int64(2) + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(16) + ci_1 * T.int64(8) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(2), T.int64(28)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), ax1)
                                v2 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.where(n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1 < T.int64(6272))
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.where(n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1 < T.int64(6272))
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                            T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                            conv2d_winograd[v_n, v_co, v_h, v_w] = inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
b5, b6 = sch.get_producers(block=b2)
sch.compute_inline(block=b6)
b7, = sch.get_consumers(block=b2)
l8, l9, l10, l11 = sch.get_loops(block=b7)
l12, l13 = sch.split(loop=l10, factors=[None, 2], preserve_unit_iters=True)
l14, l15 = sch.split(loop=l11, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l12, l14, l13, l15)
sch.compute_at(block=b2, loop=l14, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)
sch.unroll(loop=l22)
sch.unroll(loop=l23)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
b26, b27 = sch.get_producers(block=b0)
sch.compute_inline(block=b27)
b28, = sch.get_producers(block=b26)
l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b0)
sch.reorder(l31, l32, l29, l30, l33, l34)
sch.unroll(loop=l29)
sch.unroll(loop=l30)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
l35 = sch.fuse(l31, l32, preserve_unit_iters=True)
v36 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l37, l38 = sch.split(loop=l35, factors=[None, v36], preserve_unit_iters=True)
sch.bind(loop=l37, thread_axis="blockIdx.x")
sch.bind(loop=l38, thread_axis="threadIdx.x")
b39 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b39, loop=l38, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b26, loop=l38, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b26, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b28)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l40, l41, l42, l43, l44 = sch.get_loops(block=b1)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l40, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l50, l51, l52, l53, l54 = sch.split(loop=l40, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l41, n=5, max_innermost_factor=64, decision=[1, 1, 1, 4, 1])
l60, l61, l62, l63, l64 = sch.split(loop=l41, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67, v68, v69 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[2, 2, 4, 2, 1])
l70, l71, l72, l73, l74 = sch.split(loop=l42, factors=[v65, v66, v67, v68, v69], preserve_unit_iters=True)
v75, v76, v77, v78, v79 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[7, 1, 1, 14, 2])
l80, l81, l82, l83, l84 = sch.split(loop=l43, factors=[v75, v76, v77, v78, v79], preserve_unit_iters=True)
v85, v86, v87 = sch.sample_perfect_tile(loop=l44, n=3, max_innermost_factor=64, decision=[8, 2, 8])
l88, l89, l90 = sch.split(loop=l44, factors=[v85, v86, v87], preserve_unit_iters=True)
sch.reorder(l50, l60, l70, l80, l51, l61, l71, l81, l52, l62, l72, l82, l88, l89, l53, l63, l73, l83, l90, l54, l64, l74, l84)
l91 = sch.fuse(l50, l60, l70, l80, preserve_unit_iters=True)
sch.bind(loop=l91, thread_axis="blockIdx.x")
l92 = sch.fuse(l51, l61, l71, l81, preserve_unit_iters=True)
sch.bind(loop=l92, thread_axis="vthread.x")
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b94 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b94, loop=l93, preserve_unit_loops=True, index=-1)
b95 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b95, loop=l88, preserve_unit_loops=True, index=-1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b95)
l104 = sch.fuse(l100, l101, l102, l103, preserve_unit_iters=True)
v105 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b95, ann_key="meta_schedule.cooperative_fetch", ann_val=v105)
b106 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b106, loop=l88, preserve_unit_loops=True, index=-1)
l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b106)
l115 = sch.fuse(l111, l112, l113, l114, preserve_unit_iters=True)
v116 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b106, ann_key="meta_schedule.cooperative_fetch", ann_val=v116)
v117 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v117)
l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b3)
l124 = sch.fuse(l118, l119, l120, l121, preserve_unit_iters=True)
v125 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l126, l127 = sch.split(loop=l124, factors=[None, v125], preserve_unit_iters=True)
sch.bind(loop=l126, thread_axis="blockIdx.x")
sch.bind(loop=l127, thread_axis="threadIdx.x")
2024-04-29 16:32:28 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(7168)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1792))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(1792) // T.int64(448))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(448) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(4), T.int64(2), T.int64(14), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused * T.int64(2) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + p_3 * T.int64(2) + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(16) + ci_1 * T.int64(8) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(2), T.int64(28)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), ax1)
                                v2 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(13), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.where(n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1 < T.int64(6272))
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.where(n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1 < T.int64(6272))
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                            T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                            conv2d_winograd[v_n, v_co, v_h, v_w] = inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
b5, b6 = sch.get_producers(block=b2)
sch.compute_inline(block=b6)
b7, = sch.get_consumers(block=b2)
l8, l9, l10, l11 = sch.get_loops(block=b7)
l12, l13 = sch.split(loop=l10, factors=[None, 2], preserve_unit_iters=True)
l14, l15 = sch.split(loop=l11, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l12, l14, l13, l15)
sch.compute_at(block=b2, loop=l14, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)
sch.unroll(loop=l22)
sch.unroll(loop=l23)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
b26, b27 = sch.get_producers(block=b0)
sch.compute_inline(block=b27)
b28, = sch.get_producers(block=b26)
l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b0)
sch.reorder(l31, l32, l29, l30, l33, l34)
sch.unroll(loop=l29)
sch.unroll(loop=l30)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
l35 = sch.fuse(l31, l32, preserve_unit_iters=True)
v36 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l37, l38 = sch.split(loop=l35, factors=[None, v36], preserve_unit_iters=True)
sch.bind(loop=l37, thread_axis="blockIdx.x")
sch.bind(loop=l38, thread_axis="threadIdx.x")
b39 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b39, loop=l38, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b26, loop=l38, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b26, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b28)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l40, l41, l42, l43, l44 = sch.get_loops(block=b1)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l40, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l50, l51, l52, l53, l54 = sch.split(loop=l40, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l41, n=5, max_innermost_factor=64, decision=[1, 1, 1, 4, 1])
l60, l61, l62, l63, l64 = sch.split(loop=l41, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67, v68, v69 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[2, 2, 4, 2, 1])
l70, l71, l72, l73, l74 = sch.split(loop=l42, factors=[v65, v66, v67, v68, v69], preserve_unit_iters=True)
v75, v76, v77, v78, v79 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[7, 1, 1, 14, 2])
l80, l81, l82, l83, l84 = sch.split(loop=l43, factors=[v75, v76, v77, v78, v79], preserve_unit_iters=True)
v85, v86, v87 = sch.sample_perfect_tile(loop=l44, n=3, max_innermost_factor=64, decision=[8, 2, 8])
l88, l89, l90 = sch.split(loop=l44, factors=[v85, v86, v87], preserve_unit_iters=True)
sch.reorder(l50, l60, l70, l80, l51, l61, l71, l81, l52, l62, l72, l82, l88, l89, l53, l63, l73, l83, l90, l54, l64, l74, l84)
l91 = sch.fuse(l50, l60, l70, l80, preserve_unit_iters=True)
sch.bind(loop=l91, thread_axis="blockIdx.x")
l92 = sch.fuse(l51, l61, l71, l81, preserve_unit_iters=True)
sch.bind(loop=l92, thread_axis="vthread.x")
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b94 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b94, loop=l93, preserve_unit_loops=True, index=-1)
b95 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b95, loop=l88, preserve_unit_loops=True, index=-1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b95)
l104 = sch.fuse(l100, l101, l102, l103, preserve_unit_iters=True)
v105 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b95, ann_key="meta_schedule.cooperative_fetch", ann_val=v105)
b106 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b106, loop=l88, preserve_unit_loops=True, index=-1)
l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b106)
l115 = sch.fuse(l111, l112, l113, l114, preserve_unit_iters=True)
v116 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b106, ann_key="meta_schedule.cooperative_fetch", ann_val=v116)
l117 = sch.fuse(l88, preserve_unit_iters=True)
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_async_stages", ann_val=[0])
v118 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v118)
l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b3)
l125 = sch.fuse(l119, l120, l121, l122, preserve_unit_iters=True)
v126 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l127, l128 = sch.split(loop=l125, factors=[None, v126], preserve_unit_iters=True)
sch.bind(loop=l127, thread_axis="blockIdx.x")
sch.bind(loop=l128, thread_axis="threadIdx.x")
2024-04-29 16:32:28 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(7168)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1792))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(1792) // T.int64(448))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(448) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + ax0_ax1_ax2_ax3_fused % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(4), T.int64(2), T.int64(14), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(2)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused * T.int64(2) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + p_3 * T.int64(2) + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(16) + ci_1 * T.int64(8) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(2), T.int64(28)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), ax1)
                                v2 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused * T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(49), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                            T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                            conv2d_winograd[v_n, v_co, v_h, v_w] = inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
b5, b6 = sch.get_producers(block=b2)
sch.compute_inline(block=b6)
b7, = sch.get_consumers(block=b2)
l8, l9, l10, l11 = sch.get_loops(block=b7)
l12, l13 = sch.split(loop=l10, factors=[None, 2], preserve_unit_iters=True)
l14, l15 = sch.split(loop=l11, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l12, l14, l13, l15)
sch.compute_at(block=b2, loop=l14, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)
sch.unroll(loop=l22)
sch.unroll(loop=l23)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
b26, b27 = sch.get_producers(block=b0)
sch.compute_inline(block=b27)
b28, = sch.get_producers(block=b26)
l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b0)
sch.reorder(l31, l32, l29, l30, l33, l34)
sch.unroll(loop=l29)
sch.unroll(loop=l30)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
l35 = sch.fuse(l31, l32, preserve_unit_iters=True)
v36 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l37, l38 = sch.split(loop=l35, factors=[None, v36], preserve_unit_iters=True)
sch.bind(loop=l37, thread_axis="blockIdx.x")
sch.bind(loop=l38, thread_axis="threadIdx.x")
b39 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b39, loop=l38, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b26, loop=l38, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b26, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b28)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l40, l41, l42, l43, l44 = sch.get_loops(block=b1)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l40, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l50, l51, l52, l53, l54 = sch.split(loop=l40, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l41, n=5, max_innermost_factor=64, decision=[1, 1, 1, 4, 1])
l60, l61, l62, l63, l64 = sch.split(loop=l41, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67, v68, v69 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[2, 2, 4, 2, 1])
l70, l71, l72, l73, l74 = sch.split(loop=l42, factors=[v65, v66, v67, v68, v69], preserve_unit_iters=True)
v75, v76, v77, v78, v79 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[7, 1, 1, 14, 2])
l80, l81, l82, l83, l84 = sch.split(loop=l43, factors=[v75, v76, v77, v78, v79], preserve_unit_iters=True)
v85, v86, v87 = sch.sample_perfect_tile(loop=l44, n=3, max_innermost_factor=64, decision=[8, 2, 8])
l88, l89, l90 = sch.split(loop=l44, factors=[v85, v86, v87], preserve_unit_iters=True)
sch.reorder(l50, l60, l70, l80, l51, l61, l71, l81, l52, l62, l72, l82, l88, l89, l53, l63, l73, l83, l90, l54, l64, l74, l84)
l91 = sch.fuse(l50, l60, l70, l80, preserve_unit_iters=True)
sch.bind(loop=l91, thread_axis="blockIdx.x")
l92 = sch.fuse(l51, l61, l71, l81, preserve_unit_iters=True)
sch.bind(loop=l92, thread_axis="vthread.x")
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b94 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b94, loop=l93, preserve_unit_loops=True, index=-1)
b95 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b95, loop=l88, preserve_unit_loops=True, index=-1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b95)
l104 = sch.fuse(l100, l101, l102, l103, preserve_unit_iters=True)
v105 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b95, ann_key="meta_schedule.cooperative_fetch", ann_val=v105)
b106 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b106, loop=l88, preserve_unit_loops=True, index=-1)
l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b106)
l115 = sch.fuse(l111, l112, l113, l114, preserve_unit_iters=True)
v116 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b106, ann_key="meta_schedule.cooperative_fetch", ann_val=v116)
l117 = sch.fuse(l88, preserve_unit_iters=True)
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_async_stages", ann_val=[0])
v118 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v118)
l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b3)
l125 = sch.fuse(l119, l120, l121, l122, preserve_unit_iters=True)
v126 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l127, l128 = sch.split(loop=l125, factors=[None, v126], preserve_unit_iters=True)
sch.bind(loop=l127, thread_axis="blockIdx.x")
sch.bind(loop=l128, thread_axis="threadIdx.x")
2024-04-29 19:34:52 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 19:34:52 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-29 19:34:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 492 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 19:35:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 991 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 19:35:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 1477 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 19:35:06 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2024-04-29 19:35:16 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 110 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 19:35:26 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 110 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 19:35:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 96 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 19:35:45 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 19:35:46 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9992  0.9986  0.9969  0.9966  0.9965  0.9964  0.9956  0.9954  0.9953  0.9951  0.9943  0.9941  0.9939  0.9930  0.9909  0.9906
[17 : 32]:	0.9905  0.9883  0.9880  0.9873  0.9873  0.9856  0.9851  0.9847  0.9844  0.9835  0.9832  0.9830  0.9825  0.9817  0.9817  0.9810
[33 : 48]:	0.9807  0.9781  0.9774  0.9763  0.9759  0.9746  0.9743  0.9729  0.9729  0.9720  0.9711  0.9697  0.9691  0.9691  0.9687  0.9661
[49 : 64]:	0.9655  0.9653  0.9651  0.9648  0.9634  0.9623  0.9617  0.9609  0.9605  0.9604  0.9601  0.9600  0.9593  0.9577  0.9572  0.9557
2024-04-29 19:35:46 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 19:35:46 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #1: GFLOPs: 1722.3187. Time: 26.8022 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #2: GFLOPs: 72.8035. Time: 634.0616 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #3: GFLOPs: 926.8750. Time: 49.8038 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #4: GFLOPs: 872.0098. Time: 52.9374 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #5: GFLOPs: 242.2512. Time: 190.5540 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #6: GFLOPs: 1245.4826. Time: 37.0635 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #7: GFLOPs: 429.9272. Time: 107.3715 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #8: GFLOPs: 926.9059. Time: 49.8022 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #9: GFLOPs: 115.7086. Time: 398.9499 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #10: GFLOPs: 1573.8763. Time: 29.3301 us. Best GFLOPs: 1722.3187
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #11: GFLOPs: 1997.8572. Time: 23.1057 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #12: GFLOPs: 897.9207. Time: 51.4098 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #13: GFLOPs: 868.1363. Time: 53.1736 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #14: GFLOPs: 1085.5303. Time: 42.5248 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #15: GFLOPs: 266.1448. Time: 173.4466 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #16: GFLOPs: 166.5656. Time: 277.1396 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #17: GFLOPs: 1739.8068. Time: 26.5328 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #18: GFLOPs: 482.9660. Time: 95.5801 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #19: GFLOPs: 614.1459. Time: 75.1644 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #20: GFLOPs: 1453.4885. Time: 31.7594 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #21: GFLOPs: 27.2470. Time: 1694.2026 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #22: GFLOPs: 843.0588. Time: 54.7553 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #23: GFLOPs: 1780.0453. Time: 25.9330 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #24: GFLOPs: 285.9569. Time: 161.4297 us. Best GFLOPs: 1997.8572
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #25: GFLOPs: 2325.7483. Time: 19.8482 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #26: GFLOPs: 546.5915. Time: 84.4541 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #27: GFLOPs: 3.6645. Time: 12597.1202 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #28: GFLOPs: 857.8458. Time: 53.8114 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #29: GFLOPs: 685.0869. Time: 67.3811 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #30: GFLOPs: 632.1564. Time: 73.0229 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #31: GFLOPs: 12.9995. Time: 3551.0554 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #32: GFLOPs: 42.0646. Time: 1097.4052 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #33: GFLOPs: 159.2902. Time: 289.7975 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #34: GFLOPs: 1341.5399. Time: 34.4097 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #35: GFLOPs: 340.0437. Time: 135.7529 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #36: GFLOPs: 13.4665. Time: 3427.9083 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #37: GFLOPs: 27.4926. Time: 1679.0699 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #38: GFLOPs: 581.3727. Time: 79.4016 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #39: GFLOPs: 1106.9081. Time: 41.7035 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #40: GFLOPs: 29.3337. Time: 1573.6800 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #41: GFLOPs: 687.9895. Time: 67.0968 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #42: GFLOPs: 1126.2084. Time: 40.9888 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #43: GFLOPs: 662.7708. Time: 69.6499 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #44: GFLOPs: 13.7580. Time: 3355.2724 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #45: GFLOPs: 1209.5575. Time: 38.1643 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #46: GFLOPs: 11.2096. Time: 4118.0774 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #47: GFLOPs: 612.4892. Time: 75.3677 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #48: GFLOPs: 749.9201. Time: 61.5558 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #49: GFLOPs: 344.1538. Time: 134.1317 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #50: GFLOPs: 255.1831. Time: 180.8973 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #51: GFLOPs: 803.1339. Time: 57.4772 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #52: GFLOPs: 430.4679. Time: 107.2366 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #53: GFLOPs: 481.1975. Time: 95.9313 us. Best GFLOPs: 2325.7483
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #54: GFLOPs: 2753.8806. Time: 16.7625 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #55: GFLOPs: 438.3239. Time: 105.3146 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #56: GFLOPs: 840.2465. Time: 54.9385 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #57: GFLOPs: 2295.2869. Time: 20.1116 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #58: GFLOPs: 567.5676. Time: 81.3329 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #59: GFLOPs: 710.1478. Time: 65.0033 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #60: GFLOPs: 168.0914. Time: 274.6239 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #61: GFLOPs: 312.5952. Time: 147.6732 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #62: GFLOPs: 841.6938. Time: 54.8441 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #63: GFLOPs: 1401.2072. Time: 32.9444 us. Best GFLOPs: 2753.8806
2024-04-29 20:32:54 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #64: GFLOPs: 1397.2766. Time: 33.0371 us. Best GFLOPs: 2753.8806
2024-04-29 20:49:36 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 20:49:38 [INFO] [evolutionary_search.cc:715] Picked top 64 candidate(s) from database
2024-04-29 20:49:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 432 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 20:49:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 863 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 20:49:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 1297 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 20:49:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 1725 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 20:49:55 [INFO] [evolutionary_search.cc:723] Sampled 67 candidate(s)
2024-04-29 20:50:07 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 131 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 20:50:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 20:50:33 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 68 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 20:50:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 86 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 20:50:50 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.8157  1.7315  1.6304  1.6248  1.5482  1.5237  1.5172  1.4775  1.4620  1.4485  1.4466  1.4421  1.4388  1.4360  1.4296  1.4203
[17 : 32]:	1.4203  1.4150  1.4083  1.4038  1.3998  1.3998  1.3981  1.3940  1.3915  1.3873  1.3837  1.3812  1.3811  1.3807  1.3783  1.3771
[33 : 48]:	1.3759  1.3674  1.3668  1.3664  1.3664  1.3514  1.3472  1.3465  1.3315  1.3148  1.3051  1.3051  1.3051  1.3046  1.3005  1.2975
[49 : 64]:	1.2904  1.2889  1.2886  1.2821  1.2818  1.2818  1.2818  1.2734  1.2705  1.2705  1.2704  1.2704  1.2649  1.2640  1.2628  1.2587
2024-04-29 20:50:50 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 20:50:50 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #65: GFLOPs: 983.5700. Time: 46.9330 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #66: GFLOPs: 1098.7577. Time: 42.0128 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #67: GFLOPs: 2470.8767. Time: 18.6824 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #68: GFLOPs: 2516.5437. Time: 18.3434 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #69: GFLOPs: 1043.8717. Time: 44.2218 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #70: GFLOPs: 1118.9762. Time: 41.2537 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #71: GFLOPs: 964.2529. Time: 47.8732 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #72: GFLOPs: 585.3275. Time: 78.8651 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #73: GFLOPs: 979.0174. Time: 47.1513 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #74: GFLOPs: 1036.6966. Time: 44.5279 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #75: GFLOPs: 690.9355. Time: 66.8108 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #76: GFLOPs: 569.8615. Time: 81.0055 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #77: GFLOPs: 963.0877. Time: 47.9312 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #78: GFLOPs: 1117.0662. Time: 41.3242 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #79: GFLOPs: 952.1191. Time: 48.4833 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #80: GFLOPs: 585.3226. Time: 78.8658 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #81: GFLOPs: 585.3649. Time: 78.8601 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #82: GFLOPs: 688.1574. Time: 67.0805 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #83: GFLOPs: 663.2537. Time: 69.5992 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #84: GFLOPs: 898.8156. Time: 51.3586 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #85: GFLOPs: 1062.7691. Time: 43.4355 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #86: GFLOPs: 1039.9602. Time: 44.3882 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #87: GFLOPs: 2199.9326. Time: 20.9833 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #88: GFLOPs: 2455.4373. Time: 18.7999 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #89: GFLOPs: 1511.0189. Time: 30.5502 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #90: GFLOPs: 1630.3560. Time: 28.3140 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #91: GFLOPs: 663.1048. Time: 69.6148 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #92: GFLOPs: 1511.1344. Time: 30.5479 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #93: GFLOPs: 1699.2752. Time: 27.1657 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #94: GFLOPs: 689.4143. Time: 66.9582 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #95: GFLOPs: 1556.8158. Time: 29.6515 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #96: GFLOPs: 1508.5584. Time: 30.6000 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #97: GFLOPs: 1511.3409. Time: 30.5437 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #98: GFLOPs: 1196.8728. Time: 38.5688 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #99: GFLOPs: 1794.2028. Time: 25.7284 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #100: GFLOPs: 1067.6752. Time: 43.2359 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #101: GFLOPs: 1063.6384. Time: 43.4000 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #102: GFLOPs: 1248.6045. Time: 36.9708 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #103: GFLOPs: 1637.1866. Time: 28.1959 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #104: GFLOPs: 663.0639. Time: 69.6191 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #105: GFLOPs: 934.0719. Time: 49.4201 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #106: GFLOPs: 609.0804. Time: 75.7895 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #107: GFLOPs: 497.2463. Time: 92.8351 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #108: GFLOPs: 490.7383. Time: 94.0663 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #109: GFLOPs: 490.8002. Time: 94.0544 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #110: GFLOPs: 1189.0823. Time: 38.8215 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #111: GFLOPs: 678.0282. Time: 68.0826 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #112: GFLOPs: 763.0935. Time: 60.4931 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #113: GFLOPs: 291.7351. Time: 158.2323 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #114: GFLOPs: 497.4027. Time: 92.8059 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #115: GFLOPs: 1651.2152. Time: 27.9563 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #116: GFLOPs: 1548.8041. Time: 29.8049 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #117: GFLOPs: 1194.4065. Time: 38.6484 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #118: GFLOPs: 1194.1722. Time: 38.6560 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #119: GFLOPs: 1182.8496. Time: 39.0260 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #120: GFLOPs: 838.2567. Time: 55.0690 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #121: GFLOPs: 685.4002. Time: 67.3503 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #122: GFLOPs: 673.9672. Time: 68.4928 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #123: GFLOPs: 682.1342. Time: 67.6728 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #124: GFLOPs: 693.0474. Time: 66.6072 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #125: GFLOPs: 690.0722. Time: 66.8943 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #126: GFLOPs: 556.7975. Time: 82.9061 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #127: GFLOPs: 364.2960. Time: 126.7154 us. Best GFLOPs: 2753.8806
2024-04-29 20:53:01 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #128: GFLOPs: 26.4402. Time: 1745.9023 us. Best GFLOPs: 2753.8806
2024-04-29 21:08:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 21:08:37 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 21:08:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 397 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:08:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 795 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:08:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 1187 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:08:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 1581 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:08:53 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2024-04-29 21:09:04 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 99 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:09:18 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:09:31 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 85 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:09:44 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 83 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:09:48 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	2.7468  2.5261  2.3959  2.3205  2.3104  2.3051  2.2935  2.2852  2.2781  2.2763  2.1407  2.1185  2.1152  2.1070  2.1019  2.0918
[17 : 32]:	2.0865  2.0833  2.0749  2.0718  2.0682  2.0666  2.0603  1.9829  1.9766  1.9743  1.9717  1.9272  1.9249  1.9119  1.9060  1.8849
[33 : 48]:	1.8810  1.8690  1.8567  1.8563  1.8479  1.8258  1.8258  1.8196  1.8189  1.8171  1.8128  1.7904  1.7898  1.7796  1.7551  1.7426
[49 : 64]:	1.7203  1.7176  1.7082  1.7008  1.6993  1.6886  1.6840  1.6795  1.6795  1.6795  1.6580  1.6545  1.6543  1.6409  1.6255  1.6176
2024-04-29 21:09:49 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 21:09:49 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #129: GFLOPs: 2128.5489. Time: 21.6870 us. Best GFLOPs: 2753.8806
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #130: GFLOPs: 2563.2657. Time: 18.0090 us. Best GFLOPs: 2753.8806
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #131: GFLOPs: 2508.2505. Time: 18.4040 us. Best GFLOPs: 2753.8806
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #132: GFLOPs: 2781.3726. Time: 16.5968 us. Best GFLOPs: 2781.3726
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #133: GFLOPs: 2789.1479. Time: 16.5505 us. Best GFLOPs: 2789.1479
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #134: GFLOPs: 2777.3842. Time: 16.6206 us. Best GFLOPs: 2789.1479
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #135: GFLOPs: 2563.3099. Time: 18.0087 us. Best GFLOPs: 2789.1479
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #136: GFLOPs: 2472.2460. Time: 18.6721 us. Best GFLOPs: 2789.1479
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #137: GFLOPs: 2488.2927. Time: 18.5516 us. Best GFLOPs: 2789.1479
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #138: GFLOPs: 2745.2719. Time: 16.8151 us. Best GFLOPs: 2789.1479
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #139: GFLOPs: 2083.7097. Time: 22.1537 us. Best GFLOPs: 2789.1479
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #140: GFLOPs: 2799.3309. Time: 16.4903 us. Best GFLOPs: 2799.3309
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #141: GFLOPs: 1784.1796. Time: 25.8729 us. Best GFLOPs: 2799.3309
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #142: GFLOPs: 2158.3912. Time: 21.3872 us. Best GFLOPs: 2799.3309
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #143: GFLOPs: 2796.0128. Time: 16.5099 us. Best GFLOPs: 2799.3309
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #144: GFLOPs: 2786.2220. Time: 16.5679 us. Best GFLOPs: 2799.3309
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #145: GFLOPs: 2803.7217. Time: 16.4645 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #146: GFLOPs: 2150.3093. Time: 21.4676 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #147: GFLOPs: 2578.0159. Time: 17.9060 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #148: GFLOPs: 2673.3771. Time: 17.2673 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #149: GFLOPs: 2419.2258. Time: 19.0813 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #150: GFLOPs: 2478.0301. Time: 18.6285 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #151: GFLOPs: 2642.4992. Time: 17.4690 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #152: GFLOPs: 2216.7660. Time: 20.8240 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #153: GFLOPs: 2257.8010. Time: 20.4455 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #154: GFLOPs: 2179.6765. Time: 21.1783 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #155: GFLOPs: 1730.7281. Time: 26.6720 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #156: GFLOPs: 2252.5913. Time: 20.4928 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #157: GFLOPs: 2151.5213. Time: 21.4555 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #158: GFLOPs: 2275.8731. Time: 20.2832 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #159: GFLOPs: 2240.8830. Time: 20.5999 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #160: GFLOPs: 2179.7996. Time: 21.1771 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #161: GFLOPs: 2675.4976. Time: 17.2536 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #162: GFLOPs: 2146.1097. Time: 21.5096 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #163: GFLOPs: 2179.1385. Time: 21.1836 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #164: GFLOPs: 2181.7018. Time: 21.1587 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #165: GFLOPs: 1275.1371. Time: 36.2015 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #166: GFLOPs: 1187.7869. Time: 38.8638 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #167: GFLOPs: 1212.5448. Time: 38.0703 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #168: GFLOPs: 2153.1934. Time: 21.4388 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #169: GFLOPs: 1250.2937. Time: 36.9209 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #170: GFLOPs: 2206.1044. Time: 20.9246 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #171: GFLOPs: 1851.8352. Time: 24.9277 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #172: GFLOPs: 1275.2319. Time: 36.1988 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #173: GFLOPs: 1540.7084. Time: 29.9615 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #174: GFLOPs: 2145.8423. Time: 21.5123 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #175: GFLOPs: 2132.7552. Time: 21.6443 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #176: GFLOPs: 2782.5721. Time: 16.5897 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #177: GFLOPs: 2475.2206. Time: 18.6496 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #178: GFLOPs: 2550.9495. Time: 18.0960 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #179: GFLOPs: 2783.0093. Time: 16.5871 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #180: GFLOPs: 1659.6303. Time: 27.8146 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #181: GFLOPs: 2538.2797. Time: 18.1863 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #182: GFLOPs: 2183.1680. Time: 21.1445 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #183: GFLOPs: 2375.7190. Time: 19.4307 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #184: GFLOPs: 2221.8134. Time: 20.7767 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #185: GFLOPs: 2214.6025. Time: 20.8443 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #186: GFLOPs: 2226.1774. Time: 20.7360 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #187: GFLOPs: 1010.8483. Time: 45.6665 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #188: GFLOPs: 2204.4394. Time: 20.9404 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #189: GFLOPs: 2675.1230. Time: 17.2560 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #190: GFLOPs: 1682.8168. Time: 27.4313 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #191: GFLOPs: 545.6980. Time: 84.5924 us. Best GFLOPs: 2803.7217
2024-04-29 21:11:46 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #192: GFLOPs: 409.2667. Time: 112.7918 us. Best GFLOPs: 2803.7217
2024-04-29 21:38:36 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 21:38:39 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 21:38:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 397 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:38:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 787 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:38:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 1186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:38:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 1583 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:38:54 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-04-29 21:39:06 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 120 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:39:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:39:33 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:39:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe6649c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xdf31278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x149b4418)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x10a4a778)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x1413f578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd9e4c78)]: 114 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x149b43e8)]: 0 failure(s)
2024-04-29 21:39:51 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	2.7718  2.3989  2.3481  2.3323  2.3147  2.3134  2.3130  2.3066  2.3036  2.2979  2.2945  2.2904  2.2706  2.2696  2.2595  2.2520
[17 : 32]:	2.2490  2.2478  2.2222  2.2188  2.2146  2.0353  1.9623  1.9243  1.9198  1.9166  1.8727  1.8678  1.8649  1.8339  1.8301  1.8290
[33 : 48]:	1.8156  1.6180  1.6164  1.6016  1.5985  1.5944  1.5938  1.5784  1.5558  1.5402  1.5084  1.5041  1.4730  1.4568  1.4443  1.4345
[49 : 64]:	1.4208  1.4193  1.4132  1.4084  1.4051  1.4014  1.4013  1.3884  1.3881  1.3787  1.3786  1.3662  1.2993  1.2818  1.2811  1.2641
2024-04-29 21:39:51 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 21:39:51 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #193: GFLOPs: 2127.2052. Time: 21.7007 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #194: GFLOPs: 2559.9168. Time: 18.0326 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #195: GFLOPs: 2168.3540. Time: 21.2889 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #196: GFLOPs: 2405.6991. Time: 19.1886 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #197: GFLOPs: 2385.8821. Time: 19.3479 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #198: GFLOPs: 2166.1837. Time: 21.3103 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #199: GFLOPs: 2265.3970. Time: 20.3770 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #200: GFLOPs: 2107.1722. Time: 21.9070 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #201: GFLOPs: 2134.0845. Time: 21.6308 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #202: GFLOPs: 2117.6132. Time: 21.7990 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:121] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #203: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  279: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  278: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  277: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  276: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  275: tvm::transform::Pass::operator()(tvm::IRModule) const
  274: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  273: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  272: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  271: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  270: _ZN3tvm7runtime13PackedFun
  269: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  268: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  267: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorI
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorI
  260: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  259: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  258: _ZZN3tvm3tir11StmtFunctorI
  257: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  256: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  255: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  254: _ZZN3tvm3tir11StmtFunctorI
  253: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  252: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  251: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  250: _ZZN3tvm3tir11StmtFunctorI
  249: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  248: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  247: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  246: _ZZN3tvm3tir11StmtFunctorI
  245: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  244: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  243: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  242: _ZZN3tvm3tir11StmtFunctorI
  241: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorI
  237: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  236: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  235: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  234: _ZZN3tvm3tir11StmtFunctorI
  233: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  232: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  231: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  230: _ZZN3tvm3tir11StmtFunctorI
  229: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  228: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  227: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  226: _ZZN3tvm3tir11StmtFunctorI
  225: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  224: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  223: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  222: _ZZN3tvm3tir11StmtFunctorI
  221: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  220: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  219: _ZZN3tvm3tir11StmtFunctorI
  218: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  217: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  216: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  215: _ZZN3tvm3tir11StmtFunctorI
  214: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  213: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  212: _ZZN3tvm3tir11StmtFunctorI
  211: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  210: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  209: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  208: _ZZN3tvm3tir11StmtFunctorI
  207: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  206: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  205: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  204: _ZZN3tvm3tir11StmtFunctorI
  203: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  200: _ZZN3tvm3tir11StmtFunctorI
  199: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  198: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  197: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  196: _ZZN3tvm3tir11StmtFunctorI
  195: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  194: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  193: _ZZN3tvm3tir11StmtFunctorI
  192: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  191: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  190: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  189: _ZZN3tvm3tir11StmtFunctorI
  188: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  187: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  186: _ZZN3tvm3tir11StmtFunctorI
  185: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorI
  181: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  180: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  179: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  178: _ZZN3tvm3tir11StmtFunctorI
  177: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  176: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  175: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorI
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorI
  170: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  167: _ZZN3tvm3tir11StmtFunctorI
  166: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  165: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  164: _ZZN3tvm3tir11StmtFunctorI
  163: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  162: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorI
  160: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  159: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorI
  157: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  156: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorI
  154: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  151: _ZZN3tvm3tir11StmtFunctorI
  150: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  149: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  148: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorI
  146: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  145: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  144: _ZZN3tvm3tir11StmtFunctorI
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  141: _ZZN3tvm3tir11StmtFunctorI
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  138: _ZZN3tvm3tir11StmtFunctorI
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  135: _ZZN3tvm3tir11StmtFunctorI
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  132: _ZZN3tvm3tir11StmtFunctorI
  131: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  130: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  129: _ZZN3tvm3tir11StmtFunctorI
  128: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  127: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  126: _ZZN3tvm3tir11StmtFunctorIFNS
  125: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  124: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorI
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorI
  117: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  116: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  115: _ZZN3tvm3tir11StmtFunctorI
  114: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  113: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  112: _ZZN3tvm3tir11StmtFunctorI
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  109: _ZZN3tvm3tir11StmtFunctorI
  108: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  107: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorI
  105: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  104: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  103: _ZZN3tvm3tir11StmtFunctorI
  102: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  101: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  100: _ZZN3tvm3tir11StmtFunctorI
  99: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  98: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  97: _ZZN3tvm3tir11StmtFunctorI
  96: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  95: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS
  93: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  92: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorI
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorI
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorI
  82: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  81: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  80: _ZZN3tvm3tir11StmtFunctorI
  79: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  78: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  77: _ZZN3tvm3tir11StmtFunctorI
  76: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  75: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  74: _ZZN3tvm3tir11StmtFunctorI
  73: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  72: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  71: _ZZN3tvm3tir11StmtFunctorI
  70: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  69: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  68: _ZZN3tvm3tir11StmtFunctorI
  67: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  66: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  65: _ZZN3tvm3tir11StmtFunctorI
  64: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  63: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  62: _ZZN3tvm3tir11StmtFunctorI
  61: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  60: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  59: _ZZN3tvm3tir11StmtFunctorIFNS
  58: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  57: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorI
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorI
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorI
  47: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  46: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorI
  44: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  43: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  42: _ZZN3tvm3tir11StmtFunctorI
  41: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  40: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorI
  38: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  37: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  36: _ZZN3tvm3tir11StmtFunctorIFNS
  35: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  34: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorI
  30: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  29: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  28: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  27: _ZZN3tvm3tir11StmtFunctorI
  26: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  25: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  24: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorI
  22: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  21: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  20: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  19: _ZZN3tvm3tir11StmtFunctorI
  18: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AllocateNode const*)
  17: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  16: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  15: _ZZN3tvm3tir11StmtFunctorI
  14: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AllocateNode const*)
  13: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  12: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS
  10: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  8: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  7: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  6: _ZZN3tvm3tir11StmtFunctorIFNS
  5: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  4: _ZZN3tvm3tir11ExprFunctorI
  3: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  2: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  1: tvm::tir::APIType(tvm::runtime::DataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(392) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(392) // T.int64(98) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(32), eps_2_nu_2_co_2_p_2_fused // T.int64(2) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(98) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(392))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(392) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(98) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(392))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(392) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(392) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(392) // T.int64(98) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(32), eps_2_nu_2_co_2_p_2_fused // T.int64(2) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(98) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(128) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(392) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(392) // T.int64(98) + ax1)
                            v2 = T.axis.spatial(T.int64(32), eps_2_nu_2_co_2_p_2_fused // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(98) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(13), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1 < T.int64(6272))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1 < T.int64(6272))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1 < T.int64(6272))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                        T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                        conv2d_winograd[v_n, v_co, v_h, v_w] = inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
b5, b6 = sch.get_producers(block=b2)
sch.compute_inline(block=b6)
b7, = sch.get_consumers(block=b2)
l8, l9, l10, l11 = sch.get_loops(block=b7)
l12, l13 = sch.split(loop=l10, factors=[None, 2], preserve_unit_iters=True)
l14, l15 = sch.split(loop=l11, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l12, l14, l13, l15)
sch.compute_at(block=b2, loop=l14, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)
sch.unroll(loop=l22)
sch.unroll(loop=l23)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
b26, b27 = sch.get_producers(block=b0)
sch.compute_inline(block=b27)
b28, = sch.get_producers(block=b26)
l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b0)
sch.reorder(l31, l32, l29, l30, l33, l34)
sch.unroll(loop=l29)
sch.unroll(loop=l30)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
l35 = sch.fuse(l31, l32, preserve_unit_iters=True)
v36 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l37, l38 = sch.split(loop=l35, factors=[None, v36], preserve_unit_iters=True)
sch.bind(loop=l37, thread_axis="blockIdx.x")
sch.bind(loop=l38, thread_axis="threadIdx.x")
b39 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b39, loop=l38, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b26, loop=l38, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b26, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b28)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l40, l41, l42, l43, l44 = sch.get_loops(block=b1)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l40, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l50, l51, l52, l53, l54 = sch.split(loop=l40, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l41, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l60, l61, l62, l63, l64 = sch.split(loop=l41, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67, v68, v69 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 1])
l70, l71, l72, l73, l74 = sch.split(loop=l42, factors=[v65, v66, v67, v68, v69], preserve_unit_iters=True)
v75, v76, v77, v78, v79 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[98, 1, 2, 1, 1])
l80, l81, l82, l83, l84 = sch.split(loop=l43, factors=[v75, v76, v77, v78, v79], preserve_unit_iters=True)
v85, v86, v87 = sch.sample_perfect_tile(loop=l44, n=3, max_innermost_factor=64, decision=[1, 8, 16])
l88, l89, l90 = sch.split(loop=l44, factors=[v85, v86, v87], preserve_unit_iters=True)
sch.reorder(l50, l60, l70, l80, l51, l61, l71, l81, l52, l62, l72, l82, l88, l89, l53, l63, l73, l83, l90, l54, l64, l74, l84)
l91 = sch.fuse(l50, l60, l70, l80, preserve_unit_iters=True)
sch.bind(loop=l91, thread_axis="blockIdx.x")
l92 = sch.fuse(l51, l61, l71, l81, preserve_unit_iters=True)
sch.bind(loop=l92, thread_axis="vthread.x")
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b94 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b94, loop=l93, preserve_unit_loops=True, index=-1)
b95 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b95, loop=l88, preserve_unit_loops=True, index=-1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b95)
l104 = sch.fuse(l100, l101, l102, l103, preserve_unit_iters=True)
v105 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b95, ann_key="meta_schedule.cooperative_fetch", ann_val=v105)
b106 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b106, loop=l88, preserve_unit_loops=True, index=-1)
l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b106)
l115 = sch.fuse(l111, l112, l113, l114, preserve_unit_iters=True)
v116 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b106, ann_key="meta_schedule.cooperative_fetch", ann_val=v116)
v117 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v117)
l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b3)
l124 = sch.fuse(l118, l119, l120, l121, preserve_unit_iters=True)
v125 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l126, l127 = sch.split(loop=l124, factors=[None, v125], preserve_unit_iters=True)
sch.bind(loop=l126, thread_axis="blockIdx.x")
sch.bind(loop=l127, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b95, ann_key="meta_schedule.cooperative_fetch")
l128, l129, l130, l131, l132 = sch.get_loops(block=b95)
l133, l134, l135 = sch.split(loop=l132, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l135)
sch.bind(loop=l134, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b106, ann_key="meta_schedule.cooperative_fetch")
l136, l137, l138, l139, l140 = sch.get_loops(block=b106)
l141, l142, l143 = sch.split(loop=l140, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l143)
sch.bind(loop=l142, thread_axis="threadIdx.x")
b144 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b144, ann_key="meta_schedule.unroll_explicit")
b145, b146, b147, b148, b149, b150, b151, b152, b153 = sch.get_child_blocks(b144)
l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b145)
l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b146)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b147)
l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b148)
l179, l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b149)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b151)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b153)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #204: GFLOPs: 2096.3431. Time: 22.0202 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #205: GFLOPs: 2156.8053. Time: 21.4029 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #206: GFLOPs: 2231.3395. Time: 20.6880 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #207: GFLOPs: 2120.2355. Time: 21.7721 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #208: GFLOPs: 2169.9990. Time: 21.2728 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #209: GFLOPs: 2235.0404. Time: 20.6537 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #210: GFLOPs: 1730.3215. Time: 26.6782 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #211: GFLOPs: 2213.6629. Time: 20.8532 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #212: GFLOPs: 1638.8186. Time: 28.1678 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #213: GFLOPs: 2063.5322. Time: 22.3703 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #214: GFLOPs: 2583.0495. Time: 17.8711 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #215: GFLOPs: 2309.4971. Time: 19.9879 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #216: GFLOPs: 2317.1562. Time: 19.9218 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #217: GFLOPs: 2339.5349. Time: 19.7312 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #218: GFLOPs: 2375.2894. Time: 19.4342 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #219: GFLOPs: 2277.6012. Time: 20.2678 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #220: GFLOPs: 2314.4895. Time: 19.9448 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #221: GFLOPs: 2277.7468. Time: 20.2665 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #222: GFLOPs: 1897.8420. Time: 24.3234 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #223: GFLOPs: 2298.3972. Time: 20.0844 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #224: GFLOPs: 2326.1867. Time: 19.8445 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #225: GFLOPs: 1940.8444. Time: 23.7845 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #226: GFLOPs: 1957.5125. Time: 23.5819 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #227: GFLOPs: 2559.3935. Time: 18.0363 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #228: GFLOPs: 2539.3989. Time: 18.1783 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #229: GFLOPs: 2538.0004. Time: 18.1883 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #230: GFLOPs: 2457.2401. Time: 18.7861 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #231: GFLOPs: 1810.4526. Time: 25.4974 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #232: GFLOPs: 2296.8472. Time: 20.0979 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #233: GFLOPs: 2278.5079. Time: 20.2597 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #234: GFLOPs: 1738.5945. Time: 26.5513 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #235: GFLOPs: 2369.7859. Time: 19.4794 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #236: GFLOPs: 2205.0525. Time: 20.9346 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #237: GFLOPs: 2179.0467. Time: 21.1845 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #238: GFLOPs: 2466.8624. Time: 18.7128 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #239: GFLOPs: 1936.6432. Time: 23.8360 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #240: GFLOPs: 1609.1392. Time: 28.6873 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #241: GFLOPs: 2169.1289. Time: 21.2813 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #242: GFLOPs: 2233.1912. Time: 20.6708 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #243: GFLOPs: 2467.0666. Time: 18.7113 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #244: GFLOPs: 2214.6045. Time: 20.8443 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #245: GFLOPs: 2237.7827. Time: 20.6284 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #246: GFLOPs: 1584.0935. Time: 29.1409 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #247: GFLOPs: 1599.2423. Time: 28.8649 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #248: GFLOPs: 1837.6904. Time: 25.1195 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #249: GFLOPs: 2190.1303. Time: 21.0772 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #250: GFLOPs: 1967.7931. Time: 23.4587 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #251: GFLOPs: 1219.9696. Time: 37.8386 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #252: GFLOPs: 1846.2255. Time: 25.0034 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #253: GFLOPs: 2289.0869. Time: 20.1661 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #254: GFLOPs: 829.6484. Time: 55.6403 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #255: GFLOPs: 1977.5392. Time: 23.3431 us. Best GFLOPs: 2803.7217
2024-04-29 21:41:42 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #256: GFLOPs: 1271.6804. Time: 36.2999 us. Best GFLOPs: 2803.7217
2024-05-01 15:06:09 [INFO] [task_scheduler.cc:160] Initializing Task #186: "fused_nn_contrib_conv2d_winograd_without_weight_transform_2"
2024-05-01 15:06:09 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        data_pad = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(30), T.int64(30)))
        input_tile = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)))
        B = T.alloc_buffer((T.int64(4), T.int64(4)))
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
        A = T.alloc_buffer((T.int64(4), T.int64(2)))
        inverse = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(128), T.int64(30), T.int64(30)):
            with T.block("data_pad"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(data_pad[v_i0, v_i1, v_i2, v_i3])
                data_pad[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(29) and T.int64(1) <= v_i3 and v_i3 < T.int64(29), p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for ci, p, eps, nu in T.grid(T.int64(128), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("input_tile"):
                v_ci, v_p, v_eps, v_nu = T.axis.remap("SSSS", [ci, p, eps, nu])
                T.reads(data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu])
                T.writes(input_tile[v_ci, v_p, v_eps, v_nu])
                T.block_attr({"schedule_rule": "None"})
                input_tile[v_ci, v_p, v_eps, v_nu] = data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu]
        for i, j in T.grid(T.int64(4), T.int64(4)):
            with T.block("B"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(B[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                B[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
        for eps, nu, ci, p, r_a, r_b in T.grid(T.int64(4), T.int64(4), T.int64(128), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("data_pack"):
                v_eps, v_nu, v_ci, v_p, v_r_a, v_r_b = T.axis.remap("SSSSRR", [eps, nu, ci, p, r_a, r_b])
                T.reads(input_tile[v_ci, v_p, v_r_a, v_r_b], B[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_eps, v_nu):T.min(v_eps, v_nu) + (T.max(v_eps, v_nu) + T.int64(1) - T.min(v_eps, v_nu))])
                T.writes(data_pack[v_eps, v_nu, v_ci, v_p])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                with T.init():
                    data_pack[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                data_pack[v_eps, v_nu, v_ci, v_p] = data_pack[v_eps, v_nu, v_ci, v_p] + input_tile[v_ci, v_p, v_r_a, v_r_b] * B[v_r_a, v_eps] * B[v_r_b, v_nu]
        for eps, nu, co, p, ci in T.grid(T.int64(4), T.int64(4), T.int64(32), T.int64(196), T.int64(128)):
            with T.block("bgemm"):
                v_eps, v_nu, v_co, v_p, v_ci = T.axis.remap("SSSSR", [eps, nu, co, p, ci])
                T.reads(data_pack[v_eps, v_nu, v_ci, v_p], p1[v_eps, v_nu, v_ci, v_co])
                T.writes(bgemm[v_eps, v_nu, v_co, v_p])
                with T.init():
                    bgemm[v_eps, v_nu, v_co, v_p] = T.float32(0)
                bgemm[v_eps, v_nu, v_co, v_p] = bgemm[v_eps, v_nu, v_co, v_p] + data_pack[v_eps, v_nu, v_ci, v_p] * p1[v_eps, v_nu, v_ci, v_co]
        for i, j in T.grid(T.int64(4), T.int64(2)):
            with T.block("A"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(A[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                A[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
        for co, p, vh, vw, r_a, r_b in T.grid(T.int64(32), T.int64(196), T.int64(2), T.int64(2), T.int64(4), T.int64(4)):
            with T.block("inverse"):
                v_co, v_p, v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSSSRR", [co, p, vh, vw, r_a, r_b])
                T.reads(bgemm[v_r_a, v_r_b, v_co, v_p], A[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_vh, v_vw):T.min(v_vh, v_vw) + (T.max(v_vh, v_vw) + T.int64(1) - T.min(v_vh, v_vw))])
                T.writes(inverse[v_co, v_p, v_vh, v_vw])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                with T.init():
                    inverse[v_co, v_p, v_vh, v_vw] = T.float32(0)
                inverse[v_co, v_p, v_vh, v_vw] = inverse[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * A[v_r_a, v_vh] * A[v_r_b, v_vw]
        for n, co, h, w in T.grid(T.int64(1), T.int64(32), T.int64(28), T.int64(28)):
            with T.block("conv2d_winograd"):
                v_n, v_co, v_h, v_w = T.axis.remap("SSSS", [n, co, h, w])
                T.reads(inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                conv2d_winograd[v_n, v_co, v_h, v_w] = inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
2024-05-01 15:06:09 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-05-01 15:06:09 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for ci_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(6272)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1568))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(1568) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + ax0_ax1_ax2_ax3_fused % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(256)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(64) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(4))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                    v_co = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3 * T.int64(7) + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(16) + ci_1 * T.int64(16) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                            T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                            conv2d_winograd[v_n, v_co, v_h, v_w] = inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
b5, b6 = sch.get_producers(block=b2)
sch.compute_inline(block=b6)
b7, = sch.get_consumers(block=b2)
l8, l9, l10, l11 = sch.get_loops(block=b7)
l12, l13 = sch.split(loop=l10, factors=[None, 2], preserve_unit_iters=True)
l14, l15 = sch.split(loop=l11, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l12, l14, l13, l15)
sch.compute_at(block=b2, loop=l14, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)
sch.unroll(loop=l22)
sch.unroll(loop=l23)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
b26, b27 = sch.get_producers(block=b0)
sch.compute_inline(block=b27)
b28, = sch.get_producers(block=b26)
l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b0)
sch.reorder(l31, l32, l29, l30, l33, l34)
sch.unroll(loop=l29)
sch.unroll(loop=l30)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
l35 = sch.fuse(l31, l32, preserve_unit_iters=True)
v36 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l37, l38 = sch.split(loop=l35, factors=[None, v36], preserve_unit_iters=True)
sch.bind(loop=l37, thread_axis="blockIdx.x")
sch.bind(loop=l38, thread_axis="threadIdx.x")
b39 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b39, loop=l38, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b26, loop=l38, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b26, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b28)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l40, l41, l42, l43, l44 = sch.get_loops(block=b1)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l40, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l50, l51, l52, l53, l54 = sch.split(loop=l40, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l41, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l60, l61, l62, l63, l64 = sch.split(loop=l41, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67, v68, v69 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[8, 2, 2, 1, 1])
l70, l71, l72, l73, l74 = sch.split(loop=l42, factors=[v65, v66, v67, v68, v69], preserve_unit_iters=True)
v75, v76, v77, v78, v79 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 7])
l80, l81, l82, l83, l84 = sch.split(loop=l43, factors=[v75, v76, v77, v78, v79], preserve_unit_iters=True)
v85, v86, v87 = sch.sample_perfect_tile(loop=l44, n=3, max_innermost_factor=64, decision=[8, 1, 16])
l88, l89, l90 = sch.split(loop=l44, factors=[v85, v86, v87], preserve_unit_iters=True)
sch.reorder(l50, l60, l70, l80, l51, l61, l71, l81, l52, l62, l72, l82, l88, l89, l53, l63, l73, l83, l90, l54, l64, l74, l84)
l91 = sch.fuse(l50, l60, l70, l80, preserve_unit_iters=True)
sch.bind(loop=l91, thread_axis="blockIdx.x")
l92 = sch.fuse(l51, l61, l71, l81, preserve_unit_iters=True)
sch.bind(loop=l92, thread_axis="vthread.x")
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b94 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b94, loop=l93, preserve_unit_loops=True, index=-1)
b95 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b95, loop=l88, preserve_unit_loops=True, index=-1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b95)
l104 = sch.fuse(l100, l101, l102, l103, preserve_unit_iters=True)
v105 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b95, ann_key="meta_schedule.cooperative_fetch", ann_val=v105)
b106 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b106, loop=l88, preserve_unit_loops=True, index=-1)
l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b106)
l115 = sch.fuse(l111, l112, l113, l114, preserve_unit_iters=True)
v116 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b106, ann_key="meta_schedule.cooperative_fetch", ann_val=v116)
v117 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v117)
l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b3)
l124 = sch.fuse(l118, l119, l120, l121, preserve_unit_iters=True)
v125 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l126, l127 = sch.split(loop=l124, factors=[None, v125], preserve_unit_iters=True)
sch.bind(loop=l126, thread_axis="blockIdx.x")
sch.bind(loop=l127, thread_axis="threadIdx.x")
2024-05-01 15:06:09 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(6272)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1568))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(1568) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + ax0_ax1_ax2_ax3_fused % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(256)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(64) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(4))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                    v_co = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3 * T.int64(7) + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(16) + ci_1 * T.int64(16) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                            T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                            conv2d_winograd[v_n, v_co, v_h, v_w] = inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
b5, b6 = sch.get_producers(block=b2)
sch.compute_inline(block=b6)
b7, = sch.get_consumers(block=b2)
l8, l9, l10, l11 = sch.get_loops(block=b7)
l12, l13 = sch.split(loop=l10, factors=[None, 2], preserve_unit_iters=True)
l14, l15 = sch.split(loop=l11, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l12, l14, l13, l15)
sch.compute_at(block=b2, loop=l14, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)
sch.unroll(loop=l22)
sch.unroll(loop=l23)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
b26, b27 = sch.get_producers(block=b0)
sch.compute_inline(block=b27)
b28, = sch.get_producers(block=b26)
l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b0)
sch.reorder(l31, l32, l29, l30, l33, l34)
sch.unroll(loop=l29)
sch.unroll(loop=l30)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
l35 = sch.fuse(l31, l32, preserve_unit_iters=True)
v36 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l37, l38 = sch.split(loop=l35, factors=[None, v36], preserve_unit_iters=True)
sch.bind(loop=l37, thread_axis="blockIdx.x")
sch.bind(loop=l38, thread_axis="threadIdx.x")
b39 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b39, loop=l38, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b26, loop=l38, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b26, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b28)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l40, l41, l42, l43, l44 = sch.get_loops(block=b1)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l40, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l50, l51, l52, l53, l54 = sch.split(loop=l40, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l41, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l60, l61, l62, l63, l64 = sch.split(loop=l41, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67, v68, v69 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[8, 2, 2, 1, 1])
l70, l71, l72, l73, l74 = sch.split(loop=l42, factors=[v65, v66, v67, v68, v69], preserve_unit_iters=True)
v75, v76, v77, v78, v79 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 7])
l80, l81, l82, l83, l84 = sch.split(loop=l43, factors=[v75, v76, v77, v78, v79], preserve_unit_iters=True)
v85, v86, v87 = sch.sample_perfect_tile(loop=l44, n=3, max_innermost_factor=64, decision=[8, 1, 16])
l88, l89, l90 = sch.split(loop=l44, factors=[v85, v86, v87], preserve_unit_iters=True)
sch.reorder(l50, l60, l70, l80, l51, l61, l71, l81, l52, l62, l72, l82, l88, l89, l53, l63, l73, l83, l90, l54, l64, l74, l84)
l91 = sch.fuse(l50, l60, l70, l80, preserve_unit_iters=True)
sch.bind(loop=l91, thread_axis="blockIdx.x")
l92 = sch.fuse(l51, l61, l71, l81, preserve_unit_iters=True)
sch.bind(loop=l92, thread_axis="vthread.x")
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b94 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b94, loop=l93, preserve_unit_loops=True, index=-1)
b95 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b95, loop=l88, preserve_unit_loops=True, index=-1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b95)
l104 = sch.fuse(l100, l101, l102, l103, preserve_unit_iters=True)
v105 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b95, ann_key="meta_schedule.cooperative_fetch", ann_val=v105)
b106 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b106, loop=l88, preserve_unit_loops=True, index=-1)
l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b106)
l115 = sch.fuse(l111, l112, l113, l114, preserve_unit_iters=True)
v116 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b106, ann_key="meta_schedule.cooperative_fetch", ann_val=v116)
l117 = sch.fuse(l88, preserve_unit_iters=True)
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_async_stages", ann_val=[0])
v118 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v118)
l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b3)
l125 = sch.fuse(l119, l120, l121, l122, preserve_unit_iters=True)
v126 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l127, l128 = sch.split(loop=l125, factors=[None, v126], preserve_unit_iters=True)
sch.bind(loop=l127, thread_axis="blockIdx.x")
sch.bind(loop=l128, thread_axis="threadIdx.x")
2024-05-01 15:06:09 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), "float32"), conv2d_winograd: T.Buffer((T.int64(1), T.int64(32), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(32), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(32), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(32)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(6272)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1568))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(1568) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + ax0_ax1_ax2_ax3_fused % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(256)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(64) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(4))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                    v_co = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3 * T.int64(7) + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(16) + ci_1 * T.int64(16) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(16) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(32), eps_0_nu_0_co_0_p_0_fused % T.int64(16) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(32), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                            T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                            conv2d_winograd[v_n, v_co, v_h, v_w] = inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
b5, b6 = sch.get_producers(block=b2)
sch.compute_inline(block=b6)
b7, = sch.get_consumers(block=b2)
l8, l9, l10, l11 = sch.get_loops(block=b7)
l12, l13 = sch.split(loop=l10, factors=[None, 2], preserve_unit_iters=True)
l14, l15 = sch.split(loop=l11, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l12, l14, l13, l15)
sch.compute_at(block=b2, loop=l14, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25 = sch.get_loops(block=b2)
sch.unroll(loop=l22)
sch.unroll(loop=l23)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
b26, b27 = sch.get_producers(block=b0)
sch.compute_inline(block=b27)
b28, = sch.get_producers(block=b26)
l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b0)
sch.reorder(l31, l32, l29, l30, l33, l34)
sch.unroll(loop=l29)
sch.unroll(loop=l30)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
l35 = sch.fuse(l31, l32, preserve_unit_iters=True)
v36 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l37, l38 = sch.split(loop=l35, factors=[None, v36], preserve_unit_iters=True)
sch.bind(loop=l37, thread_axis="blockIdx.x")
sch.bind(loop=l38, thread_axis="threadIdx.x")
b39 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b39, loop=l38, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b26, loop=l38, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b26, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b28)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l40, l41, l42, l43, l44 = sch.get_loops(block=b1)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l40, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l50, l51, l52, l53, l54 = sch.split(loop=l40, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l41, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l60, l61, l62, l63, l64 = sch.split(loop=l41, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67, v68, v69 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[8, 2, 2, 1, 1])
l70, l71, l72, l73, l74 = sch.split(loop=l42, factors=[v65, v66, v67, v68, v69], preserve_unit_iters=True)
v75, v76, v77, v78, v79 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 7])
l80, l81, l82, l83, l84 = sch.split(loop=l43, factors=[v75, v76, v77, v78, v79], preserve_unit_iters=True)
v85, v86, v87 = sch.sample_perfect_tile(loop=l44, n=3, max_innermost_factor=64, decision=[8, 1, 16])
l88, l89, l90 = sch.split(loop=l44, factors=[v85, v86, v87], preserve_unit_iters=True)
sch.reorder(l50, l60, l70, l80, l51, l61, l71, l81, l52, l62, l72, l82, l88, l89, l53, l63, l73, l83, l90, l54, l64, l74, l84)
l91 = sch.fuse(l50, l60, l70, l80, preserve_unit_iters=True)
sch.bind(loop=l91, thread_axis="blockIdx.x")
l92 = sch.fuse(l51, l61, l71, l81, preserve_unit_iters=True)
sch.bind(loop=l92, thread_axis="vthread.x")
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b94 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b94, loop=l93, preserve_unit_loops=True, index=-1)
b95 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b95, loop=l88, preserve_unit_loops=True, index=-1)
l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b95)
l104 = sch.fuse(l100, l101, l102, l103, preserve_unit_iters=True)
v105 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b95, ann_key="meta_schedule.cooperative_fetch", ann_val=v105)
b106 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b106, loop=l88, preserve_unit_loops=True, index=-1)
l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b106)
l115 = sch.fuse(l111, l112, l113, l114, preserve_unit_iters=True)
v116 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b106, ann_key="meta_schedule.cooperative_fetch", ann_val=v116)
l117 = sch.fuse(l88, preserve_unit_iters=True)
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l117, ann_key="software_pipeline_async_stages", ann_val=[0])
v118 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v118)
l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b3)
l125 = sch.fuse(l119, l120, l121, l122, preserve_unit_iters=True)
v126 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l127, l128 = sch.split(loop=l125, factors=[None, v126], preserve_unit_iters=True)
sch.bind(loop=l127, thread_axis="blockIdx.x")
sch.bind(loop=l128, thread_axis="threadIdx.x")
2024-05-01 17:58:22 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-05-01 17:58:24 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-05-01 17:58:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 392 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 17:58:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 790 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 17:58:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 1187 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 17:58:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 1582 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 17:58:40 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2024-05-01 17:58:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 17:59:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 17:59:10 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 17:59:21 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 112 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 17:59:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9996  0.9996  0.9991  0.9978  0.9972  0.9964  0.9951  0.9921  0.9919  0.9918  0.9897  0.9896  0.9885  0.9863  0.9857  0.9855
[17 : 32]:	0.9849  0.9845  0.9841  0.9828  0.9795  0.9790  0.9783  0.9778  0.9777  0.9758  0.9758  0.9756  0.9746  0.9739  0.9733  0.9724
[33 : 48]:	0.9718  0.9715  0.9713  0.9689  0.9686  0.9676  0.9667  0.9665  0.9665  0.9662  0.9658  0.9657  0.9634  0.9631  0.9620  0.9600
[49 : 64]:	0.9591  0.9584  0.9581  0.9574  0.9573  0.9568  0.9565  0.9549  0.9546  0.9538  0.9532  0.9531  0.9516  0.9511  0.9508  0.9507
2024-05-01 17:59:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-05-01 17:59:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #1: GFLOPs: 1773.9530. Time: 26.0221 us. Best GFLOPs: 1773.9530
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #2: GFLOPs: 2127.7446. Time: 21.6952 us. Best GFLOPs: 2127.7446
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #3: GFLOPs: 2505.1616. Time: 18.4267 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #4: GFLOPs: 2497.9590. Time: 18.4799 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #5: GFLOPs: 18.1243. Time: 2546.9696 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #6: GFLOPs: 2384.8427. Time: 19.3564 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #7: GFLOPs: 945.3621. Time: 48.8299 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #8: GFLOPs: 2475.0897. Time: 18.6506 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #9: GFLOPs: 41.6530. Time: 1108.2493 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #10: GFLOPs: 1342.4543. Time: 34.3862 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #11: GFLOPs: 912.6074. Time: 50.5825 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #12: GFLOPs: 704.1008. Time: 65.5615 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #13: GFLOPs: 23.5836. Time: 1957.3760 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #14: GFLOPs: 1195.2582. Time: 38.6209 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #15: GFLOPs: 2134.5555. Time: 21.6260 us. Best GFLOPs: 2505.1616
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #16: GFLOPs: 2607.4079. Time: 17.7041 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #17: GFLOPs: 2601.8806. Time: 17.7418 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #18: GFLOPs: 114.5000. Time: 403.1609 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #19: GFLOPs: 2094.4315. Time: 22.0403 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #20: GFLOPs: 2206.3663. Time: 20.9221 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #21: GFLOPs: 1926.8144. Time: 23.9576 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #22: GFLOPs: 367.3102. Time: 125.6756 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #23: GFLOPs: 999.1278. Time: 46.2022 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #24: GFLOPs: 2063.1235. Time: 22.3748 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #25: GFLOPs: 126.7071. Time: 364.3198 us. Best GFLOPs: 2607.4079
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #26: GFLOPs: 2759.8824. Time: 16.7260 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #27: GFLOPs: 651.3744. Time: 70.8685 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #28: GFLOPs: 2313.4424. Time: 19.9538 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #29: GFLOPs: 2254.2740. Time: 20.4775 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #30: GFLOPs: 2530.3074. Time: 18.2436 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #31: GFLOPs: 2143.9136. Time: 21.5316 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #32: GFLOPs: 2394.0682. Time: 19.2818 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #33: GFLOPs: 2089.3811. Time: 22.0936 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #34: GFLOPs: 83.1644. Time: 555.0682 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #35: GFLOPs: 2599.2710. Time: 17.7596 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #36: GFLOPs: 64.9228. Time: 711.0276 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #37: GFLOPs: 945.3211. Time: 48.8320 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #38: GFLOPs: 2462.7220. Time: 18.7443 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #39: GFLOPs: 468.1988. Time: 98.5947 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #40: GFLOPs: 2271.8851. Time: 20.3188 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #41: GFLOPs: 1999.5971. Time: 23.0856 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #42: GFLOPs: 2358.3170. Time: 19.5741 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #43: GFLOPs: 1918.1407. Time: 24.0660 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #44: GFLOPs: 120.9737. Time: 381.5863 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #45: GFLOPs: 2196.8831. Time: 21.0125 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #46: GFLOPs: 751.6429. Time: 61.4147 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #47: GFLOPs: 868.7141. Time: 53.1382 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #48: GFLOPs: 1058.3874. Time: 43.6153 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #49: GFLOPs: 1081.9251. Time: 42.6665 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #50: GFLOPs: 246.2009. Time: 187.4970 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #51: GFLOPs: 2278.8626. Time: 20.2566 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #52: GFLOPs: 67.0241. Time: 688.7363 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #53: GFLOPs: 51.4633. Time: 896.9874 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #54: GFLOPs: 328.3724. Time: 140.5780 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #55: GFLOPs: 483.5505. Time: 95.4645 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #56: GFLOPs: 966.5075. Time: 47.7616 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #57: GFLOPs: 2004.7938. Time: 23.0258 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #58: GFLOPs: 2678.0835. Time: 17.2369 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #59: GFLOPs: 1387.4037. Time: 33.2722 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #60: GFLOPs: 1902.9459. Time: 24.2581 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #61: GFLOPs: 574.3213. Time: 80.3765 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #62: GFLOPs: 608.5604. Time: 75.8543 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #63: GFLOPs: 44.6879. Time: 1032.9838 us. Best GFLOPs: 2759.8824
2024-05-01 18:39:59 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #64: GFLOPs: 350.8125. Time: 131.5857 us. Best GFLOPs: 2759.8824
2024-05-01 18:53:15 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-05-01 18:53:17 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-05-01 18:53:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 393 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 18:53:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 785 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 18:53:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 1188 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 18:53:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 1585 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 18:53:33 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-05-01 18:53:44 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 114 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 18:53:58 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 18:54:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 18:54:26 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 152 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 18:54:30 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	2.0778  1.9688  1.9634  1.9581  1.9504  1.9266  1.9236  1.9236  1.9216  1.9146  1.9107  1.9051  1.8787  1.8355  1.8350  1.8021
[17 : 32]:	1.8021  1.8006  1.8005  1.7794  1.7787  1.7762  1.7729  1.7649  1.7633  1.7532  1.7403  1.7389  1.7383  1.7360  1.7227  1.7223
[33 : 48]:	1.7190  1.7054  1.7006  1.6953  1.6944  1.6897  1.6796  1.6783  1.6698  1.6672  1.6639  1.6636  1.6635  1.6619  1.6575  1.6531
[49 : 64]:	1.6526  1.6505  1.6503  1.6465  1.6461  1.6439  1.6431  1.6407  1.6397  1.6354  1.6352  1.6345  1.6339  1.6334  1.6314  1.6305
2024-05-01 18:54:30 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-05-01 18:54:30 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #65: GFLOPs: 2397.3713. Time: 19.2552 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #66: GFLOPs: 2437.4630. Time: 18.9385 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #67: GFLOPs: 2286.4158. Time: 20.1896 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #68: GFLOPs: 2258.6130. Time: 20.4382 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #69: GFLOPs: 2539.8132. Time: 18.1753 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #70: GFLOPs: 2099.3974. Time: 21.9882 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #71: GFLOPs: 2363.1694. Time: 19.5339 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #72: GFLOPs: 2363.3973. Time: 19.5320 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #73: GFLOPs: 2236.7895. Time: 20.6376 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #74: GFLOPs: 2346.8030. Time: 19.6701 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #75: GFLOPs: 2093.6919. Time: 22.0481 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #76: GFLOPs: 2106.3500. Time: 21.9156 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #77: GFLOPs: 1872.2920. Time: 24.6553 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #78: GFLOPs: 2184.3918. Time: 21.1326 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #79: GFLOPs: 1704.5165. Time: 27.0821 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #80: GFLOPs: 2377.1557. Time: 19.4190 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #81: GFLOPs: 2377.5106. Time: 19.4161 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #82: GFLOPs: 2353.8958. Time: 19.6109 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #83: GFLOPs: 2068.7258. Time: 22.3142 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #84: GFLOPs: 2499.3976. Time: 18.4692 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #85: GFLOPs: 2532.9037. Time: 18.2249 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #86: GFLOPs: 2536.5571. Time: 18.1987 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #87: GFLOPs: 2390.3563. Time: 19.3117 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #88: GFLOPs: 2336.4882. Time: 19.7570 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #89: GFLOPs: 2077.6558. Time: 22.2183 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #90: GFLOPs: 2095.1259. Time: 22.0330 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #91: GFLOPs: 2202.1642. Time: 20.9621 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #92: GFLOPs: 2291.5014. Time: 20.1448 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #93: GFLOPs: 2308.9301. Time: 19.9928 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #94: GFLOPs: 2284.3697. Time: 20.2077 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #95: GFLOPs: 2099.3985. Time: 21.9882 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #96: GFLOPs: 2201.6826. Time: 20.9667 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #97: GFLOPs: 2306.9291. Time: 20.0101 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #98: GFLOPs: 2291.2139. Time: 20.1474 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #99: GFLOPs: 2321.1039. Time: 19.8879 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #100: GFLOPs: 2324.0876. Time: 19.8624 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #101: GFLOPs: 2297.5256. Time: 20.0920 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #102: GFLOPs: 2343.1300. Time: 19.7010 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #103: GFLOPs: 2598.8036. Time: 17.7628 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #104: GFLOPs: 2051.3574. Time: 22.5031 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #105: GFLOPs: 2598.7618. Time: 17.7630 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #106: GFLOPs: 2291.3856. Time: 20.1459 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #107: GFLOPs: 2374.6094. Time: 19.4398 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #108: GFLOPs: 2330.7097. Time: 19.8060 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #109: GFLOPs: 2443.5797. Time: 18.8911 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #110: GFLOPs: 2279.0315. Time: 20.2551 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #111: GFLOPs: 1385.1198. Time: 33.3270 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #112: GFLOPs: 2063.1886. Time: 22.3741 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #113: GFLOPs: 2048.8604. Time: 22.5305 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #114: GFLOPs: 2288.2708. Time: 20.1733 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #115: GFLOPs: 2330.5281. Time: 19.8075 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #116: GFLOPs: 2282.0211. Time: 20.2285 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #117: GFLOPs: 2383.4795. Time: 19.3675 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #118: GFLOPs: 2212.4435. Time: 20.8647 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #119: GFLOPs: 2624.0598. Time: 17.5918 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #120: GFLOPs: 2621.1172. Time: 17.6115 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #121: GFLOPs: 2101.9099. Time: 21.9619 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #122: GFLOPs: 2299.0738. Time: 20.0785 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #123: GFLOPs: 2252.8624. Time: 20.4903 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #124: GFLOPs: 2325.2040. Time: 19.8528 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #125: GFLOPs: 2265.7001. Time: 20.3742 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #126: GFLOPs: 114.0191. Time: 404.8614 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #127: GFLOPs: 336.9075. Time: 137.0166 us. Best GFLOPs: 2759.8824
2024-05-01 18:56:13 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #128: GFLOPs: 712.7872. Time: 64.7626 us. Best GFLOPs: 2759.8824
2024-05-01 19:07:54 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-05-01 19:07:58 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-05-01 19:08:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:08:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 796 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:08:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 1187 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:08:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 1583 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:08:13 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-05-01 19:08:24 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 119 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:08:37 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 96 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:08:50 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 89 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:09:03 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 79 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:09:07 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	2.4116  2.1960  2.1504  2.1401  2.1183  2.1182  2.0841  2.0832  2.0271  2.0260  1.9940  1.9935  1.9481  1.9377  1.9281  1.9277
[17 : 32]:	1.9251  1.9229  1.9168  1.9040  1.8990  1.8949  1.8786  1.8774  1.8592  1.8578  1.8570  1.8543  1.8468  1.8462  1.8450  1.8437
[33 : 48]:	1.8355  1.8348  1.8290  1.8224  1.8219  1.8138  1.8133  1.8094  1.7901  1.7881  1.7858  1.7857  1.7846  1.7845  1.7733  1.7669
[49 : 64]:	1.7626  1.7585  1.7568  1.7553  1.7515  1.7515  1.7512  1.7414  1.7374  1.7361  1.7352  1.7346  1.7346  1.7333  1.7293  1.7288
2024-05-01 19:09:08 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-05-01 19:09:08 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #129: GFLOPs: 1426.1725. Time: 32.3677 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #130: GFLOPs: 931.6856. Time: 49.5467 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #131: GFLOPs: 932.7436. Time: 49.4905 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #132: GFLOPs: 1823.5200. Time: 25.3147 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #133: GFLOPs: 1631.0393. Time: 28.3021 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #134: GFLOPs: 942.9211. Time: 48.9563 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #135: GFLOPs: 1027.6384. Time: 44.9204 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #136: GFLOPs: 2044.7500. Time: 22.5758 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #137: GFLOPs: 1984.4749. Time: 23.2615 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #138: GFLOPs: 1644.9634. Time: 28.0626 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #139: GFLOPs: 2286.0953. Time: 20.1925 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #140: GFLOPs: 2634.5113. Time: 17.5220 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #141: GFLOPs: 1711.2736. Time: 26.9752 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #142: GFLOPs: 990.2967. Time: 46.6142 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #143: GFLOPs: 712.7995. Time: 64.7614 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #144: GFLOPs: 2452.0119. Time: 18.8261 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #145: GFLOPs: 959.1289. Time: 48.1290 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #146: GFLOPs: 1061.1238. Time: 43.5029 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #147: GFLOPs: 2026.1211. Time: 22.7834 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #148: GFLOPs: 2038.1439. Time: 22.6490 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #149: GFLOPs: 1062.9870. Time: 43.4266 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #150: GFLOPs: 990.5416. Time: 46.6027 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #151: GFLOPs: 872.4422. Time: 52.9112 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #152: GFLOPs: 787.9310. Time: 58.5862 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #153: GFLOPs: 1100.9726. Time: 41.9283 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #154: GFLOPs: 2008.8004. Time: 22.9798 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #155: GFLOPs: 2071.4067. Time: 22.2853 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #156: GFLOPs: 1985.5504. Time: 23.2489 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #157: GFLOPs: 740.6696. Time: 62.3246 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #158: GFLOPs: 1769.9398. Time: 26.0811 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #159: GFLOPs: 730.9886. Time: 63.1500 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #160: GFLOPs: 2352.8241. Time: 19.6198 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #161: GFLOPs: 1870.1437. Time: 24.6836 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #162: GFLOPs: 1840.0254. Time: 25.0877 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #163: GFLOPs: 1648.2545. Time: 28.0065 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #164: GFLOPs: 2125.0519. Time: 21.7227 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #165: GFLOPs: 2083.4142. Time: 22.1569 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #166: GFLOPs: 1027.5520. Time: 44.9242 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #167: GFLOPs: 1024.9464. Time: 45.0384 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #168: GFLOPs: 740.7793. Time: 62.3153 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #169: GFLOPs: 808.8506. Time: 57.0710 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #170: GFLOPs: 1796.0548. Time: 25.7018 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #171: GFLOPs: 733.6552. Time: 62.9205 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #172: GFLOPs: 1573.8123. Time: 29.3313 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #173: GFLOPs: 2002.5226. Time: 23.0519 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #174: GFLOPs: 2002.4911. Time: 23.0522 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #175: GFLOPs: 2403.6337. Time: 19.2051 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #176: GFLOPs: 716.9937. Time: 64.3826 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #177: GFLOPs: 2470.4994. Time: 18.6853 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #178: GFLOPs: 2465.2509. Time: 18.7250 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #179: GFLOPs: 767.8259. Time: 60.1203 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #180: GFLOPs: 1999.8350. Time: 23.0829 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #181: GFLOPs: 984.5583. Time: 46.8859 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #182: GFLOPs: 2388.8602. Time: 19.3238 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #183: GFLOPs: 741.6446. Time: 62.2426 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #184: GFLOPs: 984.8077. Time: 46.8740 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #185: GFLOPs: 897.9250. Time: 51.4095 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #186: GFLOPs: 1600.0112. Time: 28.8510 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #187: GFLOPs: 986.5063. Time: 46.7933 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #188: GFLOPs: 2435.7901. Time: 18.9515 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #189: GFLOPs: 1756.6023. Time: 26.2791 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #190: GFLOPs: 162.7757. Time: 283.5922 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #191: GFLOPs: 1331.6424. Time: 34.6654 us. Best GFLOPs: 2759.8824
2024-05-01 19:10:29 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #192: GFLOPs: 678.0251. Time: 68.0829 us. Best GFLOPs: 2759.8824
2024-05-01 19:31:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-05-01 19:32:01 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-05-01 19:32:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 397 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:32:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 793 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:32:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 1190 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:32:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 1589 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:32:15 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-05-01 19:32:26 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:32:39 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 98 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:32:52 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:33:05 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xba8b148)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122b9228)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe093fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaa9ccc8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x12c2f728)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x14fbfa78)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xea64838)]: 0 failure(s)
2024-05-01 19:33:09 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	2.8084  2.7701  2.7306  2.7306  2.7275  2.7146  2.6960  2.6960  2.6960  2.6910  2.6840  2.6520  2.6520  2.6353  2.6213  2.6213
[17 : 32]:	2.6213  2.6166  2.6166  2.6166  2.6022  2.5269  2.4745  2.4183  2.4110  2.3596  2.2801  2.2523  2.2290  2.2228  2.2163  2.1931
[33 : 48]:	2.1859  2.1786  2.1192  2.1128  2.1057  2.1057  2.1037  2.1036  2.0757  2.0700  2.0677  2.0645  2.0645  2.0601  2.0588  2.0530
[49 : 64]:	2.0424  2.0409  2.0222  2.0215  2.0150  2.0111  2.0063  2.0025  1.9960  1.9779  1.9758  1.9726  1.9726  1.9715  1.9713  1.9706
2024-05-01 19:33:09 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-05-01 19:33:09 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #193: GFLOPs: 1502.6777. Time: 30.7198 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #194: GFLOPs: 1743.5670. Time: 26.4756 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #195: GFLOPs: 1469.2157. Time: 31.4194 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #196: GFLOPs: 1469.2015. Time: 31.4197 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #197: GFLOPs: 1735.3187. Time: 26.6014 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #198: GFLOPs: 1502.6865. Time: 30.7196 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #199: GFLOPs: 1502.6962. Time: 30.7194 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #200: GFLOPs: 1502.6962. Time: 30.7194 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #201: GFLOPs: 1502.7551. Time: 30.7182 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #202: GFLOPs: 1502.6775. Time: 30.7198 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #203: GFLOPs: 1484.3799. Time: 31.0985 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #204: GFLOPs: 1695.1007. Time: 27.2326 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #205: GFLOPs: 1694.7658. Time: 27.2379 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #206: GFLOPs: 1810.0805. Time: 25.5027 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #207: GFLOPs: 1743.5659. Time: 26.4756 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #208: GFLOPs: 1743.5753. Time: 26.4754 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #209: GFLOPs: 1743.3679. Time: 26.4786 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #210: GFLOPs: 1810.0854. Time: 25.5026 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #211: GFLOPs: 1809.9034. Time: 25.5052 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #212: GFLOPs: 1809.8821. Time: 25.5055 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #213: GFLOPs: 1810.1212. Time: 25.5021 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #214: GFLOPs: 1719.7500. Time: 26.8422 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #215: GFLOPs: 1445.0083. Time: 31.9458 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #216: GFLOPs: 1553.6805. Time: 29.7113 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #217: GFLOPs: 1571.5562. Time: 29.3734 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #218: GFLOPs: 1567.8577. Time: 29.4427 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #219: GFLOPs: 1685.4151. Time: 27.3891 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #220: GFLOPs: 1877.7144. Time: 24.5841 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #221: GFLOPs: 1482.9691. Time: 31.1280 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #222: GFLOPs: 1443.0584. Time: 31.9889 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #223: GFLOPs: 2620.9386. Time: 17.6127 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #224: GFLOPs: 1500.7727. Time: 30.7588 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #225: GFLOPs: 1254.7067. Time: 36.7910 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #226: GFLOPs: 1500.7917. Time: 30.7584 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #227: GFLOPs: 1611.5210. Time: 28.6449 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #228: GFLOPs: 1346.3813. Time: 34.2859 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #229: GFLOPs: 1345.7110. Time: 34.3030 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #230: GFLOPs: 1345.6849. Time: 34.3037 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #231: GFLOPs: 1731.9793. Time: 26.6527 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #232: GFLOPs: 1440.2623. Time: 32.0510 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #233: GFLOPs: 1568.4897. Time: 29.4308 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #234: GFLOPs: 1564.3267. Time: 29.5091 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #235: GFLOPs: 1445.5013. Time: 31.9349 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #236: GFLOPs: 1751.6678. Time: 26.3531 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #237: GFLOPs: 1751.6910. Time: 26.3528 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #238: GFLOPs: 1809.9658. Time: 25.5043 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #239: GFLOPs: 1212.1473. Time: 38.0828 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #240: GFLOPs: 1568.0052. Time: 29.4399 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #241: GFLOPs: 2241.2672. Time: 20.5963 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #242: GFLOPs: 2347.5018. Time: 19.6643 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #243: GFLOPs: 1481.5233. Time: 31.1584 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #244: GFLOPs: 1234.0860. Time: 37.4058 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #245: GFLOPs: 2360.2353. Time: 19.5582 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #246: GFLOPs: 1449.5625. Time: 31.8454 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #247: GFLOPs: 2313.9948. Time: 19.9490 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #248: GFLOPs: 1563.1080. Time: 29.5321 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #249: GFLOPs: 1095.1534. Time: 42.1511 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #250: GFLOPs: 1388.0370. Time: 33.2570 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #251: GFLOPs: 1017.8433. Time: 45.3527 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #252: GFLOPs: 1677.9222. Time: 27.5114 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #253: GFLOPs: 1677.2252. Time: 27.5228 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #254: GFLOPs: 851.7744. Time: 54.1950 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #255: GFLOPs: 1183.1611. Time: 39.0158 us. Best GFLOPs: 2759.8824
2024-05-01 19:34:53 [INFO] [task_scheduler.cc:131] [Task #186: fused_nn_contrib_conv2d_winograd_without_weight_transform_2] Trial #256: GFLOPs: 369.9427. Time: 124.7813 us. Best GFLOPs: 2759.8824
