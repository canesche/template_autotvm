2024-04-29 16:32:34 [INFO] [task_scheduler.cc:160] Initializing Task #201: "fused_nn_max_pool2d"
2024-04-29 16:32:34 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(114), T.int64(114)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(114), T.int64(114)):
            with T.block("pad_temp"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(p0[v_ax0, v_ax1, v_ax2 - T.int64(1), v_ax3 - T.int64(1)])
                T.writes(pad_temp[v_ax0, v_ax1, v_ax2, v_ax3])
                pad_temp[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1) <= v_ax2 and v_ax2 < T.int64(113) and T.int64(1) <= v_ax3 and v_ax3 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 - T.int64(1), v_ax3 - T.int64(1)], T.float32(-3.4028234663852886e+38))
        for ax0, ax1, ax2, ax3, rv0, rv1 in T.grid(T.int64(1), T.int64(64), T.int64(56), T.int64(56), T.int64(3), T.int64(3)):
            with T.block("pool_max"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_rv0, v_rv1 = T.axis.remap("SSSSRR", [ax0, ax1, ax2, ax3, rv0, rv1])
                T.reads(pad_temp[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1])
                T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                T.block_attr({"schedule_rule": "meta_schedule.pool_max"})
                with T.init():
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], pad_temp[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1])
2024-04-29 16:32:34 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2024-04-29 16:32:34 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(6272), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                        with T.block("pool_max"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                            v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                            v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                            v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                            T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
2024-04-29 16:32:34 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
                for rv0_rv1_fused_0 in range(T.int64(1)):
                    for rv0_rv1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                        with T.block("pool_max"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                            v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                            v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                            v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) // T.int64(3))
                            v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) % T.int64(3))
                            T.where(rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1 < T.int64(9))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                            T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
2024-04-29 19:58:41 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 19:58:41 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-29 19:58:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x13331868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122ad4c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xd57ed78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb5266a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xb2a3408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x1336e468)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xd57f1e8)]: 0 failure(s)
2024-04-29 19:58:44 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-04-29 19:58:45 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x13331868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122ad4c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xd57ed78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb5266a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xb2a3408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x1336e468)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xd57f1e8)]: 0 failure(s)
2024-04-29 19:58:46 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x13331868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122ad4c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xd57ed78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb5266a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xb2a3408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x1336e468)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xd57f1e8)]: 0 failure(s)
2024-04-29 19:58:47 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x13331868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122ad4c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xd57ed78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb5266a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xb2a3408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x1336e468)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xd57f1e8)]: 0 failure(s)
2024-04-29 19:58:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x13331868)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x122ad4c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xd57ed78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb5266a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xb2a3408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x1336e468)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xd57f1e8)]: 0 failure(s)
2024-04-29 19:58:47 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9967  0.9789  0.9495  0.9294  0.9251  0.9222  0.9012  0.8970  0.8745  0.8514  0.8507  0.8463  0.8251  0.8180  0.7655  0.7573
[17 : 32]:	0.7488  0.7455  0.7360  0.7220  0.7010  0.6962  0.6903  0.6451  0.5798  0.5796  0.5758  0.5268  0.5190  0.5019  0.4798  0.4665
[33 : 48]:	0.4664  0.4597  0.4554  0.4357  0.3986  0.3961  0.3897  0.3594  0.3104  0.2948  0.2883  0.2736  0.2604  0.2435  0.2412  0.2255
[49 : 64]:	0.2251  0.2197  0.2167  0.2059  0.2016  0.1939  0.1920  0.1805  0.1714  0.1664  0.1543  0.1471  0.1377  0.1374  0.1259  0.1234
2024-04-29 19:58:48 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 19:58:48 [INFO] [evolutionary_search.cc:730] Sending 61 candidates(s) for measurement
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #1: GFLOPs: 346.7334. Time: 5.2096 us. Best GFLOPs: 346.7334
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #2: GFLOPs: 11.8810. Time: 152.0360 us. Best GFLOPs: 346.7334
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #3: GFLOPs: 11.8930. Time: 151.8826 us. Best GFLOPs: 346.7334
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #4: GFLOPs: 11.8284. Time: 152.7112 us. Best GFLOPs: 346.7334
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #5: GFLOPs: 236.3840. Time: 7.6415 us. Best GFLOPs: 346.7334
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #6: GFLOPs: 11.9113. Time: 151.6485 us. Best GFLOPs: 346.7334
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #7: GFLOPs: 236.4084. Time: 7.6407 us. Best GFLOPs: 346.7334
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #8: GFLOPs: 383.3497. Time: 4.7120 us. Best GFLOPs: 383.3497
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #9: GFLOPs: 11.8916. Time: 151.8998 us. Best GFLOPs: 383.3497
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #10: GFLOPs: 11.9181. Time: 151.5627 us. Best GFLOPs: 383.3497
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #11: GFLOPs: 406.2201. Time: 4.4467 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #12: GFLOPs: 4.2329. Time: 426.7411 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #13: GFLOPs: 11.9195. Time: 151.5452 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #14: GFLOPs: 11.8783. Time: 152.0699 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #15: GFLOPs: 11.9101. Time: 151.6647 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #16: GFLOPs: 236.3821. Time: 7.6416 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #17: GFLOPs: 403.0181. Time: 4.4820 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #18: GFLOPs: 8.5712. Time: 210.7451 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #19: GFLOPs: 11.8784. Time: 152.0694 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #20: GFLOPs: 383.8660. Time: 4.7056 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #21: GFLOPs: 11.8806. Time: 152.0406 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #22: GFLOPs: 11.8819. Time: 152.0242 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #23: GFLOPs: 11.8927. Time: 151.8862 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #24: GFLOPs: 383.4611. Time: 4.7106 us. Best GFLOPs: 406.2201
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #25: GFLOPs: 418.3217. Time: 4.3181 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #26: GFLOPs: 4.2255. Time: 427.4873 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #27: GFLOPs: 240.3940. Time: 7.5141 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #28: GFLOPs: 11.8807. Time: 152.0396 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #29: GFLOPs: 383.6508. Time: 4.7083 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #30: GFLOPs: 8.5557. Time: 211.1278 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #31: GFLOPs: 11.9234. Time: 151.4957 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #32: GFLOPs: 383.8543. Time: 4.7058 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #33: GFLOPs: 11.8696. Time: 152.1814 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #34: GFLOPs: 347.5066. Time: 5.1980 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #35: GFLOPs: 396.0415. Time: 4.5610 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #36: GFLOPs: 11.8791. Time: 152.0601 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #37: GFLOPs: 236.3064. Time: 7.6440 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #38: GFLOPs: 345.7431. Time: 5.2245 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #39: GFLOPs: 11.9218. Time: 151.5160 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #40: GFLOPs: 4.2268. Time: 427.3484 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #41: GFLOPs: 11.8783. Time: 152.0698 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #42: GFLOPs: 4.2330. Time: 426.7248 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #43: GFLOPs: 406.1289. Time: 4.4477 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #44: GFLOPs: 4.2253. Time: 427.5022 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #45: GFLOPs: 11.9200. Time: 151.5381 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #46: GFLOPs: 396.7275. Time: 4.5531 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #47: GFLOPs: 11.9096. Time: 151.6709 us. Best GFLOPs: 418.3217
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #48: GFLOPs: 432.1511. Time: 4.1799 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #49: GFLOPs: 383.3727. Time: 4.7117 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #50: GFLOPs: 11.8786. Time: 152.0669 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #51: GFLOPs: 383.3897. Time: 4.7115 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #52: GFLOPs: 347.5712. Time: 5.1970 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #53: GFLOPs: 11.8921. Time: 151.8940 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #54: GFLOPs: 11.8799. Time: 152.0494 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #55: GFLOPs: 396.6973. Time: 4.5534 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #56: GFLOPs: 11.8818. Time: 152.0255 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #57: GFLOPs: 11.8813. Time: 152.0323 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #58: GFLOPs: 347.5820. Time: 5.1969 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #59: GFLOPs: 8.5723. Time: 210.7189 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #60: GFLOPs: 11.9108. Time: 151.6558 us. Best GFLOPs: 432.1511
2024-04-29 20:34:28 [INFO] [task_scheduler.cc:131] [Task #201: fused_nn_max_pool2d] Trial #61: GFLOPs: 8.5573. Time: 211.0864 us. Best GFLOPs: 432.1511
2024-05-01 15:06:15 [INFO] [task_scheduler.cc:160] Initializing Task #201: "fused_nn_max_pool2d"
2024-05-01 15:06:15 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(114), T.int64(114)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(114), T.int64(114)):
            with T.block("pad_temp"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(p0[v_ax0, v_ax1, v_ax2 - T.int64(1), v_ax3 - T.int64(1)])
                T.writes(pad_temp[v_ax0, v_ax1, v_ax2, v_ax3])
                pad_temp[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1) <= v_ax2 and v_ax2 < T.int64(113) and T.int64(1) <= v_ax3 and v_ax3 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 - T.int64(1), v_ax3 - T.int64(1)], T.float32(-3.4028234663852886e+38))
        for ax0, ax1, ax2, ax3, rv0, rv1 in T.grid(T.int64(1), T.int64(64), T.int64(56), T.int64(56), T.int64(3), T.int64(3)):
            with T.block("pool_max"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_rv0, v_rv1 = T.axis.remap("SSSSRR", [ax0, ax1, ax2, ax3, rv0, rv1])
                T.reads(pad_temp[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1])
                T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                T.block_attr({"schedule_rule": "meta_schedule.pool_max"})
                with T.init():
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], pad_temp[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1])
2024-05-01 15:06:15 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2024-05-01 15:06:15 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                        with T.block("pool_max"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                            v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                            v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                            v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                            T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
2024-05-01 15:06:15 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 1024})
            for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
                for rv0_rv1_fused_0 in range(T.int64(3)):
                    for rv0_rv1_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                        with T.block("pool_max"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                            v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                            v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                            v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) // T.int64(3))
                            v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) % T.int64(3))
                            T.where(rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1 < T.int64(9))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                            T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
2024-05-01 18:16:36 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-05-01 18:16:37 [INFO] [evolutionary_search.cc:715] Picked top 61 candidate(s) from database
2024-05-01 18:16:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb58d308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf014a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x145daa68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xe00aed8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd39eb48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x11e95e68)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x2eb4368)]: 0 failure(s)
2024-05-01 18:16:38 [INFO] [evolutionary_search.cc:723] Sampled 451 candidate(s)
2024-05-01 18:16:39 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb58d308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf014a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x145daa68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xe00aed8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd39eb48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x11e95e68)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x2eb4368)]: 0 failure(s)
2024-05-01 18:16:40 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb58d308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf014a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x145daa68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xe00aed8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd39eb48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x11e95e68)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x2eb4368)]: 0 failure(s)
2024-05-01 18:16:40 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb58d308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf014a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x145daa68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xe00aed8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd39eb48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x11e95e68)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x2eb4368)]: 0 failure(s)
2024-05-01 18:16:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb58d308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf014a28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x145daa68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xe00aed8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd39eb48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x11e95e68)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x2eb4368)]: 0 failure(s)
2024-05-01 18:16:41 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9961  0.9933  0.9525  0.9404  0.9252  0.9010  0.8765  0.8696  0.8565  0.8482  0.8256  0.8054  0.7938  0.7922  0.7855  0.7388
[17 : 32]:	0.7382  0.7377  0.7062  0.6889  0.6868  0.6085  0.5834  0.5806  0.5761  0.5707  0.5139  0.5076  0.4839  0.4801  0.4702  0.4647
[33 : 48]:	0.4318  0.4133  0.4095  0.4032  0.3960  0.3764  0.3336  0.3314  0.3104  0.3025  0.2910  0.2824  0.2795  0.2620  0.2493  0.2467
[49 : 64]:	0.2465  0.2246  0.2192  0.1982  0.1743  0.1725  0.1696  0.1674  0.1670  0.1619  0.1477  0.1348  0.1289  0.1147  0.1089  0.0990
2024-05-01 18:16:41 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-05-01 18:16:41 [INFO] [evolutionary_search.cc:730] Sending 62 candidates(s) for measurement
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #1: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #2: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(3)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #3: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #4: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #5: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #6: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #7: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #8: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #9: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #10: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
            for rv0_rv1_fused_0 in range(T.int64(2)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #11: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(6272), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #12: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #13: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #14: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #15: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(3)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #16: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(3)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #17: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #18: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #19: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(6272), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #20: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #21: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #22: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #23: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #24: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(6272), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #25: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #26: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #27: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #28: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #29: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #30: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #31: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
            for rv0_rv1_fused_0 in range(T.int64(3)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #32: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #33: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #34: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #35: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #36: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(128) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #37: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #38: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #39: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #40: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #41: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #42: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #43: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #44: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #45: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #46: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #47: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #48: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(32) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #49: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(64) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #50: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #51: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #52: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #53: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(2)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #54: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(2)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #55: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(6272), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #56: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
sch.enter_postproc()
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18, l19 = sch.get_loops(block=b15)
b20 = sch.get_block(name="pool_max", func_name="main")
l21, l22, l23, l24 = sch.get_loops(block=b20)
b25 = sch.decompose_reduction(block=b20, loop=l23)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #57: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(256) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #58: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(2)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(8) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #59: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(512) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #60: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #61: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for rv0_rv1_fused_0 in range(T.int64(3)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(4) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
2024-05-01 18:41:06 [INFO] [task_scheduler.cc:121] [Task #201: fused_nn_max_pool2d] Trial #62: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), pool_max: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_ax3_fused in T.thread_binding(T.int64(200704), thread="blockIdx.x"):
            for rv0_rv1_fused_0 in range(T.int64(1)):
                for rv0_rv1_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_fused % T.int64(56))
                        v_rv0 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1) % T.int64(3))
                        T.where(rv0_rv1_fused_0 * T.int64(16) + rv0_rv1_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9, preserve_unit_iters=True)
l11, l12 = sch.split(loop=l10, factors=[None, v3], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
v13 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v13)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.enter_postproc()
b21 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b21, ann_key="meta_schedule.unroll_explicit")
b22, = sch.get_child_blocks(b21)
l23, l24, l25 = sch.get_loops(block=b22)
