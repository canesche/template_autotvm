2024-04-29 16:32:37 [INFO] [task_scheduler.cc:160] Initializing Task #211: "fused_nn_conv2d_2"
2024-04-29 16:32:37 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1792), T.int64(14), T.int64(14)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(896), T.int64(14), T.int64(14), T.int64(1792), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1[v_ff, v_rc, v_ry, v_rx])
                T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1[v_ff, v_rc, v_ry, v_rx]
2024-04-29 16:32:37 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-29 16:32:37 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(2), thread="threadIdx.x"):
                        for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(43904)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(14336)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused % T.int64(224))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(28), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(14), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ff_3 * T.int64(8) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), yy_3 * T.int64(14) + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1792), rc_0 * T.int64(224) + rc_1 * T.int64(28) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(14), T.int64(7)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ax1)
                                v2 = T.axis.spatial(T.int64(14), ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14], preserve_unit_iters=True)
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[14, 1, 2, 4, 8])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24], preserve_unit_iters=True)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34], preserve_unit_iters=True)
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44], preserve_unit_iters=True)
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[8, 8, 28])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52], preserve_unit_iters=True)
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58], preserve_unit_iters=True)
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64], preserve_unit_iters=True)
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45, preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True, index=-1)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True, index=-1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82, preserve_unit_iters=True)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True, index=-1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95, preserve_unit_iters=True)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
2024-04-29 16:32:37 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(2), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(43904)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(224) + ax0_ax1_ax2_ax3_fused // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(14336)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(224) + ax0_ax1_ax2_ax3_fused % T.int64(224))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(28), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(14), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ff_3 * T.int64(8) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), yy_3 * T.int64(14) + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(224) + rc_1 * T.int64(28) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(14), T.int64(7)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ax1)
                                v2 = T.axis.spatial(T.int64(14), ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14], preserve_unit_iters=True)
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[14, 1, 2, 4, 8])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24], preserve_unit_iters=True)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34], preserve_unit_iters=True)
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44], preserve_unit_iters=True)
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[8, 8, 28])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52], preserve_unit_iters=True)
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58], preserve_unit_iters=True)
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64], preserve_unit_iters=True)
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45, preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True, index=-1)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True, index=-1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82, preserve_unit_iters=True)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True, index=-1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95, preserve_unit_iters=True)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
l98 = sch.fuse(l53, l59, l65, preserve_unit_iters=True)
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
2024-04-29 16:32:37 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(2), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(43904)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(224) + ax0_ax1_ax2_ax3_fused // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(14336)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(224) + ax0_ax1_ax2_ax3_fused % T.int64(224))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(28), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(14), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ff_3 * T.int64(8) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), yy_3 * T.int64(14) + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(224) + rc_1 * T.int64(28) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(14), T.int64(7)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ax1)
                                v2 = T.axis.spatial(T.int64(14), ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused * T.int64(7) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14], preserve_unit_iters=True)
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[14, 1, 2, 4, 8])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24], preserve_unit_iters=True)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34], preserve_unit_iters=True)
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44], preserve_unit_iters=True)
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[8, 8, 28])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52], preserve_unit_iters=True)
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58], preserve_unit_iters=True)
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64], preserve_unit_iters=True)
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45, preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True, index=-1)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True, index=-1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82, preserve_unit_iters=True)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True, index=-1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95, preserve_unit_iters=True)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
l98 = sch.fuse(l53, l59, l65, preserve_unit_iters=True)
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
2024-04-29 20:09:49 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 20:09:49 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-29 20:09:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 496 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 20:09:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 988 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 20:09:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 1494 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 20:09:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 1991 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 20:09:55 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-04-29 20:09:58 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 141 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 20:10:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 20:10:02 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 120 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 20:10:04 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 116 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 20:10:04 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9999  0.9996  0.9981  0.9966  0.9962  0.9936  0.9935  0.9927  0.9916  0.9905  0.9903  0.9901  0.9888  0.9887  0.9886  0.9883
[17 : 32]:	0.9877  0.9877  0.9876  0.9869  0.9863  0.9855  0.9839  0.9835  0.9830  0.9822  0.9799  0.9782  0.9772  0.9771  0.9753  0.9751
[33 : 48]:	0.9736  0.9722  0.9686  0.9679  0.9675  0.9670  0.9668  0.9666  0.9659  0.9657  0.9645  0.9641  0.9627  0.9619  0.9614  0.9612
[49 : 64]:	0.9607  0.9601  0.9593  0.9587  0.9581  0.9574  0.9570  0.9568  0.9567  0.9561  0.9541  0.9514  0.9501  0.9485  0.9483  0.9458
2024-04-29 20:10:05 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 20:10:05 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #1: GFLOPs: 2486.2839. Time: 253.1520 us. Best GFLOPs: 2486.2839
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:121] [Task #211: fused_nn_conv2d_2] Trial #2: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(896), nn_1_ff_1_yy_1_xx_1_fused * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(16) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(448), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(7) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(896), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(3584))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(896), nn_1_ff_1_yy_1_xx_1_fused * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(16) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(2), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(896), nn_1_ff_1_yy_1_xx_1_fused * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_0_ff_0_yy_0_xx_0_fused * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                            T.writes(conv2d_nchw[v0, v1, v2, v3])
                            conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14], preserve_unit_iters=True)
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 8, 7, 4, 4])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24], preserve_unit_iters=True)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34], preserve_unit_iters=True)
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44], preserve_unit_iters=True)
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[448, 4, 1])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52], preserve_unit_iters=True)
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58], preserve_unit_iters=True)
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64], preserve_unit_iters=True)
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45, preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True, index=-1)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True, index=-1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82, preserve_unit_iters=True)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True, index=-1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95, preserve_unit_iters=True)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
l98 = sch.fuse(l53, l59, l65, preserve_unit_iters=True)
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104 = sch.get_loops(block=b72)
l105, l106, l107 = sch.split(loop=l104, factors=[None, 49, 2], preserve_unit_iters=True)
sch.vectorize(loop=l107)
sch.bind(loop=l106, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch")
l108, l109, l110, l111, l112 = sch.get_loops(block=b85)
l113, l114, l115 = sch.split(loop=l112, factors=[None, 49, 4], preserve_unit_iters=True)
sch.vectorize(loop=l115)
sch.bind(loop=l114, thread_axis="threadIdx.x")
b116 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b116, ann_key="meta_schedule.unroll_explicit")
b117, b118, b119, b120 = sch.get_child_blocks(b116)
l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b117)
l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b118)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b120)
b160 = sch.get_block(name="conv2d_nchw", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b160)
b179 = sch.decompose_reduction(block=b160, loop=l164)
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #3: GFLOPs: 562.9692. Time: 1118.0146 us. Best GFLOPs: 2486.2839
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #4: GFLOPs: 2773.1246. Time: 226.9670 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #5: GFLOPs: 460.6916. Time: 1366.2236 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #6: GFLOPs: 2030.4785. Time: 309.9800 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #7: GFLOPs: 2463.3392. Time: 255.5100 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #8: GFLOPs: 1993.7415. Time: 315.6918 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #9: GFLOPs: 1562.0972. Time: 402.9248 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #10: GFLOPs: 584.1842. Time: 1077.4132 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #11: GFLOPs: 1302.3782. Time: 483.2757 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #12: GFLOPs: 579.3730. Time: 1086.3602 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #13: GFLOPs: 421.1135. Time: 1494.6273 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #14: GFLOPs: 2482.9680. Time: 253.4901 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #15: GFLOPs: 2215.0753. Time: 284.1473 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #16: GFLOPs: 695.4115. Time: 905.0868 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:121] [Task #211: fused_nn_conv2d_2] Trial #17: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(28), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(896), nn_2_ff_2_yy_2_xx_2_fused * T.int64(28) + ff_3_init * T.int64(28) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(1792), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused)
                                    v2 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(14))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(196))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(28)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(896), ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused)
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(28), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(896), nn_2_ff_2_yy_2_xx_2_fused * T.int64(28) + ff_3 * T.int64(28) + ff_4)
                                v_yy = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(1792), rc_0_ry_0_rx_0_fused + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(28), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(896), nn_2_ff_2_yy_2_xx_2_fused * T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(14), nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                            T.writes(conv2d_nchw[v0, v1, v2, v3])
                            conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14], preserve_unit_iters=True)
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 28])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24], preserve_unit_iters=True)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34], preserve_unit_iters=True)
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44], preserve_unit_iters=True)
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[1792, 1, 1])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52], preserve_unit_iters=True)
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58], preserve_unit_iters=True)
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64], preserve_unit_iters=True)
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45, preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True, index=-1)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True, index=-1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82, preserve_unit_iters=True)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True, index=-1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95, preserve_unit_iters=True)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
l98 = sch.fuse(l53, l59, l65, preserve_unit_iters=True)
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104 = sch.get_loops(block=b72)
l105, l106 = sch.split(loop=l104, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l106, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch")
l107, l108, l109, l110, l111 = sch.get_loops(block=b85)
l112, l113 = sch.split(loop=l111, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l113, thread_axis="threadIdx.x")
b114 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b114, ann_key="meta_schedule.unroll_explicit")
b115, b116, b117, b118 = sch.get_child_blocks(b114)
l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b115)
l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b116)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148 = sch.get_loops(block=b117)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l149, l150, l151, l152, l153, l154, l155 = sch.get_loops(block=b118)
b156 = sch.get_block(name="conv2d_nchw", func_name="main")
l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b156)
b175 = sch.decompose_reduction(block=b156, loop=l160)
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #18: GFLOPs: 1215.8134. Time: 517.6845 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #19: GFLOPs: 8.3450. Time: 75423.7413 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #20: GFLOPs: 2217.1678. Time: 283.8792 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #21: GFLOPs: 23.8461. Time: 26394.6245 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #22: GFLOPs: 606.5026. Time: 1037.7660 us. Best GFLOPs: 2773.1246
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #23: GFLOPs: 2785.4680. Time: 225.9612 us. Best GFLOPs: 2785.4680
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #24: GFLOPs: 1424.6671. Time: 441.7929 us. Best GFLOPs: 2785.4680
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #25: GFLOPs: 969.8883. Time: 648.9487 us. Best GFLOPs: 2785.4680
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #26: GFLOPs: 370.3012. Time: 1699.7185 us. Best GFLOPs: 2785.4680
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #27: GFLOPs: 2639.5235. Time: 238.4551 us. Best GFLOPs: 2785.4680
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #28: GFLOPs: 377.7945. Time: 1666.0057 us. Best GFLOPs: 2785.4680
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #29: GFLOPs: 5931.8465. Time: 106.1065 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #30: GFLOPs: 4717.7105. Time: 133.4138 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #31: GFLOPs: 23.5952. Time: 26675.1995 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #32: GFLOPs: 1971.5648. Time: 319.2427 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #33: GFLOPs: 950.7982. Time: 661.9782 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #34: GFLOPs: 193.7775. Time: 3248.0948 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #35: GFLOPs: 848.7206. Time: 741.5960 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #36: GFLOPs: 586.1273. Time: 1073.8414 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #37: GFLOPs: 1647.5573. Time: 382.0248 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #38: GFLOPs: 1100.5874. Time: 571.8835 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #39: GFLOPs: 2647.5056. Time: 237.7361 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #40: GFLOPs: 4493.0878. Time: 140.0836 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #41: GFLOPs: 2380.0246. Time: 264.4543 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #42: GFLOPs: 2072.9397. Time: 303.6305 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #43: GFLOPs: 380.4220. Time: 1654.4986 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #44: GFLOPs: 776.8843. Time: 810.1692 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #45: GFLOPs: 698.8942. Time: 900.5765 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #46: GFLOPs: 472.5445. Time: 1331.9546 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #47: GFLOPs: 33.3542. Time: 18870.4427 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #48: GFLOPs: 97.7749. Time: 6437.3121 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #49: GFLOPs: 625.3049. Time: 1006.5613 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #50: GFLOPs: 368.4614. Time: 1708.2055 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #51: GFLOPs: 19.9064. Time: 31618.3033 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #52: GFLOPs: 816.7685. Time: 770.6073 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #53: GFLOPs: 151.7909. Time: 4146.5445 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #54: GFLOPs: 1340.2851. Time: 469.6074 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #55: GFLOPs: 304.0149. Time: 2070.3190 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #56: GFLOPs: 57.4434. Time: 10957.0045 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #57: GFLOPs: 1235.7520. Time: 509.3318 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #58: GFLOPs: 802.7505. Time: 784.0640 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #59: GFLOPs: 1618.6448. Time: 388.8486 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #60: GFLOPs: 1338.3208. Time: 470.2966 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #61: GFLOPs: 3050.1549. Time: 206.3527 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #62: GFLOPs: 508.8420. Time: 1236.9414 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #63: GFLOPs: 427.5780. Time: 1472.0301 us. Best GFLOPs: 5931.8465
2024-04-29 20:35:32 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #64: GFLOPs: 39.8447. Time: 15796.5163 us. Best GFLOPs: 5931.8465
2024-04-29 21:03:06 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 21:03:07 [INFO] [evolutionary_search.cc:715] Picked top 62 candidate(s) from database
2024-04-29 21:03:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 431 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 21:03:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 867 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 21:03:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 1299 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 21:03:12 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-04-29 21:03:15 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 118 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 21:03:18 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 110 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 21:03:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 21:03:25 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x147360a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x12005788)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xdb323e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x4386688)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xbd0fe38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xf5725a8)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x120052f8)]: 0 failure(s)
2024-04-29 21:03:26 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5411  1.5242  1.5215  1.4711  1.4687  1.4687  1.4607  1.4582  1.4558  1.4558  1.4558  1.4558  1.4496  1.4448  1.4431  1.4431
[17 : 32]:	1.4431  1.4395  1.4370  1.4210  1.4203  1.4181  1.4168  1.4069  1.4032  1.3867  1.3814  1.3812  1.3809  1.3796  1.3775  1.3756
[33 : 48]:	1.3736  1.3736  1.3731  1.3697  1.3692  1.3653  1.3633  1.3627  1.3605  1.3591  1.3586  1.3582  1.3538  1.3529  1.3517  1.3512
[49 : 64]:	1.3434  1.3430  1.3429  1.3422  1.3412  1.3411  1.3399  1.3390  1.3382  1.3363  1.3350  1.3346  1.3338  1.3329  1.3328  1.3323
2024-04-29 21:03:26 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 21:03:26 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #65: GFLOPs: 3419.2295. Time: 184.0788 us. Best GFLOPs: 5931.8465
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #66: GFLOPs: 4415.1001. Time: 142.5580 us. Best GFLOPs: 5931.8465
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #67: GFLOPs: 4496.0402. Time: 139.9916 us. Best GFLOPs: 5931.8465
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #68: GFLOPs: 7796.5807. Time: 80.7287 us. Best GFLOPs: 7796.5807
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #69: GFLOPs: 7797.6978. Time: 80.7171 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #70: GFLOPs: 7795.6333. Time: 80.7385 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #71: GFLOPs: 6952.4590. Time: 90.5302 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #72: GFLOPs: 7728.6309. Time: 81.4385 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #73: GFLOPs: 7727.3144. Time: 81.4523 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #74: GFLOPs: 7726.0725. Time: 81.4654 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #75: GFLOPs: 7728.3084. Time: 81.4419 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #76: GFLOPs: 7728.3835. Time: 81.4411 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #77: GFLOPs: 7727.9448. Time: 81.4457 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #78: GFLOPs: 7531.6045. Time: 83.5689 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #79: GFLOPs: 7664.8076. Time: 82.1166 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #80: GFLOPs: 7675.6630. Time: 82.0004 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #81: GFLOPs: 7664.0695. Time: 82.1245 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #82: GFLOPs: 7475.8856. Time: 84.1917 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #83: GFLOPs: 7664.9025. Time: 82.1156 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #84: GFLOPs: 7504.8900. Time: 83.8664 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #85: GFLOPs: 5325.0484. Time: 118.1976 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #86: GFLOPs: 7572.5326. Time: 83.1172 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #87: GFLOPs: 7491.2177. Time: 84.0194 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #88: GFLOPs: 5324.6993. Time: 118.2053 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #89: GFLOPs: 7592.2372. Time: 82.9015 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #90: GFLOPs: 6575.9623. Time: 95.7134 us. Best GFLOPs: 7797.6978
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #91: GFLOPs: 7841.1751. Time: 80.2696 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #92: GFLOPs: 7697.8160. Time: 81.7645 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #93: GFLOPs: 7378.2472. Time: 85.3059 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #94: GFLOPs: 7523.6383. Time: 83.6574 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #95: GFLOPs: 7564.2122. Time: 83.2086 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #96: GFLOPs: 7830.7795. Time: 80.3761 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #97: GFLOPs: 7385.6318. Time: 85.2206 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #98: GFLOPs: 5546.1373. Time: 113.4858 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #99: GFLOPs: 7570.4262. Time: 83.1403 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #100: GFLOPs: 5756.8644. Time: 109.3317 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #101: GFLOPs: 3143.4991. Time: 200.2252 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #102: GFLOPs: 6066.3340. Time: 103.7542 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #103: GFLOPs: 6494.1455. Time: 96.9193 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #104: GFLOPs: 3603.6564. Time: 174.6581 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #105: GFLOPs: 7575.3392. Time: 83.0864 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #106: GFLOPs: 5910.3474. Time: 106.4925 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #107: GFLOPs: 5522.7591. Time: 113.9662 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #108: GFLOPs: 7592.7362. Time: 82.8960 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #109: GFLOPs: 7582.6126. Time: 83.0067 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #110: GFLOPs: 5885.8346. Time: 106.9360 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #111: GFLOPs: 6097.4244. Time: 103.2252 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #112: GFLOPs: 7717.0662. Time: 81.5605 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #113: GFLOPs: 6522.4700. Time: 96.4984 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #114: GFLOPs: 6417.8511. Time: 98.0714 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #115: GFLOPs: 5841.4088. Time: 107.7493 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #116: GFLOPs: 5835.9448. Time: 107.8502 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #117: GFLOPs: 5922.8262. Time: 106.2681 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #118: GFLOPs: 5699.8983. Time: 110.4244 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #119: GFLOPs: 6541.5135. Time: 96.2174 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #120: GFLOPs: 4514.7052. Time: 139.4128 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #121: GFLOPs: 7634.3456. Time: 82.4442 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #122: GFLOPs: 7710.8223. Time: 81.6265 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #123: GFLOPs: 7700.0430. Time: 81.7408 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #124: GFLOPs: 5446.6044. Time: 115.5597 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #125: GFLOPs: 7791.9424. Time: 80.7767 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #126: GFLOPs: 450.7377. Time: 1396.3946 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #127: GFLOPs: 2752.2847. Time: 228.6856 us. Best GFLOPs: 7841.1751
2024-04-29 21:05:28 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #128: GFLOPs: 5109.7797. Time: 123.1771 us. Best GFLOPs: 7841.1751
2024-05-01 15:06:18 [INFO] [task_scheduler.cc:160] Initializing Task #211: "fused_nn_conv2d_2"
2024-05-01 15:06:18 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(1792), T.int64(14), T.int64(14)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(896), T.int64(14), T.int64(14), T.int64(1792), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1[v_ff, v_rc, v_ry, v_rx])
                T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1[v_ff, v_rc, v_ry, v_rx]
2024-05-01 15:06:18 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-05-01 15:06:18 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(7), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(1), T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(10976)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(7168)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + ax0_ax1_ax2_ax3_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused % T.int64(56))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(2)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ff_3 * T.int64(16) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) * T.int64(7) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1792), rc_0 * T.int64(56) + rc_1 * T.int64(8) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(7), T.int64(2)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) * T.int64(7) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14], preserve_unit_iters=True)
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[7, 1, 2, 4, 16])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24], preserve_unit_iters=True)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34], preserve_unit_iters=True)
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44], preserve_unit_iters=True)
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[32, 7, 8])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52], preserve_unit_iters=True)
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58], preserve_unit_iters=True)
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64], preserve_unit_iters=True)
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45, preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True, index=-1)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True, index=-1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82, preserve_unit_iters=True)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True, index=-1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95, preserve_unit_iters=True)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
2024-05-01 15:06:18 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(7), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(10976)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(56) + ax0_ax1_ax2_ax3_fused // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(7168)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + ax0_ax1_ax2_ax3_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(56) + ax0_ax1_ax2_ax3_fused % T.int64(56))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(2)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ff_3 * T.int64(16) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) * T.int64(7) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(56) + rc_1 * T.int64(8) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(7), T.int64(2)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) * T.int64(7) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14], preserve_unit_iters=True)
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[7, 1, 2, 4, 16])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24], preserve_unit_iters=True)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34], preserve_unit_iters=True)
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44], preserve_unit_iters=True)
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[32, 7, 8])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52], preserve_unit_iters=True)
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58], preserve_unit_iters=True)
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64], preserve_unit_iters=True)
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45, preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True, index=-1)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True, index=-1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82, preserve_unit_iters=True)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True, index=-1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95, preserve_unit_iters=True)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
l98 = sch.fuse(l53, l59, l65, preserve_unit_iters=True)
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
2024-05-01 15:06:18 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), "float32"), p1: T.Buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), "float32"), conv2d_nchw: T.Buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(896), T.int64(14), T.int64(14)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(1792), T.int64(14), T.int64(14)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(896), T.int64(1792), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(7), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(28), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(10976)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(56) + ax0_ax1_ax2_ax3_fused // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(196) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(14), ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(7168)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + ax0_ax1_ax2_ax3_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(56) + ax0_ax1_ax2_ax3_fused % T.int64(56))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(2)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ff_3 * T.int64(16) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) * T.int64(7) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + xx_3 * T.int64(2) + xx_4)
                                    v_rc = T.axis.reduce(T.int64(1792), rc_0_ry_0_rx_0_fused * T.int64(56) + rc_1 * T.int64(8) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(7), T.int64(2)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(896), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(14) * T.int64(64) + ax1)
                                v2 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) // T.int64(7) * T.int64(7) + ax2)
                                v3 = T.axis.spatial(T.int64(14), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(2) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14], preserve_unit_iters=True)
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[7, 1, 2, 4, 16])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24], preserve_unit_iters=True)
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34], preserve_unit_iters=True)
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44], preserve_unit_iters=True)
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[32, 7, 8])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52], preserve_unit_iters=True)
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58], preserve_unit_iters=True)
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64], preserve_unit_iters=True)
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45, preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True, index=-1)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True, index=-1)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82, preserve_unit_iters=True)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True, index=-1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95, preserve_unit_iters=True)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
l98 = sch.fuse(l53, l59, l65, preserve_unit_iters=True)
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l98, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
2024-05-01 18:24:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-05-01 18:24:10 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-05-01 18:24:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 397 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 794 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 1192 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 1591 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 1983 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:15 [INFO] [evolutionary_search.cc:723] Sampled 67 candidate(s)
2024-05-01 18:24:18 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 105 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:22 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 98 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:24 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 18:24:24 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9997  0.9976  0.9970  0.9970  0.9950  0.9948  0.9944  0.9943  0.9938  0.9932  0.9906  0.9904  0.9901  0.9889  0.9886  0.9886
[17 : 32]:	0.9872  0.9871  0.9851  0.9849  0.9845  0.9836  0.9833  0.9819  0.9815  0.9806  0.9779  0.9773  0.9772  0.9768  0.9767  0.9763
[33 : 48]:	0.9763  0.9763  0.9759  0.9756  0.9746  0.9738  0.9734  0.9724  0.9724  0.9717  0.9717  0.9699  0.9698  0.9689  0.9679  0.9672
[49 : 64]:	0.9663  0.9650  0.9646  0.9633  0.9632  0.9625  0.9616  0.9612  0.9607  0.9597  0.9583  0.9573  0.9568  0.9552  0.9549  0.9548
2024-05-01 18:24:24 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-05-01 18:24:24 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #1: GFLOPs: 1679.4739. Time: 374.7648 us. Best GFLOPs: 1679.4739
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #2: GFLOPs: 1787.4502. Time: 352.1260 us. Best GFLOPs: 1787.4502
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #3: GFLOPs: 1660.3485. Time: 379.0817 us. Best GFLOPs: 1787.4502
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #4: GFLOPs: 928.6729. Time: 677.7496 us. Best GFLOPs: 1787.4502
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #5: GFLOPs: 5318.1342. Time: 118.3512 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #6: GFLOPs: 889.9157. Time: 707.2667 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #7: GFLOPs: 1278.1222. Time: 492.4472 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #8: GFLOPs: 490.0461. Time: 1284.3848 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #9: GFLOPs: 3951.1597. Time: 159.2970 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #10: GFLOPs: 578.6017. Time: 1087.8084 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #11: GFLOPs: 3552.8570. Time: 177.1554 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #12: GFLOPs: 80.1256. Time: 7855.2615 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #13: GFLOPs: 800.0030. Time: 786.7567 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #14: GFLOPs: 2263.6675. Time: 278.0478 us. Best GFLOPs: 5318.1342
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #15: GFLOPs: 7776.1694. Time: 80.9406 us. Best GFLOPs: 7776.1694
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #16: GFLOPs: 4532.6831. Time: 138.8599 us. Best GFLOPs: 7776.1694
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #17: GFLOPs: 428.9859. Time: 1467.1990 us. Best GFLOPs: 7776.1694
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #18: GFLOPs: 1822.8974. Time: 345.2788 us. Best GFLOPs: 7776.1694
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #19: GFLOPs: 3008.2260. Time: 209.2289 us. Best GFLOPs: 7776.1694
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #20: GFLOPs: 3144.0004. Time: 200.1933 us. Best GFLOPs: 7776.1694
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #21: GFLOPs: 211.8558. Time: 2970.9253 us. Best GFLOPs: 7776.1694
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #22: GFLOPs: 6246.5444. Time: 100.7610 us. Best GFLOPs: 7776.1694
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #23: GFLOPs: 7847.5336. Time: 80.2045 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #24: GFLOPs: 6571.0730. Time: 95.7846 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #25: GFLOPs: 1599.3124. Time: 393.5490 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #26: GFLOPs: 7754.4629. Time: 81.1672 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #27: GFLOPs: 179.8056. Time: 3500.4909 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #28: GFLOPs: 156.6125. Time: 4018.8848 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #29: GFLOPs: 1119.1224. Time: 562.4119 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #30: GFLOPs: 1473.9485. Time: 427.0215 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #31: GFLOPs: 7724.8451. Time: 81.4784 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #32: GFLOPs: 417.8025. Time: 1506.4721 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #33: GFLOPs: 4914.9278. Time: 128.0604 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #34: GFLOPs: 1826.6443. Time: 344.5705 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #35: GFLOPs: 1057.2020. Time: 595.3524 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #36: GFLOPs: 7736.0795. Time: 81.3600 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #37: GFLOPs: 617.4580. Time: 1019.3531 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #38: GFLOPs: 118.2246. Time: 5323.8300 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #39: GFLOPs: 9.9266. Time: 63406.4227 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #40: GFLOPs: 3903.2571. Time: 161.2519 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #41: GFLOPs: 2343.5256. Time: 268.5730 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #42: GFLOPs: 1208.0082. Time: 521.0293 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #43: GFLOPs: 2721.2198. Time: 231.2962 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #44: GFLOPs: 1908.3837. Time: 329.8119 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #45: GFLOPs: 3483.2702. Time: 180.6945 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #46: GFLOPs: 4938.3847. Time: 127.4522 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #47: GFLOPs: 126.1202. Time: 4990.5370 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #48: GFLOPs: 917.4814. Time: 686.0169 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #49: GFLOPs: 4655.2882. Time: 135.2027 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #50: GFLOPs: 11.9927. Time: 52482.3913 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #51: GFLOPs: 58.4724. Time: 10764.1853 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #52: GFLOPs: 2925.2978. Time: 215.1602 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #53: GFLOPs: 111.0189. Time: 5669.3759 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #54: GFLOPs: 165.5367. Time: 3802.2257 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #55: GFLOPs: 575.1635. Time: 1094.3109 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #56: GFLOPs: 1268.8130. Time: 496.0603 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #57: GFLOPs: 7602.8152. Time: 82.7861 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #58: GFLOPs: 134.0687. Time: 4694.6675 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #59: GFLOPs: 50.1044. Time: 12561.9201 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #60: GFLOPs: 490.0461. Time: 1284.3848 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #61: GFLOPs: 771.8892. Time: 815.4120 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #62: GFLOPs: 299.2392. Time: 2103.3600 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #63: GFLOPs: 384.5593. Time: 1636.6988 us. Best GFLOPs: 7847.5336
2024-05-01 18:41:47 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #64: GFLOPs: 4593.0366. Time: 137.0352 us. Best GFLOPs: 7847.5336
2024-05-01 19:19:48 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-05-01 19:19:49 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-05-01 19:19:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 19:19:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 794 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 19:19:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 1194 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 19:19:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 1588 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 19:19:55 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-05-01 19:19:57 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 19:20:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 98 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 19:20:03 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 117 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 19:20:06 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xe5fa428)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xf012878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xee52448)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xaf60398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0xd1e0048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x13801998)]: 99 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xf0105f8)]: 0 failure(s)
2024-05-01 19:20:07 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5426  1.5239  1.4924  1.4893  1.4500  1.4382  1.4382  1.4274  1.4274  1.4266  1.4240  1.4240  1.4149  1.4148  1.4145  1.4133
[17 : 32]:	1.4133  1.4123  1.4115  1.4109  1.4086  1.4066  1.4060  1.4049  1.4049  1.4049  1.4048  1.4014  1.4006  1.3998  1.3990  1.3966
[33 : 48]:	1.3960  1.3946  1.3946  1.3946  1.3941  1.3941  1.3933  1.3929  1.3926  1.3897  1.3896  1.3854  1.3833  1.3827  1.3781  1.3737
[49 : 64]:	1.3726  1.3714  1.3707  1.3699  1.3698  1.3681  1.3580  1.3578  1.3545  1.3507  1.3470  1.3470  1.3465  1.3438  1.3396  1.3394
2024-05-01 19:20:07 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-05-01 19:20:07 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #65: GFLOPs: 6376.9912. Time: 98.6998 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #66: GFLOPs: 6384.6484. Time: 98.5814 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #67: GFLOPs: 6474.4696. Time: 97.2138 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #68: GFLOPs: 6530.8978. Time: 96.3738 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #69: GFLOPs: 7543.0449. Time: 83.4421 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #70: GFLOPs: 7474.0193. Time: 84.2128 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #71: GFLOPs: 7484.9226. Time: 84.0901 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #72: GFLOPs: 7525.0562. Time: 83.6416 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #73: GFLOPs: 7525.5839. Time: 83.6357 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #74: GFLOPs: 7283.0148. Time: 86.4213 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #75: GFLOPs: 7478.2004. Time: 84.1657 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #76: GFLOPs: 7475.2167. Time: 84.1993 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #77: GFLOPs: 6384.7701. Time: 98.5795 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #78: GFLOPs: 7253.6026. Time: 86.7717 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #79: GFLOPs: 6384.5253. Time: 98.5833 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #80: GFLOPs: 6814.4822. Time: 92.3633 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #81: GFLOPs: 7526.5904. Time: 83.6246 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #82: GFLOPs: 2121.1873. Time: 296.7243 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #83: GFLOPs: 2121.6052. Time: 296.6658 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #84: GFLOPs: 6850.9271. Time: 91.8719 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #85: GFLOPs: 7526.3041. Time: 83.6277 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #86: GFLOPs: 5910.2296. Time: 106.4946 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #87: GFLOPs: 6896.9924. Time: 91.2583 us. Best GFLOPs: 7847.5336
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #88: GFLOPs: 8533.4177. Time: 73.7580 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #89: GFLOPs: 5903.1811. Time: 106.6218 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #90: GFLOPs: 5918.6753. Time: 106.3427 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #91: GFLOPs: 6496.9182. Time: 96.8779 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #92: GFLOPs: 5927.4675. Time: 106.1849 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #93: GFLOPs: 7327.2713. Time: 85.8993 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #94: GFLOPs: 5904.4422. Time: 106.5990 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #95: GFLOPs: 6799.3096. Time: 92.5694 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #96: GFLOPs: 6047.8510. Time: 104.0713 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #97: GFLOPs: 1248.2226. Time: 504.2432 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #98: GFLOPs: 1190.5357. Time: 528.6761 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #99: GFLOPs: 1191.1406. Time: 528.4076 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #100: GFLOPs: 1189.7679. Time: 529.0172 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #101: GFLOPs: 7515.5195. Time: 83.7477 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #102: GFLOPs: 7562.8205. Time: 83.2239 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #103: GFLOPs: 6399.0137. Time: 98.3601 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #104: GFLOPs: 7596.3560. Time: 82.8565 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #105: GFLOPs: 7560.7530. Time: 83.2467 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #106: GFLOPs: 5956.6999. Time: 105.6638 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #107: GFLOPs: 5956.5098. Time: 105.6672 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #108: GFLOPs: 5927.4504. Time: 106.1852 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #109: GFLOPs: 7607.1481. Time: 82.7390 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #110: GFLOPs: 5919.3071. Time: 106.3313 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #111: GFLOPs: 7362.8807. Time: 85.4839 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #112: GFLOPs: 7546.1683. Time: 83.4076 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #113: GFLOPs: 5844.6271. Time: 107.6900 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #114: GFLOPs: 6090.4407. Time: 103.3435 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #115: GFLOPs: 7305.5931. Time: 86.1542 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #116: GFLOPs: 6515.6623. Time: 96.5992 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #117: GFLOPs: 6092.8092. Time: 103.3034 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #118: GFLOPs: 7595.3040. Time: 82.8680 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #119: GFLOPs: 7518.6921. Time: 83.7124 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #120: GFLOPs: 7542.9636. Time: 83.4430 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #121: GFLOPs: 7035.0339. Time: 89.4676 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #122: GFLOPs: 6474.3982. Time: 97.2149 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #123: GFLOPs: 7041.3166. Time: 89.3878 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #124: GFLOPs: 7420.9619. Time: 84.8148 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #125: GFLOPs: 7963.5650. Time: 79.0359 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #126: GFLOPs: 3076.5484. Time: 204.5824 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #127: GFLOPs: 80.5456. Time: 7814.3017 us. Best GFLOPs: 8533.4177
2024-05-01 19:21:30 [INFO] [task_scheduler.cc:131] [Task #211: fused_nn_conv2d_2] Trial #128: GFLOPs: 396.6384. Time: 1586.8553 us. Best GFLOPs: 8533.4177
