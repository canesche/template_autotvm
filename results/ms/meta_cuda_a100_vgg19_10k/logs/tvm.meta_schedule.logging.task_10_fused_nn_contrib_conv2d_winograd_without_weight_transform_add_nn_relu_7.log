2024-04-30 10:10:00 [INFO] [task_scheduler.cc:160] Initializing Task #10: "fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7"
2024-04-30 10:10:00 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        data_pad = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(30), T.int64(30)))
        input_tile = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)))
        B = T.alloc_buffer((T.int64(4), T.int64(4)))
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        A = T.alloc_buffer((T.int64(4), T.int64(2)))
        inverse = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)))
        conv2d_winograd = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(30), T.int64(30)):
            with T.block("data_pad"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(data_pad[v_i0, v_i1, v_i2, v_i3])
                data_pad[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(29) and T.int64(1) <= v_i3 and v_i3 < T.int64(29), p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for ci, p, eps, nu in T.grid(T.int64(512), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("input_tile"):
                v_ci, v_p, v_eps, v_nu = T.axis.remap("SSSS", [ci, p, eps, nu])
                T.reads(data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu])
                T.writes(input_tile[v_ci, v_p, v_eps, v_nu])
                T.block_attr({"schedule_rule": "None"})
                input_tile[v_ci, v_p, v_eps, v_nu] = data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu]
        for i, j in T.grid(T.int64(4), T.int64(4)):
            with T.block("B"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(B[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                B[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
        for eps, nu, ci, p, r_a, r_b in T.grid(T.int64(4), T.int64(4), T.int64(512), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("data_pack"):
                v_eps, v_nu, v_ci, v_p, v_r_a, v_r_b = T.axis.remap("SSSSRR", [eps, nu, ci, p, r_a, r_b])
                T.reads(input_tile[v_ci, v_p, v_r_a, v_r_b], B[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_eps, v_nu):T.min(v_eps, v_nu) + (T.max(v_eps, v_nu) + T.int64(1) - T.min(v_eps, v_nu))])
                T.writes(data_pack[v_eps, v_nu, v_ci, v_p])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                with T.init():
                    data_pack[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                data_pack[v_eps, v_nu, v_ci, v_p] = data_pack[v_eps, v_nu, v_ci, v_p] + input_tile[v_ci, v_p, v_r_a, v_r_b] * B[v_r_a, v_eps] * B[v_r_b, v_nu]
        for eps, nu, co, p, ci in T.grid(T.int64(4), T.int64(4), T.int64(512), T.int64(196), T.int64(512)):
            with T.block("bgemm"):
                v_eps, v_nu, v_co, v_p, v_ci = T.axis.remap("SSSSR", [eps, nu, co, p, ci])
                T.reads(data_pack[v_eps, v_nu, v_ci, v_p], p1[v_eps, v_nu, v_ci, v_co])
                T.writes(bgemm[v_eps, v_nu, v_co, v_p])
                with T.init():
                    bgemm[v_eps, v_nu, v_co, v_p] = T.float32(0)
                bgemm[v_eps, v_nu, v_co, v_p] = bgemm[v_eps, v_nu, v_co, v_p] + data_pack[v_eps, v_nu, v_ci, v_p] * p1[v_eps, v_nu, v_ci, v_co]
        for i, j in T.grid(T.int64(4), T.int64(2)):
            with T.block("A"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(A[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                A[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
        for co, p, vh, vw, r_a, r_b in T.grid(T.int64(512), T.int64(196), T.int64(2), T.int64(2), T.int64(4), T.int64(4)):
            with T.block("inverse"):
                v_co, v_p, v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSSSRR", [co, p, vh, vw, r_a, r_b])
                T.reads(bgemm[v_r_a, v_r_b, v_co, v_p], A[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_vh, v_vw):T.min(v_vh, v_vw) + (T.max(v_vh, v_vw) + T.int64(1) - T.min(v_vh, v_vw))])
                T.writes(inverse[v_co, v_p, v_vh, v_vw])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                with T.init():
                    inverse[v_co, v_p, v_vh, v_vw] = T.float32(0)
                inverse[v_co, v_p, v_vh, v_vw] = inverse[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * A[v_r_a, v_vh] * A[v_r_b, v_vw]
        for n, co, h, w in T.grid(T.int64(1), T.int64(512), T.int64(28), T.int64(28)):
            with T.block("conv2d_winograd"):
                v_n, v_co, v_h, v_w = T.axis.remap("SSSS", [n, co, h, w])
                T.reads(inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                conv2d_winograd[v_n, v_co, v_h, v_w] = inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(28), T.int64(28)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_winograd[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_winograd[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(28), T.int64(28)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2024-04-30 10:10:00 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-30 10:10:00 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(28), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        for ci_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(114688)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(28672))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(28672) // T.int64(7168))
                                    v2 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(7168) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(2097152)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(524288))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(524288) // T.int64(131072))
                                    v2 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(131072) // T.int64(256))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(4), T.int64(2), T.int64(8), T.int64(7)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_3 * T.int64(4) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                    v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + eps_2_nu_2_co_2_p_2_fused * T.int64(8) + co_3 * T.int64(8) + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(7) + p_3 * T.int64(7) + p_4)
                                    v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(512) + ci_1 * T.int64(8) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(2), T.int64(8), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + eps_2_nu_2_co_2_p_2_fused * T.int64(8) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 4])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 32, 1, 8])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 7])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[1, 64, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
2024-04-30 10:10:00 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(28), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(1), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(114688)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(28672))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(28672) // T.int64(7168))
                                    v2 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(7168) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(2097152)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(524288))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(524288) // T.int64(131072))
                                    v2 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(131072) // T.int64(256))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(4), T.int64(2), T.int64(8), T.int64(7)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_3 * T.int64(4) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                    v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + eps_2_nu_2_co_2_p_2_fused * T.int64(8) + co_3 * T.int64(8) + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(7) + p_3 * T.int64(7) + p_4)
                                    v_ci = T.axis.reduce(T.int64(512), ci_1 * T.int64(8) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(2), T.int64(8), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + eps_2_nu_2_co_2_p_2_fused * T.int64(8) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 4])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 32, 1, 8])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 7])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[1, 64, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
2024-04-30 10:10:00 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(28), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(1), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(114688)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(28672))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(28672) // T.int64(7168))
                                    v2 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(7168) // T.int64(14))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + ax0_ax1_ax2_ax3_fused % T.int64(14))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(2097152)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(524288))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(524288) // T.int64(131072))
                                    v2 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(131072) // T.int64(256))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(4), T.int64(2), T.int64(8), T.int64(7)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_3 * T.int64(4) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                    v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + eps_2_nu_2_co_2_p_2_fused * T.int64(8) + co_3 * T.int64(8) + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(7) + p_3 * T.int64(7) + p_4)
                                    v_ci = T.axis.reduce(T.int64(512), ci_1 * T.int64(8) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(2), T.int64(8), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(14) * T.int64(256) + eps_2_nu_2_co_2_p_2_fused * T.int64(8) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(14) * T.int64(14) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 4])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 32, 1, 8])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 7])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[1, 64, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
2024-04-30 10:37:35 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 10:37:35 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-30 10:37:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 504 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:37:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1007 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:37:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1510 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:38:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2012 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:38:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2512 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:38:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3015 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:38:18 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-04-30 10:38:32 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:38:46 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:38:59 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 110 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:39:12 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:39:13 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9996  0.9991  0.9988  0.9980  0.9976  0.9974  0.9966  0.9966  0.9956  0.9952  0.9951  0.9947  0.9945  0.9941  0.9938  0.9937
[17 : 32]:	0.9923  0.9919  0.9913  0.9909  0.9900  0.9865  0.9865  0.9861  0.9861  0.9859  0.9857  0.9853  0.9851  0.9847  0.9847  0.9841
[33 : 48]:	0.9837  0.9831  0.9823  0.9821  0.9800  0.9790  0.9787  0.9786  0.9777  0.9767  0.9759  0.9759  0.9759  0.9754  0.9753  0.9751
[49 : 64]:	0.9750  0.9741  0.9741  0.9734  0.9708  0.9707  0.9703  0.9701  0.9696  0.9689  0.9673  0.9663  0.9645  0.9644  0.9636  0.9635
2024-04-30 10:39:13 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 10:39:13 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1: GFLOPs: 4898.2125. Time: 355.4986 us. Best GFLOPs: 4898.2125
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2: GFLOPs: 3085.5730. Time: 564.3386 us. Best GFLOPs: 4898.2125
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #3: GFLOPs: 4988.7276. Time: 349.0485 us. Best GFLOPs: 4988.7276
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #4: GFLOPs: 6396.6118. Time: 272.2235 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #5: GFLOPs: 4154.0974. Time: 419.1784 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #6: GFLOPs: 3053.9851. Time: 570.1756 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #7: GFLOPs: 3950.6350. Time: 440.7666 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #8: GFLOPs: 151.2224. Time: 11514.8798 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #9: GFLOPs: 2208.9333. Time: 788.3026 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #10: GFLOPs: 3347.4949. Time: 520.1824 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #11: GFLOPs: 3802.3403. Time: 457.9569 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #12: GFLOPs: 86.8256. Time: 20055.2444 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #13: GFLOPs: 5607.8527. Time: 310.5124 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #14: GFLOPs: 308.1322. Time: 5651.1713 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #15: GFLOPs: 280.1205. Time: 6216.2825 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #16: GFLOPs: 18.1766. Time: 95799.6417 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #17: GFLOPs: 3709.0832. Time: 469.4712 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #18: GFLOPs: 243.2358. Time: 7158.9301 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #19: GFLOPs: 564.3496. Time: 3085.5126 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #20: GFLOPs: 163.0671. Time: 10678.4767 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #21: GFLOPs: 6134.9213. Time: 283.8354 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #22: GFLOPs: 22.4411. Time: 77594.6247 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #23: GFLOPs: 140.8693. Time: 12361.1593 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #24: GFLOPs: 44.2043. Time: 39392.2550 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #25: GFLOPs: 68.2040. Time: 25530.8798 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #26: GFLOPs: 3610.5044. Time: 482.2894 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #27: GFLOPs: 3434.8645. Time: 506.9510 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #28: GFLOPs: 3.2569. Time: 534647.8270 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #29: GFLOPs: 129.9850. Time: 13396.2240 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #30: GFLOPs: 304.1277. Time: 5725.5821 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #31: GFLOPs: 259.1956. Time: 6718.1228 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #32: GFLOPs: 2113.5775. Time: 823.8676 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #33: GFLOPs: 17.9115. Time: 97217.5393 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #34: GFLOPs: 123.2902. Time: 14123.6476 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #35: GFLOPs: 544.1968. Time: 3199.7759 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #36: GFLOPs: 3010.5930. Time: 578.3937 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #37: GFLOPs: 347.7141. Time: 5007.8720 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #38: GFLOPs: 4192.2968. Time: 415.3589 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #39: GFLOPs: 376.3938. Time: 4626.2925 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #40: GFLOPs: 214.2596. Time: 8127.0939 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #41: GFLOPs: 5936.0199. Time: 293.3460 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #42: GFLOPs: 3740.2536. Time: 465.5588 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #43: GFLOPs: 253.0776. Time: 6880.5293 us. Best GFLOPs: 6396.6118
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #44: GFLOPs: 6805.2753. Time: 255.8762 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #45: GFLOPs: 645.6784. Time: 2696.8657 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #46: GFLOPs: 4988.8183. Time: 349.0422 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #47: GFLOPs: 3433.6543. Time: 507.1296 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #48: GFLOPs: 122.8505. Time: 14174.2076 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #49: GFLOPs: 152.4121. Time: 11424.9953 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #50: GFLOPs: 2139.9734. Time: 813.7054 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #51: GFLOPs: 272.3435. Time: 6393.7921 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #52: GFLOPs: 3977.6502. Time: 437.7730 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #53: GFLOPs: 528.4908. Time: 3294.8687 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #54: GFLOPs: 84.0930. Time: 20706.9182 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #55: GFLOPs: 36.1569. Time: 48159.7440 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #56: GFLOPs: 4714.8687. Time: 369.3227 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #57: GFLOPs: 248.0110. Time: 7021.0901 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #58: GFLOPs: 3532.2599. Time: 492.9728 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #59: GFLOPs: 2233.0377. Time: 779.7933 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #60: GFLOPs: 251.9054. Time: 6912.5463 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #61: GFLOPs: 5947.7852. Time: 292.7658 us. Best GFLOPs: 6805.2753
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #62: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(28), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(14)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + eps_3_init * T.int64(2) + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + co_3_init * T.int64(4) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(14) + p_3_init * T.int64(14) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(256)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(28)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(224) // T.int64(56))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(128))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(4), T.int64(14)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + co_3 * T.int64(4) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(14) + p_3 * T.int64(14) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(2) + ci_1 * T.int64(2) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(4), T.int64(4), T.int64(14)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 2])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 2, 16, 1, 4])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 14])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l130, l131, l132, l133, l134 = sch.get_loops(block=b97)
l135, l136 = sch.split(loop=l134, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l136, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l137, l138, l139, l140, l141 = sch.get_loops(block=b108)
l142, l143 = sch.split(loop=l141, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l143, thread_axis="threadIdx.x")
b144 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b144, ann_key="meta_schedule.unroll_explicit")
b145, b146, b147, b148, b149, b150, b151, b152, b153 = sch.get_child_blocks(b144)
l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b145)
l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b146)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b147)
l172, l173, l174, l175, l176, l177 = sch.get_loops(block=b148)
l178, l179, l180, l181, l182, l183 = sch.get_loops(block=b149)
l184, l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l184, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l184, ann_key="pragma_unroll_explicit", ann_val=1)
l198, l199, l200, l201, l202, l203, l204 = sch.get_loops(block=b151)
l205, l206, l207, l208, l209, l210, l211, l212 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l205, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l205, ann_key="pragma_unroll_explicit", ann_val=1)
l213, l214, l215, l216 = sch.get_loops(block=b153)
b217 = sch.get_block(name="data_pack", func_name="main")
l218, l219, l220, l221, l222, l223 = sch.get_loops(block=b217)
b224 = sch.decompose_reduction(block=b217, loop=l222)
b225 = sch.get_block(name="bgemm", func_name="main")
l226, l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239 = sch.get_loops(block=b225)
b240 = sch.decompose_reduction(block=b225, loop=l229)
b241 = sch.get_block(name="inverse", func_name="main")
l242, l243, l244, l245, l246, l247, l248, l249 = sch.get_loops(block=b241)
b250 = sch.decompose_reduction(block=b241, loop=l248)
2024-04-30 10:51:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #63: GFLOPs: 4989.0718. Time: 349.0244 us. Best GFLOPs: 6805.2753
2024-04-30 10:54:54 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 10:54:55 [INFO] [evolutionary_search.cc:715] Picked top 62 candidate(s) from database
2024-04-30 10:55:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 443 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:55:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 888 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:55:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1327 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:55:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1769 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:55:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:55:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2647 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:55:33 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-04-30 10:55:48 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 106 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:56:05 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 96 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:56:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 90 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:56:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 89 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 10:56:46 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	3.7145  3.6021  3.3855  3.3811  3.3796  3.2330  1.9399  1.8361  1.1115  1.0938  1.0740  1.0460  1.0378  1.0342  1.0168  1.0168
[17 : 32]:	1.0104  1.0074  1.0074  0.9895  0.9881  0.9778  0.9778  0.9778  0.9746  0.9599  0.9598  0.9527  0.9495  0.9427  0.9362  0.9346
[33 : 48]:	0.9346  0.9316  0.9263  0.9263  0.9215  0.9213  0.9210  0.9176  0.9176  0.9175  0.9131  0.9129  0.9115  0.9114  0.9097  0.9086
[49 : 64]:	0.9086  0.9069  0.9048  0.9038  0.9009  0.9004  0.9002  0.8984  0.8962  0.8959  0.8959  0.8921  0.8921  0.8917  0.8915  0.8914
2024-04-30 10:56:46 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 10:56:46 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #64: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(44) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(44) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(25088), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(6272) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(6272) // T.int64(49) * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(49) * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(6272))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16) // T.int64(4))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(49) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(64))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(6272))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(6272) // T.int64(49) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(6272) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(6272) // T.int64(49) * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(49) * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(4) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(6272) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(6272) // T.int64(49) * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(49) * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[128, 1, 4, 1, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[49, 1, 4, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[128, 4, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #65: GFLOPs: 1109.6518. Time: 1569.2382 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #66: GFLOPs: 1321.6037. Time: 1317.5719 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #67: GFLOPs: 1308.6083. Time: 1330.6563 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #68: GFLOPs: 1338.5959. Time: 1300.8466 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #69: GFLOPs: 1306.7956. Time: 1332.5021 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #70: GFLOPs: 2293.1315. Time: 759.3581 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #71: GFLOPs: 2543.2006. Time: 684.6915 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #72: GFLOPs: 1183.8015. Time: 1470.9459 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #73: GFLOPs: 1183.8257. Time: 1470.9157 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #74: GFLOPs: 1202.9511. Time: 1447.5301 us. Best GFLOPs: 6805.2753
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #75: GFLOPs: 7326.6250. Time: 237.6685 us. Best GFLOPs: 7326.6250
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #76: GFLOPs: 5566.4632. Time: 312.8212 us. Best GFLOPs: 7326.6250
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #77: GFLOPs: 5142.5264. Time: 338.6094 us. Best GFLOPs: 7326.6250
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #78: GFLOPs: 8342.6078. Time: 208.7247 us. Best GFLOPs: 8342.6078
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #79: GFLOPs: 9149.1226. Time: 190.3251 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #80: GFLOPs: 8928.2179. Time: 195.0342 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #81: GFLOPs: 8003.6737. Time: 217.5636 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #82: GFLOPs: 8766.4530. Time: 198.6331 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #83: GFLOPs: 7313.1877. Time: 238.1052 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #84: GFLOPs: 6838.9830. Time: 254.6150 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #85: GFLOPs: 6875.1367. Time: 253.2761 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #86: GFLOPs: 6852.8421. Time: 254.1001 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #87: GFLOPs: 6847.0373. Time: 254.3155 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #88: GFLOPs: 6393.4420. Time: 272.3584 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #89: GFLOPs: 6132.8246. Time: 283.9325 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #90: GFLOPs: 6772.7722. Time: 257.1042 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #91: GFLOPs: 6519.3363. Time: 267.0990 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #92: GFLOPs: 6784.5332. Time: 256.6585 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #93: GFLOPs: 1711.4389. Time: 1017.4526 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #94: GFLOPs: 4234.6042. Time: 411.2091 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #95: GFLOPs: 6549.2453. Time: 265.8792 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #96: GFLOPs: 6550.6733. Time: 265.8212 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #97: GFLOPs: 7974.0743. Time: 218.3712 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #98: GFLOPs: 5800.3919. Time: 300.2052 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #99: GFLOPs: 6306.2679. Time: 276.1234 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #100: GFLOPs: 5776.0381. Time: 301.4710 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #101: GFLOPs: 8202.6721. Time: 212.2854 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #102: GFLOPs: 7278.3006. Time: 239.2465 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #103: GFLOPs: 5949.6648. Time: 292.6733 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #104: GFLOPs: 3339.9236. Time: 521.3616 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #105: GFLOPs: 5865.0576. Time: 296.8953 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #106: GFLOPs: 5899.2400. Time: 295.1750 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #107: GFLOPs: 3479.4735. Time: 500.4515 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #108: GFLOPs: 6225.5343. Time: 279.7042 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #109: GFLOPs: 4992.8580. Time: 348.7597 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #110: GFLOPs: 6014.3505. Time: 289.5255 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #111: GFLOPs: 6700.9177. Time: 259.8611 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #112: GFLOPs: 6707.2527. Time: 259.6157 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #113: GFLOPs: 7293.0986. Time: 238.7611 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #114: GFLOPs: 6100.3424. Time: 285.4443 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #115: GFLOPs: 5988.7362. Time: 290.7638 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #116: GFLOPs: 5884.5157. Time: 295.9135 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #117: GFLOPs: 3405.6829. Time: 511.2948 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #118: GFLOPs: 4207.1444. Time: 413.8931 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #119: GFLOPs: 7928.6294. Time: 219.6228 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #120: GFLOPs: 5852.1140. Time: 297.5519 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #121: GFLOPs: 6134.8731. Time: 283.8376 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #122: GFLOPs: 5603.1323. Time: 310.7740 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #123: GFLOPs: 6795.6689. Time: 256.2379 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #124: GFLOPs: 8110.2274. Time: 214.7052 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #125: GFLOPs: 16.7075. Time: 104223.0630 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #126: GFLOPs: 3001.9878. Time: 580.0516 us. Best GFLOPs: 9149.1226
2024-04-30 10:58:05 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #127: GFLOPs: 3020.6702. Time: 576.4641 us. Best GFLOPs: 9149.1226
2024-04-30 11:10:57 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 11:11:00 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 11:11:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 398 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:11:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 800 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:11:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1200 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:11:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1605 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:11:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2010 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:11:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2417 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:11:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2819 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:11:39 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-04-30 11:11:55 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:12:13 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:12:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 115 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:12:50 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:12:55 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9665  0.9632  0.9580  0.9580  0.9547  0.9495  0.9495  0.9362  0.9356  0.9343  0.9261  0.9176  0.9154  0.9039  0.9022  0.9022
[17 : 32]:	0.9022  0.9022  0.9005  0.8979  0.8957  0.8936  0.8920  0.8918  0.8911  0.8888  0.8886  0.8886  0.8870  0.8814  0.8804  0.8803
[33 : 48]:	0.8791  0.8791  0.8791  0.8782  0.8766  0.8766  0.8758  0.8754  0.8749  0.8748  0.8746  0.8744  0.8744  0.8743  0.8743  0.8736
[49 : 64]:	0.8726  0.8726  0.8722  0.8720  0.8709  0.8707  0.8705  0.8694  0.8694  0.8687  0.8687  0.8684  0.8681  0.8678  0.8659  0.8650
2024-04-30 11:12:55 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 11:12:55 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #128: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #129: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #130: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #131: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #132: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #133: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144 = sch.split(loop=l142, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184 = sch.get_loops(block=b150)
l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198 = sch.get_loops(block=b151)
l199, l200, l201, l202, l203, l204, l205 = sch.get_loops(block=b152)
l206, l207, l208, l209, l210, l211, l212, l213 = sch.get_loops(block=b153)
l214, l215, l216, l217 = sch.get_loops(block=b154)
b218 = sch.get_block(name="data_pack", func_name="main")
l219, l220, l221, l222, l223, l224 = sch.get_loops(block=b218)
b225 = sch.decompose_reduction(block=b218, loop=l223)
b226 = sch.get_block(name="bgemm", func_name="main")
l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240 = sch.get_loops(block=b226)
b241 = sch.decompose_reduction(block=b226, loop=l230)
b242 = sch.get_block(name="inverse", func_name="main")
l243, l244, l245, l246, l247, l248, l249, l250 = sch.get_loops(block=b242)
b251 = sch.decompose_reduction(block=b242, loop=l249)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #134: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144 = sch.split(loop=l142, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184 = sch.get_loops(block=b150)
l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198 = sch.get_loops(block=b151)
l199, l200, l201, l202, l203, l204, l205 = sch.get_loops(block=b152)
l206, l207, l208, l209, l210, l211, l212, l213 = sch.get_loops(block=b153)
l214, l215, l216, l217 = sch.get_loops(block=b154)
b218 = sch.get_block(name="data_pack", func_name="main")
l219, l220, l221, l222, l223, l224 = sch.get_loops(block=b218)
b225 = sch.decompose_reduction(block=b218, loop=l223)
b226 = sch.get_block(name="bgemm", func_name="main")
l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240 = sch.get_loops(block=b226)
b241 = sch.decompose_reduction(block=b226, loop=l230)
b242 = sch.get_block(name="inverse", func_name="main")
l243, l244, l245, l246, l247, l248, l249, l250 = sch.get_loops(block=b242)
b251 = sch.decompose_reduction(block=b242, loop=l249)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #135: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(784))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(784) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #136: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #137: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #138: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(2) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 4, 2])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #139: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #140: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 14, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #141: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #142: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(784))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(784) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #143: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(784))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(784) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #144: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(784))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(784) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #145: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(784))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(784) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(2) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(14) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 4, 2])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #146: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(784))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144 = sch.split(loop=l142, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184 = sch.get_loops(block=b150)
l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l185, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l185, ann_key="pragma_unroll_explicit", ann_val=1)
l199, l200, l201, l202, l203, l204, l205 = sch.get_loops(block=b152)
l206, l207, l208, l209, l210, l211, l212, l213 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l206, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l206, ann_key="pragma_unroll_explicit", ann_val=1)
l214, l215, l216, l217 = sch.get_loops(block=b154)
b218 = sch.get_block(name="data_pack", func_name="main")
l219, l220, l221, l222, l223, l224 = sch.get_loops(block=b218)
b225 = sch.decompose_reduction(block=b218, loop=l223)
b226 = sch.get_block(name="bgemm", func_name="main")
l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240 = sch.get_loops(block=b226)
b241 = sch.decompose_reduction(block=b226, loop=l230)
b242 = sch.get_block(name="inverse", func_name="main")
l243, l244, l245, l246, l247, l248, l249, l250 = sch.get_loops(block=b242)
b251 = sch.decompose_reduction(block=b242, loop=l249)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #147: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #148: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(16) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[32, 1, 16])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #149: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 7, 28, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #150: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #151: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(16) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[32, 2, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #152: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #153: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(128) // T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 7, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #154: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 7, 28, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #155: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1568))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 7, 28, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #156: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(4) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(4) * T.int64(49) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(7) + eps_2_nu_2_co_2_p_2_fused % T.int64(7) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(4) * T.int64(49) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(392))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(4) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(4) * T.int64(49) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(7) + eps_2_nu_2_co_2_p_2_fused % T.int64(7) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(8) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(4) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(4) * T.int64(49) + eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(7) + eps_2_nu_2_co_2_p_2_fused % T.int64(7) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[16, 2, 8, 1, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 7, 7, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #157: GFLOPs: 7840.9628. Time: 222.0783 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #158: GFLOPs: 8117.2795. Time: 214.5187 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #159: GFLOPs: 8904.5735. Time: 195.5521 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #160: GFLOPs: 7552.6945. Time: 230.5545 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #161: GFLOPs: 7650.1881. Time: 227.6164 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #162: GFLOPs: 7531.8996. Time: 231.1911 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #163: GFLOPs: 8230.4946. Time: 211.5678 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #164: GFLOPs: 8261.7056. Time: 210.7686 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #165: GFLOPs: 8217.7356. Time: 211.8963 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #166: GFLOPs: 8080.1604. Time: 215.5041 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #167: GFLOPs: 8533.2493. Time: 204.0615 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #168: GFLOPs: 7624.3163. Time: 228.3887 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #169: GFLOPs: 7820.1512. Time: 222.6693 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #170: GFLOPs: 8401.9141. Time: 207.2513 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #171: GFLOPs: 8283.4560. Time: 210.2151 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #172: GFLOPs: 8325.0845. Time: 209.1640 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #173: GFLOPs: 8232.5092. Time: 211.5161 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #174: GFLOPs: 8207.0269. Time: 212.1728 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #175: GFLOPs: 8089.1168. Time: 215.2655 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #176: GFLOPs: 7881.8272. Time: 220.9269 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #177: GFLOPs: 7972.3824. Time: 218.4175 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #178: GFLOPs: 8549.1800. Time: 203.6813 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #179: GFLOPs: 8246.4819. Time: 211.1577 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #180: GFLOPs: 8094.2453. Time: 215.1291 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #181: GFLOPs: 8252.4504. Time: 211.0050 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #182: GFLOPs: 8349.2217. Time: 208.5593 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #183: GFLOPs: 8180.7657. Time: 212.8539 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #184: GFLOPs: 8263.4193. Time: 210.7249 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #185: GFLOPs: 7973.7671. Time: 218.3796 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #186: GFLOPs: 7972.9639. Time: 218.4016 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #187: GFLOPs: 8902.2241. Time: 195.6037 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #188: GFLOPs: 8203.5431. Time: 212.2629 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #189: GFLOPs: 288.7321. Time: 6030.8779 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #190: GFLOPs: 3755.0487. Time: 463.7244 us. Best GFLOPs: 9149.1226
2024-04-30 11:13:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #191: GFLOPs: 53.0241. Time: 32839.9353 us. Best GFLOPs: 9149.1226
2024-04-30 11:21:22 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 11:21:25 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 11:21:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:21:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 802 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:21:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1203 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:21:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1606 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:21:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2005 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:22:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2411 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:22:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2814 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:22:05 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2024-04-30 11:22:21 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:22:39 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:22:58 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:23:16 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:23:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0344  1.0276  1.0161  1.0102  0.9959  0.9861  0.9755  0.9743  0.9735  0.9687  0.9676  0.9579  0.9579  0.9566  0.9547  0.9534
[17 : 32]:	0.9504  0.9486  0.9461  0.9458  0.9454  0.9442  0.9435  0.9418  0.9376  0.9363  0.9357  0.9349  0.9325  0.9323  0.9323  0.9316
[33 : 48]:	0.9316  0.9311  0.9298  0.9270  0.9260  0.9254  0.9179  0.9178  0.9166  0.9166  0.9165  0.9159  0.9121  0.9095  0.9087  0.9081
[49 : 64]:	0.9081  0.9070  0.9069  0.9056  0.9051  0.9050  0.9036  0.9024  0.9018  0.9017  0.9010  0.9004  0.9004  0.9000  0.8996  0.8996
2024-04-30 11:23:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 11:23:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #192: GFLOPs: 8440.2946. Time: 206.3089 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #193: GFLOPs: 8497.1830. Time: 204.9277 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #194: GFLOPs: 8034.5056. Time: 216.7287 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #195: GFLOPs: 8131.6433. Time: 214.1397 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #196: GFLOPs: 8185.0182. Time: 212.7433 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #197: GFLOPs: 9090.8356. Time: 191.5454 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #198: GFLOPs: 8084.6862. Time: 215.3835 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #199: GFLOPs: 6942.2428. Time: 250.8279 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #200: GFLOPs: 7510.1391. Time: 231.8609 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #201: GFLOPs: 7501.9700. Time: 232.1134 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #202: GFLOPs: 8793.1451. Time: 198.0302 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #203: GFLOPs: 8422.1554. Time: 206.7532 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #204: GFLOPs: 8422.1505. Time: 206.7534 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #205: GFLOPs: 8425.6333. Time: 206.6679 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #206: GFLOPs: 7074.0671. Time: 246.1537 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #207: GFLOPs: 8468.9052. Time: 205.6119 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #208: GFLOPs: 8758.4051. Time: 198.8156 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #209: GFLOPs: 8267.3244. Time: 210.6253 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #210: GFLOPs: 9020.4210. Time: 193.0406 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #211: GFLOPs: 8587.1280. Time: 202.7812 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #212: GFLOPs: 7516.9996. Time: 231.6493 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #213: GFLOPs: 8331.6482. Time: 208.9992 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #214: GFLOPs: 8965.8825. Time: 194.2149 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #215: GFLOPs: 8286.1113. Time: 210.1478 us. Best GFLOPs: 9149.1226
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #216: GFLOPs: 9359.3188. Time: 186.0507 us. Best GFLOPs: 9359.3188
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #217: GFLOPs: 8417.6422. Time: 206.8641 us. Best GFLOPs: 9359.3188
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #218: GFLOPs: 8934.3424. Time: 194.9005 us. Best GFLOPs: 9359.3188
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #219: GFLOPs: 8043.7407. Time: 216.4799 us. Best GFLOPs: 9359.3188
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #220: GFLOPs: 9372.1405. Time: 185.7962 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #221: GFLOPs: 8216.6239. Time: 211.9250 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #222: GFLOPs: 8062.3939. Time: 215.9790 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #223: GFLOPs: 8101.6609. Time: 214.9322 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #224: GFLOPs: 8100.4456. Time: 214.9645 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #225: GFLOPs: 8739.0804. Time: 199.2553 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #226: GFLOPs: 8241.4993. Time: 211.2853 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #227: GFLOPs: 7985.4780. Time: 218.0593 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #228: GFLOPs: 8191.7774. Time: 212.5678 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #229: GFLOPs: 8133.8535. Time: 214.0815 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #230: GFLOPs: 6775.2718. Time: 257.0093 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #231: GFLOPs: 7816.6740. Time: 222.7684 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #232: GFLOPs: 8143.2994. Time: 213.8332 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #233: GFLOPs: 8752.2144. Time: 198.9563 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #234: GFLOPs: 8862.5413. Time: 196.4795 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #235: GFLOPs: 8755.1711. Time: 198.8891 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #236: GFLOPs: 7727.7370. Time: 225.3322 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #237: GFLOPs: 8403.7236. Time: 207.2067 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #238: GFLOPs: 8096.7932. Time: 215.0614 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #239: GFLOPs: 8896.8034. Time: 195.7229 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #240: GFLOPs: 7844.7596. Time: 221.9708 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #241: GFLOPs: 7924.1366. Time: 219.7473 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #242: GFLOPs: 8411.6443. Time: 207.0116 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #243: GFLOPs: 6863.1345. Time: 253.7190 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #244: GFLOPs: 8862.4249. Time: 196.4821 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #245: GFLOPs: 8395.8736. Time: 207.4004 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #246: GFLOPs: 8111.6460. Time: 214.6676 us. Best GFLOPs: 9372.1405
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #247: GFLOPs: 10088.5086. Time: 172.6031 us. Best GFLOPs: 10088.5086
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #248: GFLOPs: 8018.5688. Time: 217.1594 us. Best GFLOPs: 10088.5086
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #249: GFLOPs: 9993.3282. Time: 174.2470 us. Best GFLOPs: 10088.5086
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #250: GFLOPs: 8031.7488. Time: 216.8031 us. Best GFLOPs: 10088.5086
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #251: GFLOPs: 8925.1350. Time: 195.1016 us. Best GFLOPs: 10088.5086
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #252: GFLOPs: 8957.9656. Time: 194.3865 us. Best GFLOPs: 10088.5086
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #253: GFLOPs: 3248.3849. Time: 536.0534 us. Best GFLOPs: 10088.5086
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #254: GFLOPs: 72.6521. Time: 23967.7444 us. Best GFLOPs: 10088.5086
2024-04-30 11:24:27 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #255: GFLOPs: 2248.4897. Time: 774.4345 us. Best GFLOPs: 10088.5086
2024-04-30 11:36:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 11:37:02 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 11:37:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:37:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:37:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:37:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1611 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:37:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2013 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:37:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2415 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:37:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2820 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:37:41 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-04-30 11:37:56 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:38:14 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:38:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 112 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:38:50 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:38:54 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9524  0.9500  0.9456  0.9334  0.9308  0.9303  0.9298  0.9285  0.9274  0.9268  0.9233  0.9230  0.9221  0.9202  0.9177  0.9174
[17 : 32]:	0.9141  0.9129  0.9129  0.9106  0.9086  0.9069  0.9050  0.9046  0.9045  0.9028  0.9019  0.9013  0.9007  0.9000  0.8997  0.8994
[33 : 48]:	0.8994  0.8980  0.8980  0.8970  0.8967  0.8967  0.8967  0.8964  0.8954  0.8954  0.8948  0.8945  0.8940  0.8936  0.8928  0.8919
[49 : 64]:	0.8904  0.8901  0.8899  0.8892  0.8859  0.8856  0.8853  0.8850  0.8840  0.8840  0.8836  0.8836  0.8836  0.8834  0.8827  0.8824
2024-04-30 11:38:55 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 11:38:55 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #256: GFLOPs: 9876.8715. Time: 176.3016 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #257: GFLOPs: 10045.9909. Time: 173.3336 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #258: GFLOPs: 9999.8118. Time: 174.1341 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #259: GFLOPs: 8140.6901. Time: 213.9018 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #260: GFLOPs: 9952.2676. Time: 174.9659 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #261: GFLOPs: 9657.3762. Time: 180.3086 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #262: GFLOPs: 9874.3378. Time: 176.3468 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #263: GFLOPs: 9915.5752. Time: 175.6134 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #264: GFLOPs: 9698.3616. Time: 179.5466 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #265: GFLOPs: 9759.5781. Time: 178.4204 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #266: GFLOPs: 8478.4331. Time: 205.3809 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #267: GFLOPs: 8220.0728. Time: 211.8361 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #268: GFLOPs: 8303.2034. Time: 209.7152 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #269: GFLOPs: 8479.4380. Time: 205.3565 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #270: GFLOPs: 8423.0205. Time: 206.7320 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #271: GFLOPs: 8230.0877. Time: 211.5783 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #272: GFLOPs: 8506.3393. Time: 204.7071 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #273: GFLOPs: 8348.8125. Time: 208.5695 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #274: GFLOPs: 8253.3779. Time: 210.9812 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #275: GFLOPs: 9748.4669. Time: 178.6238 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #276: GFLOPs: 8156.0149. Time: 213.4998 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #277: GFLOPs: 8522.2816. Time: 204.3241 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #278: GFLOPs: 9934.8997. Time: 175.2718 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #279: GFLOPs: 8161.5798. Time: 213.3543 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #280: GFLOPs: 9742.9108. Time: 178.7256 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #281: GFLOPs: 9804.0163. Time: 177.6117 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #282: GFLOPs: 8526.7112. Time: 204.2180 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #283: GFLOPs: 9223.2236. Time: 188.7960 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #284: GFLOPs: 8580.1610. Time: 202.9458 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #285: GFLOPs: 8442.4981. Time: 206.2551 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #286: GFLOPs: 8335.0231. Time: 208.9146 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #287: GFLOPs: 8201.7219. Time: 212.3100 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #288: GFLOPs: 8285.1617. Time: 210.1719 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #289: GFLOPs: 7675.7676. Time: 226.8578 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #290: GFLOPs: 7759.1593. Time: 224.4197 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #291: GFLOPs: 9650.3303. Time: 180.4402 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #292: GFLOPs: 8272.0896. Time: 210.5040 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #293: GFLOPs: 8374.9635. Time: 207.9183 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #294: GFLOPs: 8297.1861. Time: 209.8673 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #295: GFLOPs: 8287.5581. Time: 210.1111 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #296: GFLOPs: 8347.1397. Time: 208.6113 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #297: GFLOPs: 9954.2709. Time: 174.9307 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #298: GFLOPs: 9689.3886. Time: 179.7129 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #299: GFLOPs: 8262.4209. Time: 210.7503 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #300: GFLOPs: 8121.3562. Time: 214.4110 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #301: GFLOPs: 8185.4614. Time: 212.7318 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #302: GFLOPs: 8376.0121. Time: 207.8922 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #303: GFLOPs: 8991.3757. Time: 193.6642 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #304: GFLOPs: 9011.3461. Time: 193.2350 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #305: GFLOPs: 9745.7745. Time: 178.6731 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #306: GFLOPs: 9810.9371. Time: 177.4864 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #307: GFLOPs: 8262.0255. Time: 210.7604 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #308: GFLOPs: 8406.1115. Time: 207.1478 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #309: GFLOPs: 7809.2992. Time: 222.9788 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #310: GFLOPs: 8338.3703. Time: 208.8307 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #311: GFLOPs: 8172.3895. Time: 213.0721 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #312: GFLOPs: 8299.7016. Time: 209.8037 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #313: GFLOPs: 9196.3734. Time: 189.3472 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #314: GFLOPs: 7724.8198. Time: 225.4173 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #315: GFLOPs: 5393.1743. Time: 322.8725 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #316: GFLOPs: 9180.8023. Time: 189.6684 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #317: GFLOPs: 743.1709. Time: 2343.0786 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #318: GFLOPs: 3393.6130. Time: 513.1133 us. Best GFLOPs: 10088.5086
2024-04-30 11:39:55 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #319: GFLOPs: 4639.5890. Time: 375.3151 us. Best GFLOPs: 10088.5086
2024-04-30 11:52:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 11:52:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 11:52:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:52:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:52:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:52:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1614 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:53:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2017 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:53:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2422 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:53:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2821 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:53:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3223 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:53:19 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-04-30 11:53:34 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:53:54 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:54:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 99 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:54:29 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 11:54:34 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0253  0.9916  0.9867  0.9858  0.9851  0.9815  0.9806  0.9793  0.9779  0.9778  0.9772  0.9764  0.9764  0.9764  0.9758  0.9720
[17 : 32]:	0.9718  0.9718  0.9715  0.9697  0.9686  0.9678  0.9676  0.9675  0.9660  0.9637  0.9619  0.9604  0.9586  0.9583  0.9572  0.9562
[33 : 48]:	0.9554  0.9552  0.9548  0.9546  0.9535  0.9534  0.9525  0.9522  0.9494  0.9493  0.9489  0.9457  0.9433  0.9415  0.9415  0.9411
[49 : 64]:	0.9411  0.9386  0.9371  0.9369  0.9360  0.9337  0.9268  0.9258  0.9238  0.9204  0.9201  0.9190  0.9149  0.9135  0.9119  0.9111
2024-04-30 11:54:34 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 11:54:34 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #320: GFLOPs: 7471.0559. Time: 233.0739 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #321: GFLOPs: 7797.9180. Time: 223.3042 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #322: GFLOPs: 9853.4535. Time: 176.7206 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #323: GFLOPs: 9197.6413. Time: 189.3211 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #324: GFLOPs: 9234.3876. Time: 188.5678 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #325: GFLOPs: 9516.5918. Time: 182.9760 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #326: GFLOPs: 9779.3373. Time: 178.0599 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #327: GFLOPs: 9236.8606. Time: 188.5173 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #328: GFLOPs: 9231.7738. Time: 188.6212 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #329: GFLOPs: 9615.0824. Time: 181.1017 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #330: GFLOPs: 9789.0393. Time: 177.8834 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #331: GFLOPs: 9871.9406. Time: 176.3896 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #332: GFLOPs: 9964.8070. Time: 174.7458 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #333: GFLOPs: 9964.3014. Time: 174.7546 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #334: GFLOPs: 9978.9446. Time: 174.4982 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #335: GFLOPs: 9357.8077. Time: 186.0808 us. Best GFLOPs: 10088.5086
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #336: GFLOPs: 10091.2342. Time: 172.5565 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #337: GFLOPs: 9138.2817. Time: 190.5509 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #338: GFLOPs: 9833.5377. Time: 177.0785 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #339: GFLOPs: 9592.7860. Time: 181.5226 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #340: GFLOPs: 9793.0936. Time: 177.8098 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #341: GFLOPs: 9764.7530. Time: 178.3259 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #342: GFLOPs: 6766.4887. Time: 257.3429 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #343: GFLOPs: 10011.6132. Time: 173.9288 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #344: GFLOPs: 8981.1092. Time: 193.8856 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #345: GFLOPs: 9884.6297. Time: 176.1632 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #346: GFLOPs: 9641.2496. Time: 180.6102 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #347: GFLOPs: 9437.0710. Time: 184.5178 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #348: GFLOPs: 9832.8516. Time: 177.0908 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #349: GFLOPs: 9618.8803. Time: 181.0302 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #350: GFLOPs: 10030.4460. Time: 173.6022 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #351: GFLOPs: 8550.2707. Time: 203.6553 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #352: GFLOPs: 9868.8590. Time: 176.4447 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #353: GFLOPs: 7652.9250. Time: 227.5349 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #354: GFLOPs: 9794.6235. Time: 177.7820 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #355: GFLOPs: 9207.3888. Time: 189.1207 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #356: GFLOPs: 9999.8169. Time: 174.1340 us. Best GFLOPs: 10091.2342
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #357: GFLOPs: 10111.8187. Time: 172.2052 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #358: GFLOPs: 10095.3037. Time: 172.4869 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #359: GFLOPs: 9849.0450. Time: 176.7997 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #360: GFLOPs: 9817.0614. Time: 177.3757 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #361: GFLOPs: 10030.8677. Time: 173.5949 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #362: GFLOPs: 9773.0633. Time: 178.1742 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #363: GFLOPs: 9606.4648. Time: 181.2642 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #364: GFLOPs: 7884.5013. Time: 220.8520 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #365: GFLOPs: 9827.5340. Time: 177.1867 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #366: GFLOPs: 9791.1595. Time: 177.8449 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #367: GFLOPs: 9486.5390. Time: 183.5557 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #368: GFLOPs: 9976.7477. Time: 174.5366 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #369: GFLOPs: 9945.7160. Time: 175.0812 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #370: GFLOPs: 9674.2276. Time: 179.9945 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #371: GFLOPs: 9690.3513. Time: 179.6950 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #372: GFLOPs: 7509.2540. Time: 231.8883 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #373: GFLOPs: 9890.6008. Time: 176.0568 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #374: GFLOPs: 9817.0047. Time: 177.3767 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #375: GFLOPs: 9801.2526. Time: 177.6618 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #376: GFLOPs: 8810.5204. Time: 197.6396 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #377: GFLOPs: 5775.4591. Time: 301.5012 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #378: GFLOPs: 9117.8252. Time: 190.9784 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #379: GFLOPs: 9685.2925. Time: 179.7889 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #380: GFLOPs: 9364.4464. Time: 185.9488 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #381: GFLOPs: 1453.1651. Time: 1198.2864 us. Best GFLOPs: 10111.8187
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #382: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(2), T.int64(2), T.int64(1), T.int64(7)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(32) * T.int64(2) + eps_3_init * T.int64(2) + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) * T.int64(2) + nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(49) + p_3_init * T.int64(7) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(256)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(25)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1568))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1568) // T.int64(392))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(392) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(6272))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(128))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(7)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(32) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(49) + p_3 * T.int64(7) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(2) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(2), T.int64(49)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(32) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(49) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 4, 16, 2, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 4, 1, 7, 7])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l130, l131, l132, l133, l134 = sch.get_loops(block=b97)
l135, l136, l137 = sch.split(loop=l134, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l137)
sch.bind(loop=l136, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144 = sch.split(loop=l142, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b149)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b150)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b152)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b154)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2024-04-30 11:56:02 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #383: GFLOPs: 2118.5004. Time: 821.9531 us. Best GFLOPs: 10111.8187
2024-04-30 12:07:11 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 12:07:13 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 12:07:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:07:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 800 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:07:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1203 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:07:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1606 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:07:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2008 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:07:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2414 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:07:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2818 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:07:51 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-04-30 12:08:06 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:08:23 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 106 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:08:40 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:08:57 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:09:02 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.6124  1.5723  1.5723  1.5622  1.4017  1.3844  1.3664  1.3664  1.3664  1.3506  1.3313  1.3260  1.3189  1.3171  1.3063  1.3007
[17 : 32]:	1.2620  1.2602  1.2602  1.2602  1.2602  1.2569  1.2568  1.2568  1.2409  1.2212  1.2168  1.2168  1.2125  1.1907  1.1808  1.1795
[33 : 48]:	1.1626  1.1613  1.1596  1.1569  1.1212  1.0429  1.0394  1.0394  0.9996  0.9943  0.9920  0.9914  0.9841  0.9841  0.9841  0.9837
[49 : 64]:	0.9836  0.9835  0.9831  0.9828  0.9828  0.9826  0.9824  0.9822  0.9821  0.9816  0.9816  0.9815  0.9812  0.9811  0.9807  0.9806
2024-04-30 12:09:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 12:09:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #384: GFLOPs: 19.5227. Time: 89193.8170 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #385: GFLOPs: 21.4904. Time: 81027.0740 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #386: GFLOPs: 25.6896. Time: 67782.6537 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #387: GFLOPs: 12.4501. Time: 139863.0370 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #388: GFLOPs: 120.2140. Time: 14485.0649 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #389: GFLOPs: 2.9355. Time: 593195.6787 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #390: GFLOPs: 21.1472. Time: 82342.2290 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #391: GFLOPs: 19.9039. Time: 87485.7787 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #392: GFLOPs: 22.3335. Time: 77968.3837 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #393: GFLOPs: 19.8839. Time: 87573.8423 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #394: GFLOPs: 19.5667. Time: 88993.4487 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #395: GFLOPs: 134.7995. Time: 12917.7599 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #396: GFLOPs: 21.3725. Time: 81474.2177 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #397: GFLOPs: 21.0520. Time: 82714.6250 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #398: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(16), T.int64(4), T.int64(2), T.int64(2), T.int64(2), T.int64(7)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_3_init * T.int64(2) + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(256) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(32) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(7) * T.int64(28) + p_3_init * T.int64(7) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(512)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(56)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(784))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(784) // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(512), ci_0)
                                    v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(196))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(256))
                                        v2 = T.axis.spatial(T.int64(512), ci_0)
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(4), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(7)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(256) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(32) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(7) * T.int64(28) + p_3 * T.int64(7) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(32), T.int64(28)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(256) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(32) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(7) * T.int64(28) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 8, 16, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 7, 4, 7])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l130, l131, l132, l133, l134 = sch.get_loops(block=b97)
l135, l136 = sch.split(loop=l134, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l136, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l137, l138, l139, l140, l141 = sch.get_loops(block=b108)
l142, l143, l144 = sch.split(loop=l141, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l144)
sch.bind(loop=l143, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b150)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b152)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b154)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #399: GFLOPs: 21.6730. Time: 80344.4060 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #400: GFLOPs: 27.2953. Time: 63795.2013 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #401: GFLOPs: 21.3162. Time: 81689.2597 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #402: GFLOPs: 21.2605. Time: 81903.2743 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #403: GFLOPs: 19.7001. Time: 88390.6553 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #404: GFLOPs: 19.8640. Time: 87661.5700 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #405: GFLOPs: 11.8276. Time: 147224.2327 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #406: GFLOPs: 11.7885. Time: 147712.3410 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #407: GFLOPs: 11.2965. Time: 154145.4467 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #408: GFLOPs: 21.5032. Time: 80978.9427 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #409: GFLOPs: 10.4065. Time: 167329.4473 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #410: GFLOPs: 11.1407. Time: 156301.9917 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #411: GFLOPs: 10.6924. Time: 162854.2277 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #412: GFLOPs: 10.3296. Time: 168574.6357 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #413: GFLOPs: 11.0466. Time: 157633.5347 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #414: GFLOPs: 10.2095. Time: 170557.7797 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #415: GFLOPs: 129.3436. Time: 13462.6560 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #416: GFLOPs: 11.0713. Time: 157280.9343 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #417: GFLOPs: 11.9035. Time: 146285.9090 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #418: GFLOPs: 9.6388. Time: 180655.7820 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #419: GFLOPs: 28.6552. Time: 60767.5730 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #420: GFLOPs: 9.6326. Time: 180772.1760 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #421: GFLOPs: 79.9767. Time: 21772.6974 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #422: GFLOPs: 28.7319. Time: 60605.4380 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #423: GFLOPs: 28.8101. Time: 60440.9177 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #424: GFLOPs: 10052.2736. Time: 173.2253 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #425: GFLOPs: 9664.5648. Time: 180.1745 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #426: GFLOPs: 9635.5383. Time: 180.7172 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #427: GFLOPs: 10052.2260. Time: 173.2261 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #428: GFLOPs: 9852.5629. Time: 176.7365 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #429: GFLOPs: 9849.5284. Time: 176.7910 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #430: GFLOPs: 9752.7572. Time: 178.5452 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #431: GFLOPs: 10093.1781. Time: 172.5233 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #432: GFLOPs: 10059.8477. Time: 173.0949 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #433: GFLOPs: 9872.3523. Time: 176.3823 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #434: GFLOPs: 9982.2682. Time: 174.4401 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #435: GFLOPs: 9933.9035. Time: 175.2894 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #436: GFLOPs: 9933.4126. Time: 175.2981 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #437: GFLOPs: 10023.2592. Time: 173.7267 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #438: GFLOPs: 9945.1170. Time: 175.0917 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #439: GFLOPs: 9809.9461. Time: 177.5043 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #440: GFLOPs: 9878.9660. Time: 176.2642 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #441: GFLOPs: 9554.3897. Time: 182.2521 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #442: GFLOPs: 9993.3159. Time: 174.2473 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #443: GFLOPs: 9907.2388. Time: 175.7612 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #444: GFLOPs: 9957.1650. Time: 174.8799 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #445: GFLOPs: 1720.5693. Time: 1012.0533 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #446: GFLOPs: 116.7740. Time: 14911.7801 us. Best GFLOPs: 10111.8187
2024-04-30 12:10:56 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #447: GFLOPs: 3417.7899. Time: 509.4836 us. Best GFLOPs: 10111.8187
2024-04-30 12:25:38 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 12:25:40 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 12:25:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:25:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:25:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:26:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1615 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:26:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2016 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:26:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2412 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:26:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2813 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:26:20 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-04-30 12:26:35 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 132 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:26:53 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:27:11 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:27:29 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:27:34 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1408  0.9965  0.9938  0.9938  0.9933  0.9928  0.9878  0.9878  0.9878  0.9869  0.9869  0.9869  0.9858  0.9852  0.9851  0.9851
[17 : 32]:	0.9851  0.9850  0.9850  0.9849  0.9849  0.9849  0.9842  0.9842  0.9840  0.9834  0.9833  0.9832  0.9827  0.9827  0.9827  0.9824
[33 : 48]:	0.9823  0.9822  0.9821  0.9819  0.9819  0.9819  0.9815  0.9813  0.9813  0.9813  0.9811  0.9809  0.9809  0.9809  0.9809  0.9805
[49 : 64]:	0.9805  0.9804  0.9804  0.9802  0.9802  0.9802  0.9802  0.9800  0.9794  0.9793  0.9792  0.9792  0.9792  0.9790  0.9790  0.9785
2024-04-30 12:27:34 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 12:27:34 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #448: GFLOPs: 486.8894. Time: 3576.3931 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #449: GFLOPs: 10103.7516. Time: 172.3427 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #450: GFLOPs: 9900.8520. Time: 175.8746 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #451: GFLOPs: 9946.9126. Time: 175.0601 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #452: GFLOPs: 9947.5840. Time: 175.0483 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #453: GFLOPs: 9980.5888. Time: 174.4695 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #454: GFLOPs: 10094.3663. Time: 172.5029 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #455: GFLOPs: 10088.7357. Time: 172.5992 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #456: GFLOPs: 9758.4926. Time: 178.4403 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #457: GFLOPs: 10062.2054. Time: 173.0543 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #458: GFLOPs: 10076.7654. Time: 172.8043 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #459: GFLOPs: 9766.7392. Time: 178.2896 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #460: GFLOPs: 9558.7000. Time: 182.1700 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #461: GFLOPs: 10061.2226. Time: 173.0712 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #462: GFLOPs: 9825.1027. Time: 177.2305 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #463: GFLOPs: 10036.2777. Time: 173.5014 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #464: GFLOPs: 10005.5140. Time: 174.0348 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #465: GFLOPs: 9749.8871. Time: 178.5978 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #466: GFLOPs: 9927.4135. Time: 175.4040 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #467: GFLOPs: 9875.8452. Time: 176.3199 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #468: GFLOPs: 9885.5632. Time: 176.1466 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #469: GFLOPs: 9657.9697. Time: 180.2975 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #470: GFLOPs: 10001.5611. Time: 174.1036 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #471: GFLOPs: 9740.9215. Time: 178.7621 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #472: GFLOPs: 9964.3534. Time: 174.7537 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #473: GFLOPs: 9914.5236. Time: 175.6320 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #474: GFLOPs: 9753.5433. Time: 178.5308 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #475: GFLOPs: 10083.7324. Time: 172.6849 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #476: GFLOPs: 9856.2672. Time: 176.6701 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #477: GFLOPs: 9856.4140. Time: 176.6675 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #478: GFLOPs: 9860.3698. Time: 176.5966 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #479: GFLOPs: 9778.5855. Time: 178.0736 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #480: GFLOPs: 9927.3687. Time: 175.4048 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #481: GFLOPs: 9905.5826. Time: 175.7906 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #482: GFLOPs: 9860.6331. Time: 176.5919 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #483: GFLOPs: 9865.5289. Time: 176.5043 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #484: GFLOPs: 9707.5061. Time: 179.3775 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #485: GFLOPs: 5920.5938. Time: 294.1103 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #486: GFLOPs: 9687.8951. Time: 179.7406 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #487: GFLOPs: 9911.5976. Time: 175.6839 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #488: GFLOPs: 9966.9929. Time: 174.7074 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #489: GFLOPs: 9742.1009. Time: 178.7405 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #490: GFLOPs: 9882.5284. Time: 176.2006 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #491: GFLOPs: 9413.6662. Time: 184.9766 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #492: GFLOPs: 9750.8371. Time: 178.5804 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #493: GFLOPs: 9827.6399. Time: 177.1847 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #494: GFLOPs: 9771.7320. Time: 178.1985 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #495: GFLOPs: 9892.9743. Time: 176.0146 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #496: GFLOPs: 9889.8206. Time: 176.0707 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #497: GFLOPs: 9912.7663. Time: 175.6632 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #498: GFLOPs: 9883.1459. Time: 176.1896 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #499: GFLOPs: 9872.5417. Time: 176.3789 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #500: GFLOPs: 9883.7072. Time: 176.1796 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #501: GFLOPs: 9865.7026. Time: 176.5012 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #502: GFLOPs: 9607.6041. Time: 181.2427 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #503: GFLOPs: 9693.8064. Time: 179.6310 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #504: GFLOPs: 9870.7271. Time: 176.4113 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #505: GFLOPs: 9728.4283. Time: 178.9917 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #506: GFLOPs: 9729.3289. Time: 178.9751 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #507: GFLOPs: 9781.2294. Time: 178.0255 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #508: GFLOPs: 9625.4173. Time: 180.9073 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #509: GFLOPs: 116.4495. Time: 14953.3254 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #510: GFLOPs: 6274.5748. Time: 277.5181 us. Best GFLOPs: 10111.8187
2024-04-30 12:28:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #511: GFLOPs: 4428.7525. Time: 393.1825 us. Best GFLOPs: 10111.8187
2024-04-30 12:35:52 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 12:35:54 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 12:36:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:36:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:36:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:36:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1609 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:36:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2011 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:36:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2418 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:36:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2823 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:36:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3229 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:36:40 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-04-30 12:36:54 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 119 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:37:12 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:37:30 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:37:48 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:37:53 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9958  0.9940  0.9927  0.9919  0.9918  0.9901  0.9895  0.9895  0.9887  0.9884  0.9868  0.9863  0.9863  0.9863  0.9861  0.9860
[17 : 32]:	0.9859  0.9853  0.9853  0.9851  0.9850  0.9848  0.9845  0.9844  0.9844  0.9843  0.9839  0.9839  0.9838  0.9837  0.9836  0.9835
[33 : 48]:	0.9833  0.9833  0.9832  0.9830  0.9829  0.9828  0.9827  0.9827  0.9827  0.9825  0.9825  0.9824  0.9824  0.9823  0.9822  0.9822
[49 : 64]:	0.9821  0.9818  0.9817  0.9817  0.9816  0.9814  0.9813  0.9812  0.9812  0.9810  0.9809  0.9809  0.9806  0.9806  0.9804  0.9804
2024-04-30 12:37:53 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 12:37:53 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #512: GFLOPs: 9944.9440. Time: 175.0948 us. Best GFLOPs: 10111.8187
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #513: GFLOPs: 9855.6702. Time: 176.6808 us. Best GFLOPs: 10111.8187
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #514: GFLOPs: 9941.6666. Time: 175.1525 us. Best GFLOPs: 10111.8187
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #515: GFLOPs: 9780.0151. Time: 178.0476 us. Best GFLOPs: 10111.8187
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #516: GFLOPs: 9905.1999. Time: 175.7974 us. Best GFLOPs: 10111.8187
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #517: GFLOPs: 9918.5675. Time: 175.5604 us. Best GFLOPs: 10111.8187
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #518: GFLOPs: 10137.8280. Time: 171.7634 us. Best GFLOPs: 10137.8280
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #519: GFLOPs: 10142.2587. Time: 171.6884 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #520: GFLOPs: 10007.3864. Time: 174.0023 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #521: GFLOPs: 10107.3066. Time: 172.2821 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #522: GFLOPs: 9866.0507. Time: 176.4949 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #523: GFLOPs: 10072.6479. Time: 172.8749 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #524: GFLOPs: 10083.1991. Time: 172.6940 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #525: GFLOPs: 9865.8516. Time: 176.4985 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #526: GFLOPs: 10058.5999. Time: 173.1163 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #527: GFLOPs: 9680.5932. Time: 179.8762 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #528: GFLOPs: 9931.8118. Time: 175.3263 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #529: GFLOPs: 9826.1099. Time: 177.2123 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #530: GFLOPs: 9826.4820. Time: 177.2056 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #531: GFLOPs: 9842.2811. Time: 176.9212 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #532: GFLOPs: 9827.2861. Time: 177.1911 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #533: GFLOPs: 9941.8352. Time: 175.1495 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #534: GFLOPs: 9828.1266. Time: 177.1760 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #535: GFLOPs: 9844.9856. Time: 176.8726 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #536: GFLOPs: 9842.0779. Time: 176.9248 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #537: GFLOPs: 10032.3764. Time: 173.5688 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #538: GFLOPs: 9993.6152. Time: 174.2420 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #539: GFLOPs: 9836.4049. Time: 177.0269 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #540: GFLOPs: 9941.6112. Time: 175.1535 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #541: GFLOPs: 9838.3963. Time: 176.9910 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #542: GFLOPs: 9969.7829. Time: 174.6586 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #543: GFLOPs: 10008.8442. Time: 173.9769 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #544: GFLOPs: 9903.3019. Time: 175.8310 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #545: GFLOPs: 9900.0923. Time: 175.8880 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #546: GFLOPs: 9987.3153. Time: 174.3520 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #547: GFLOPs: 9885.7457. Time: 176.1433 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #548: GFLOPs: 10004.8537. Time: 174.0463 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #549: GFLOPs: 8506.1349. Time: 204.7120 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #550: GFLOPs: 9865.8430. Time: 176.4986 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #551: GFLOPs: 9965.2549. Time: 174.7379 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #552: GFLOPs: 9858.3394. Time: 176.6330 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #553: GFLOPs: 9951.8872. Time: 174.9726 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #554: GFLOPs: 9679.4555. Time: 179.8973 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #555: GFLOPs: 9735.1875. Time: 178.8674 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #556: GFLOPs: 9929.4733. Time: 175.3676 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #557: GFLOPs: 9753.8680. Time: 178.5249 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #558: GFLOPs: 9668.8923. Time: 180.0938 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #559: GFLOPs: 9668.8226. Time: 180.0951 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #560: GFLOPs: 9858.0917. Time: 176.6374 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #561: GFLOPs: 9544.3803. Time: 182.4433 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #562: GFLOPs: 9734.1109. Time: 178.8872 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #563: GFLOPs: 9735.9485. Time: 178.8534 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #564: GFLOPs: 9968.4134. Time: 174.6826 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #565: GFLOPs: 9828.2258. Time: 177.1742 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #566: GFLOPs: 9892.7808. Time: 176.0180 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #567: GFLOPs: 9746.5271. Time: 178.6593 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #568: GFLOPs: 9946.5649. Time: 175.0663 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #569: GFLOPs: 9914.5702. Time: 175.6312 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #570: GFLOPs: 9764.3905. Time: 178.3325 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #571: GFLOPs: 9875.4088. Time: 176.3277 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #572: GFLOPs: 9928.4904. Time: 175.3850 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #573: GFLOPs: 126.8518. Time: 13727.1041 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #574: GFLOPs: 6.4923. Time: 268209.8387 us. Best GFLOPs: 10142.2587
2024-04-30 12:39:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #575: GFLOPs: 6096.2179. Time: 285.6374 us. Best GFLOPs: 10142.2587
2024-04-30 12:45:57 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 12:46:00 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 12:46:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:46:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:46:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1203 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:46:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1607 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:46:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2007 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:46:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2410 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:46:35 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-04-30 12:46:51 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:47:10 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:47:28 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 131 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:47:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:47:52 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0396  0.9893  0.9846  0.9845  0.9844  0.9843  0.9843  0.9842  0.9841  0.9836  0.9828  0.9815  0.9812  0.9812  0.9811  0.9811
[17 : 32]:	0.9805  0.9805  0.9805  0.9804  0.9802  0.9801  0.9801  0.9801  0.9801  0.9795  0.9795  0.9794  0.9791  0.9791  0.9789  0.9782
[33 : 48]:	0.9780  0.9780  0.9778  0.9777  0.9776  0.9776  0.9776  0.9776  0.9774  0.9774  0.9771  0.9771  0.9771  0.9771  0.9768  0.9768
[49 : 64]:	0.9768  0.9768  0.9767  0.9767  0.9766  0.9762  0.9761  0.9759  0.9759  0.9758  0.9758  0.9758  0.9756  0.9755  0.9755  0.9755
2024-04-30 12:47:52 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 12:47:52 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #576: GFLOPs: 7840.7082. Time: 222.0855 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #577: GFLOPs: 10106.6423. Time: 172.2934 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #578: GFLOPs: 10110.5931. Time: 172.2261 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #579: GFLOPs: 10074.8528. Time: 172.8371 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #580: GFLOPs: 10049.9562. Time: 173.2652 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #581: GFLOPs: 10113.8811. Time: 172.1701 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #582: GFLOPs: 10016.0213. Time: 173.8523 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #583: GFLOPs: 10069.6882. Time: 172.9257 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #584: GFLOPs: 10056.1070. Time: 173.1592 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #585: GFLOPs: 9883.0789. Time: 176.1908 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #586: GFLOPs: 10019.1057. Time: 173.7987 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #587: GFLOPs: 9897.8051. Time: 175.9287 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #588: GFLOPs: 9917.1690. Time: 175.5852 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #589: GFLOPs: 10098.9783. Time: 172.4242 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #590: GFLOPs: 10104.7865. Time: 172.3251 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #591: GFLOPs: 10050.4287. Time: 173.2571 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #592: GFLOPs: 10135.2980. Time: 171.8063 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #593: GFLOPs: 10031.1088. Time: 173.5908 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #594: GFLOPs: 9955.5386. Time: 174.9085 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #595: GFLOPs: 9928.3846. Time: 175.3868 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #596: GFLOPs: 9775.3458. Time: 178.1326 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #597: GFLOPs: 9923.3280. Time: 175.4762 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #598: GFLOPs: 9925.4004. Time: 175.4396 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #599: GFLOPs: 9875.3338. Time: 176.3290 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #600: GFLOPs: 9917.4901. Time: 175.5795 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #601: GFLOPs: 9994.5190. Time: 174.2263 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #602: GFLOPs: 9781.4742. Time: 178.0210 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #603: GFLOPs: 9923.8862. Time: 175.4663 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #604: GFLOPs: 8650.6343. Time: 201.2925 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #605: GFLOPs: 9990.9319. Time: 174.2888 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #606: GFLOPs: 9838.0818. Time: 176.9967 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #607: GFLOPs: 9930.8129. Time: 175.3439 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #608: GFLOPs: 9822.3599. Time: 177.2800 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #609: GFLOPs: 10029.1599. Time: 173.6245 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #610: GFLOPs: 9928.9694. Time: 175.3765 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #611: GFLOPs: 9993.7323. Time: 174.2400 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #612: GFLOPs: 9832.4880. Time: 177.0974 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #613: GFLOPs: 9907.0247. Time: 175.7650 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #614: GFLOPs: 10005.5621. Time: 174.0340 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #615: GFLOPs: 9979.2631. Time: 174.4926 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #616: GFLOPs: 9891.9455. Time: 176.0329 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #617: GFLOPs: 9925.7498. Time: 175.4334 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #618: GFLOPs: 9889.0922. Time: 176.0837 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #619: GFLOPs: 9944.6144. Time: 175.1006 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #620: GFLOPs: 9985.1806. Time: 174.3892 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #621: GFLOPs: 9905.0979. Time: 175.7992 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #622: GFLOPs: 9861.6101. Time: 176.5744 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #623: GFLOPs: 9915.9560. Time: 175.6067 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #624: GFLOPs: 9916.5031. Time: 175.5970 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #625: GFLOPs: 9901.4482. Time: 175.8640 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #626: GFLOPs: 10001.8531. Time: 174.0985 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #627: GFLOPs: 10006.4141. Time: 174.0192 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #628: GFLOPs: 9996.1558. Time: 174.1978 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #629: GFLOPs: 9929.1513. Time: 175.3733 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #630: GFLOPs: 9647.0820. Time: 180.5010 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #631: GFLOPs: 9890.6567. Time: 176.0558 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #632: GFLOPs: 9713.3311. Time: 179.2699 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #633: GFLOPs: 10037.0476. Time: 173.4881 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #634: GFLOPs: 9676.3773. Time: 179.9545 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #635: GFLOPs: 9966.5422. Time: 174.7153 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #636: GFLOPs: 8323.5164. Time: 209.2034 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #637: GFLOPs: 3986.5701. Time: 436.7935 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #638: GFLOPs: 242.6260. Time: 7176.9234 us. Best GFLOPs: 10142.2587
2024-04-30 12:48:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #639: GFLOPs: 232.3105. Time: 7495.6071 us. Best GFLOPs: 10142.2587
2024-04-30 12:58:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 12:58:36 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 12:58:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:58:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:58:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1213 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:59:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1614 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:59:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2019 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:59:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2418 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:59:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2820 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:59:17 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-04-30 12:59:32 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 12:59:50 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 118 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:00:08 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 134 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:00:26 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:00:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9964  0.9947  0.9931  0.9929  0.9868  0.9868  0.9859  0.9859  0.9857  0.9857  0.9849  0.9847  0.9843  0.9841  0.9841  0.9840
[17 : 32]:	0.9839  0.9839  0.9834  0.9833  0.9828  0.9826  0.9824  0.9818  0.9812  0.9810  0.9804  0.9803  0.9803  0.9803  0.9800  0.9798
[33 : 48]:	0.9798  0.9798  0.9798  0.9797  0.9795  0.9793  0.9792  0.9791  0.9791  0.9789  0.9789  0.9788  0.9787  0.9783  0.9781  0.9779
[49 : 64]:	0.9779  0.9775  0.9775  0.9770  0.9769  0.9769  0.9768  0.9767  0.9767  0.9764  0.9764  0.9764  0.9764  0.9764  0.9761  0.9760
2024-04-30 13:00:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 13:00:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #640: GFLOPs: 9956.8653. Time: 174.8852 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #641: GFLOPs: 10053.3555. Time: 173.2066 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #642: GFLOPs: 10137.0595. Time: 171.7764 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #643: GFLOPs: 10086.4650. Time: 172.6381 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #644: GFLOPs: 10037.2223. Time: 173.4850 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #645: GFLOPs: 10036.2777. Time: 173.5014 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #646: GFLOPs: 10036.7376. Time: 173.4934 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #647: GFLOPs: 10051.4963. Time: 173.2387 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #648: GFLOPs: 9903.9658. Time: 175.8193 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #649: GFLOPs: 10115.5628. Time: 172.1415 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #650: GFLOPs: 9781.8519. Time: 178.0141 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #651: GFLOPs: 10046.7585. Time: 173.3204 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #652: GFLOPs: 10015.6455. Time: 173.8588 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #653: GFLOPs: 10112.9907. Time: 172.1853 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #654: GFLOPs: 9874.7394. Time: 176.3396 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #655: GFLOPs: 9960.9676. Time: 174.8131 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #656: GFLOPs: 9967.7564. Time: 174.6941 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #657: GFLOPs: 10100.5069. Time: 172.3981 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #658: GFLOPs: 10012.0312. Time: 173.9215 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #659: GFLOPs: 10011.7464. Time: 173.9265 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #660: GFLOPs: 10018.4770. Time: 173.8096 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #661: GFLOPs: 10031.6387. Time: 173.5816 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #662: GFLOPs: 10017.8681. Time: 173.8202 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #663: GFLOPs: 9903.5625. Time: 175.8264 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #664: GFLOPs: 9927.3687. Time: 175.4048 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #665: GFLOPs: 9887.8399. Time: 176.1060 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #666: GFLOPs: 9993.2168. Time: 174.2490 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #667: GFLOPs: 10036.0951. Time: 173.5045 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #668: GFLOPs: 10034.9285. Time: 173.5247 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #669: GFLOPs: 9879.3657. Time: 176.2571 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #670: GFLOPs: 9905.7393. Time: 175.7878 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #671: GFLOPs: 9936.9040. Time: 175.2365 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #672: GFLOPs: 9946.7365. Time: 175.0632 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #673: GFLOPs: 9874.8375. Time: 176.3379 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #674: GFLOPs: 9949.7800. Time: 175.0097 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #675: GFLOPs: 9844.5115. Time: 176.8811 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #676: GFLOPs: 9996.6559. Time: 174.1890 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #677: GFLOPs: 10059.9490. Time: 173.0931 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #678: GFLOPs: 9906.2153. Time: 175.7793 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #679: GFLOPs: 9836.8731. Time: 177.0184 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #680: GFLOPs: 9828.8952. Time: 177.1621 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #681: GFLOPs: 9939.5852. Time: 175.1892 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #682: GFLOPs: 9934.5323. Time: 175.2783 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #683: GFLOPs: 9732.9697. Time: 178.9082 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #684: GFLOPs: 10040.8736. Time: 173.4220 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #685: GFLOPs: 9919.3678. Time: 175.5463 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #686: GFLOPs: 9985.1823. Time: 174.3892 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #687: GFLOPs: 9938.5800. Time: 175.2069 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #688: GFLOPs: 9930.1210. Time: 175.3562 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #689: GFLOPs: 9857.7064. Time: 176.6443 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #690: GFLOPs: 9891.7632. Time: 176.0361 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #691: GFLOPs: 9987.6640. Time: 174.3459 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #692: GFLOPs: 9851.0724. Time: 176.7633 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #693: GFLOPs: 9879.1338. Time: 176.2612 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #694: GFLOPs: 9998.4535. Time: 174.1577 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #695: GFLOPs: 9991.0088. Time: 174.2875 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #696: GFLOPs: 9947.2586. Time: 175.0541 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #697: GFLOPs: 10016.0109. Time: 173.8524 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #698: GFLOPs: 10047.9038. Time: 173.3006 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #699: GFLOPs: 10066.5294. Time: 172.9800 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #700: GFLOPs: 10070.1827. Time: 172.9172 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #701: GFLOPs: 258.4760. Time: 6736.8275 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #702: GFLOPs: 129.8448. Time: 13410.6884 us. Best GFLOPs: 10142.2587
2024-04-30 13:01:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #703: GFLOPs: 146.6971. Time: 11870.0942 us. Best GFLOPs: 10142.2587
2024-04-30 13:16:56 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 13:16:59 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 13:17:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:17:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:17:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:17:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1611 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:17:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2015 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:17:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2417 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:17:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2823 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:17:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3225 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:17:44 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-04-30 13:17:59 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:18:16 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 106 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:18:34 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:18:53 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:18:58 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0161  0.9928  0.9903  0.9871  0.9871  0.9870  0.9869  0.9866  0.9862  0.9857  0.9857  0.9852  0.9848  0.9846  0.9846  0.9845
[17 : 32]:	0.9845  0.9844  0.9842  0.9840  0.9838  0.9837  0.9837  0.9837  0.9833  0.9832  0.9831  0.9831  0.9825  0.9825  0.9822  0.9822
[33 : 48]:	0.9822  0.9821  0.9819  0.9818  0.9817  0.9813  0.9812  0.9812  0.9810  0.9810  0.9806  0.9806  0.9804  0.9804  0.9801  0.9801
[49 : 64]:	0.9799  0.9799  0.9799  0.9798  0.9798  0.9798  0.9797  0.9796  0.9795  0.9794  0.9793  0.9792  0.9790  0.9790  0.9790  0.9789
2024-04-30 13:18:58 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 13:18:58 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #704: GFLOPs: 83.0410. Time: 20969.2672 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #705: GFLOPs: 5746.6341. Time: 303.0135 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #706: GFLOPs: 5596.8405. Time: 311.1234 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #707: GFLOPs: 9917.3495. Time: 175.5820 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #708: GFLOPs: 9893.1645. Time: 176.0112 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #709: GFLOPs: 10080.5810. Time: 172.7388 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #710: GFLOPs: 10024.6939. Time: 173.7019 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #711: GFLOPs: 10115.0106. Time: 172.1509 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #712: GFLOPs: 10103.4112. Time: 172.3485 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #713: GFLOPs: 10098.3506. Time: 172.4349 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #714: GFLOPs: 10064.8538. Time: 173.0088 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #715: GFLOPs: 9923.8254. Time: 175.4674 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #716: GFLOPs: 10043.7227. Time: 173.3728 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #717: GFLOPs: 10002.9180. Time: 174.0800 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #718: GFLOPs: 10003.7771. Time: 174.0650 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #719: GFLOPs: 9933.3923. Time: 175.2984 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #720: GFLOPs: 10029.7971. Time: 173.6135 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #721: GFLOPs: 9887.6638. Time: 176.1091 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #722: GFLOPs: 10025.6968. Time: 173.6845 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #723: GFLOPs: 10002.5327. Time: 174.0867 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #724: GFLOPs: 9819.5237. Time: 177.3312 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #725: GFLOPs: 9900.0801. Time: 175.8883 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #726: GFLOPs: 10032.1326. Time: 173.5731 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #727: GFLOPs: 5577.1651. Time: 312.2210 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #728: GFLOPs: 9803.7669. Time: 177.6162 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #729: GFLOPs: 10033.6908. Time: 173.5461 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #730: GFLOPs: 9971.0306. Time: 174.6367 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #731: GFLOPs: 9959.3200. Time: 174.8420 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #732: GFLOPs: 5761.5766. Time: 302.2277 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #733: GFLOPs: 10133.9663. Time: 171.8289 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #734: GFLOPs: 10112.5790. Time: 172.1923 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #735: GFLOPs: 10111.0572. Time: 172.2182 us. Best GFLOPs: 10142.2587
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #736: GFLOPs: 10187.6535. Time: 170.9234 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #737: GFLOPs: 9864.5681. Time: 176.5215 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #738: GFLOPs: 9677.8243. Time: 179.9276 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #739: GFLOPs: 9990.2558. Time: 174.3006 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #740: GFLOPs: 10038.3864. Time: 173.4649 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #741: GFLOPs: 9735.0924. Time: 178.8692 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #742: GFLOPs: 9847.3969. Time: 176.8293 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #743: GFLOPs: 9979.6299. Time: 174.4862 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #744: GFLOPs: 9612.9503. Time: 181.1419 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #745: GFLOPs: 9951.1394. Time: 174.9858 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #746: GFLOPs: 10040.5290. Time: 173.4279 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #747: GFLOPs: 9874.3983. Time: 176.3457 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #748: GFLOPs: 9956.3940. Time: 174.8934 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #749: GFLOPs: 9937.2407. Time: 175.2305 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #750: GFLOPs: 10141.0986. Time: 171.7080 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #751: GFLOPs: 9857.8024. Time: 176.6426 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #752: GFLOPs: 10022.8967. Time: 173.7330 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #753: GFLOPs: 9684.5615. Time: 179.8025 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #754: GFLOPs: 10144.6691. Time: 171.6476 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #755: GFLOPs: 9920.1855. Time: 175.5318 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #756: GFLOPs: 10100.0296. Time: 172.4062 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #757: GFLOPs: 10138.2885. Time: 171.7556 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #758: GFLOPs: 10127.4895. Time: 171.9388 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #759: GFLOPs: 10160.7951. Time: 171.3752 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #760: GFLOPs: 9983.6357. Time: 174.4162 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #761: GFLOPs: 9898.8886. Time: 175.9094 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #762: GFLOPs: 10030.2456. Time: 173.6057 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #763: GFLOPs: 10077.6024. Time: 172.7899 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #764: GFLOPs: 10159.2209. Time: 171.4017 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #765: GFLOPs: 282.8780. Time: 6155.6854 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #766: GFLOPs: 3448.6461. Time: 504.9251 us. Best GFLOPs: 10187.6535
2024-04-30 13:19:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #767: GFLOPs: 269.0951. Time: 6470.9759 us. Best GFLOPs: 10187.6535
2024-04-30 13:32:08 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 13:32:11 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 13:32:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:32:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 804 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:32:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1205 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:32:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1607 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:32:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2013 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:32:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2416 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:32:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2816 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:32:50 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-04-30 13:33:04 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:33:22 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 147 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:33:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:33:56 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:34:01 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2192  1.1903  1.1620  0.9934  0.9924  0.9901  0.9901  0.9900  0.9880  0.9877  0.9876  0.9870  0.9865  0.9865  0.9865  0.9864
[17 : 32]:	0.9862  0.9857  0.9850  0.9846  0.9845  0.9841  0.9839  0.9834  0.9833  0.9833  0.9832  0.9831  0.9830  0.9824  0.9822  0.9820
[33 : 48]:	0.9819  0.9817  0.9814  0.9811  0.9808  0.9798  0.9797  0.9794  0.9791  0.9789  0.9786  0.9785  0.9784  0.9783  0.9781  0.9780
[49 : 64]:	0.9779  0.9779  0.9779  0.9778  0.9777  0.9777  0.9777  0.9770  0.9768  0.9766  0.9764  0.9763  0.9760  0.9757  0.9757  0.9755
2024-04-30 13:34:01 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 13:34:01 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #768: GFLOPs: 2783.1930. Time: 625.6511 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #769: GFLOPs: 2894.7921. Time: 601.5312 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #770: GFLOPs: 2784.0401. Time: 625.4608 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #771: GFLOPs: 10131.5115. Time: 171.8705 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #772: GFLOPs: 10092.1111. Time: 172.5415 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #773: GFLOPs: 10044.3739. Time: 173.3615 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #774: GFLOPs: 10054.7550. Time: 173.1825 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #775: GFLOPs: 10031.8429. Time: 173.5781 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #776: GFLOPs: 10120.8055. Time: 172.0523 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #777: GFLOPs: 9909.3004. Time: 175.7246 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #778: GFLOPs: 10022.8277. Time: 173.7342 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #779: GFLOPs: 10008.2961. Time: 173.9865 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #780: GFLOPs: 9954.1071. Time: 174.9336 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #781: GFLOPs: 10067.8214. Time: 172.9578 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #782: GFLOPs: 10051.1072. Time: 173.2454 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #783: GFLOPs: 10006.0415. Time: 174.0257 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #784: GFLOPs: 10099.9598. Time: 172.4074 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #785: GFLOPs: 9881.5042. Time: 176.2189 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #786: GFLOPs: 9945.4877. Time: 175.0852 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #787: GFLOPs: 9894.6026. Time: 175.9856 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #788: GFLOPs: 9947.6901. Time: 175.0465 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #789: GFLOPs: 9922.6852. Time: 175.4876 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #790: GFLOPs: 10155.1611. Time: 171.4702 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #791: GFLOPs: 10090.0931. Time: 172.5760 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #792: GFLOPs: 10065.4368. Time: 172.9987 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #793: GFLOPs: 10070.9566. Time: 172.9039 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #794: GFLOPs: 10067.2182. Time: 172.9681 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #795: GFLOPs: 10181.1239. Time: 171.0330 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #796: GFLOPs: 10113.4571. Time: 172.1773 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #797: GFLOPs: 9583.1412. Time: 181.7053 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #798: GFLOPs: 9578.9652. Time: 181.7846 us. Best GFLOPs: 10187.6535
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #799: GFLOPs: 10206.7442. Time: 170.6037 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #800: GFLOPs: 10174.6467. Time: 171.1419 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #801: GFLOPs: 9778.3176. Time: 178.0785 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #802: GFLOPs: 10116.6689. Time: 172.1227 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #803: GFLOPs: 9942.0023. Time: 175.1466 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #804: GFLOPs: 10163.0567. Time: 171.3370 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #805: GFLOPs: 10022.3687. Time: 173.7422 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #806: GFLOPs: 9992.4525. Time: 174.2623 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #807: GFLOPs: 10035.3020. Time: 173.5182 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #808: GFLOPs: 10059.7533. Time: 173.0965 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #809: GFLOPs: 9865.7289. Time: 176.5007 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #810: GFLOPs: 9994.4619. Time: 174.2273 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #811: GFLOPs: 9991.5997. Time: 174.2772 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #812: GFLOPs: 10078.5683. Time: 172.7733 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #813: GFLOPs: 10020.4736. Time: 173.7750 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #814: GFLOPs: 10081.6995. Time: 172.7197 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #815: GFLOPs: 10089.7734. Time: 172.5815 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #816: GFLOPs: 10056.9149. Time: 173.1453 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #817: GFLOPs: 10047.0235. Time: 173.3158 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #818: GFLOPs: 10128.8419. Time: 171.9158 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #819: GFLOPs: 10161.7937. Time: 171.3583 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #820: GFLOPs: 9945.3888. Time: 175.0870 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #821: GFLOPs: 10152.4658. Time: 171.5158 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #822: GFLOPs: 9944.3242. Time: 175.1057 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #823: GFLOPs: 9989.2393. Time: 174.3184 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #824: GFLOPs: 9952.8606. Time: 174.9555 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #825: GFLOPs: 10004.7103. Time: 174.0488 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #826: GFLOPs: 10019.8008. Time: 173.7867 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #827: GFLOPs: 10060.2708. Time: 173.0876 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #828: GFLOPs: 10134.6219. Time: 171.8177 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #829: GFLOPs: 35.9093. Time: 48491.8617 us. Best GFLOPs: 10206.7442
2024-04-30 13:35:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #830: GFLOPs: 255.0285. Time: 6827.8956 us. Best GFLOPs: 10206.7442
2024-04-30 13:39:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 13:39:12 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 13:39:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:39:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:39:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:39:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1606 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:39:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2009 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:39:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2409 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:39:46 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-04-30 13:40:01 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 103 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:40:18 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:40:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:40:53 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 118 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:40:58 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0009  0.9932  0.9917  0.9908  0.9899  0.9899  0.9899  0.9895  0.9889  0.9888  0.9880  0.9879  0.9878  0.9873  0.9869  0.9865
[17 : 32]:	0.9863  0.9859  0.9859  0.9857  0.9855  0.9850  0.9848  0.9848  0.9846  0.9844  0.9842  0.9841  0.9841  0.9839  0.9837  0.9836
[33 : 48]:	0.9833  0.9832  0.9830  0.9829  0.9827  0.9824  0.9822  0.9815  0.9813  0.9810  0.9807  0.9807  0.9807  0.9803  0.9803  0.9802
[49 : 64]:	0.9801  0.9798  0.9798  0.9796  0.9796  0.9795  0.9793  0.9792  0.9792  0.9792  0.9791  0.9790  0.9789  0.9786  0.9786  0.9785
2024-04-30 13:40:58 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 13:40:58 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #831: GFLOPs: 10102.8258. Time: 172.3585 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #832: GFLOPs: 10144.6443. Time: 171.6480 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #833: GFLOPs: 9953.1327. Time: 174.9507 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #834: GFLOPs: 10040.1213. Time: 173.4349 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #835: GFLOPs: 9928.9694. Time: 175.3765 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #836: GFLOPs: 9928.3619. Time: 175.3872 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #837: GFLOPs: 9853.7844. Time: 176.7146 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #838: GFLOPs: 9926.5225. Time: 175.4197 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #839: GFLOPs: 9883.1398. Time: 176.1897 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #840: GFLOPs: 10043.0156. Time: 173.3850 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #841: GFLOPs: 10030.9238. Time: 173.5940 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #842: GFLOPs: 9873.3062. Time: 176.3652 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #843: GFLOPs: 10008.2435. Time: 173.9874 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #844: GFLOPs: 10016.6793. Time: 173.8408 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #845: GFLOPs: 9932.6119. Time: 175.3122 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #846: GFLOPs: 9764.7298. Time: 178.3263 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #847: GFLOPs: 9924.9465. Time: 175.4476 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #848: GFLOPs: 9882.1490. Time: 176.2074 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #849: GFLOPs: 9867.5770. Time: 176.4676 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #850: GFLOPs: 9441.3819. Time: 184.4336 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #851: GFLOPs: 9869.6819. Time: 176.4300 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #852: GFLOPs: 10039.2146. Time: 173.4506 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #853: GFLOPs: 10053.6362. Time: 173.2018 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #854: GFLOPs: 9926.6253. Time: 175.4179 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #855: GFLOPs: 9991.8010. Time: 174.2737 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #856: GFLOPs: 9941.5478. Time: 175.1546 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #857: GFLOPs: 10026.3501. Time: 173.6732 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #858: GFLOPs: 9906.0921. Time: 175.7815 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #859: GFLOPs: 9906.3080. Time: 175.7777 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #860: GFLOPs: 9964.1326. Time: 174.7576 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #861: GFLOPs: 9911.4088. Time: 175.6872 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #862: GFLOPs: 10001.5611. Time: 174.1036 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #863: GFLOPs: 9998.2089. Time: 174.1620 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #864: GFLOPs: 9869.8119. Time: 176.4277 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #865: GFLOPs: 9861.8986. Time: 176.5692 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #866: GFLOPs: 9879.0420. Time: 176.2628 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #867: GFLOPs: 9904.3820. Time: 175.8119 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #868: GFLOPs: 9908.4690. Time: 175.7393 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #869: GFLOPs: 10009.6746. Time: 173.9625 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #870: GFLOPs: 10005.1702. Time: 174.0408 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #871: GFLOPs: 9759.4204. Time: 178.4233 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #872: GFLOPs: 9922.0679. Time: 175.4985 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #873: GFLOPs: 9859.2230. Time: 176.6172 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #874: GFLOPs: 9993.2984. Time: 174.2476 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #875: GFLOPs: 9756.1918. Time: 178.4823 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #876: GFLOPs: 10007.9746. Time: 173.9920 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #877: GFLOPs: 9927.1951. Time: 175.4078 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #878: GFLOPs: 9849.2798. Time: 176.7955 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #879: GFLOPs: 9902.4804. Time: 175.8456 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #880: GFLOPs: 9989.0524. Time: 174.3216 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #881: GFLOPs: 9876.2615. Time: 176.3125 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #882: GFLOPs: 9817.6697. Time: 177.3647 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #883: GFLOPs: 9817.1619. Time: 177.3739 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #884: GFLOPs: 9970.8128. Time: 174.6405 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #885: GFLOPs: 9782.9822. Time: 177.9936 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #886: GFLOPs: 9776.8436. Time: 178.1053 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #887: GFLOPs: 9173.7306. Time: 189.8146 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #888: GFLOPs: 9854.4196. Time: 176.7032 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #889: GFLOPs: 9744.1597. Time: 178.7027 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #890: GFLOPs: 9906.5239. Time: 175.7739 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #891: GFLOPs: 9859.1249. Time: 176.6189 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #892: GFLOPs: 62.0766. Time: 28050.9433 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #893: GFLOPs: 5596.5040. Time: 311.1421 us. Best GFLOPs: 10206.7442
2024-04-30 13:41:59 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #894: GFLOPs: 258.7906. Time: 6728.6356 us. Best GFLOPs: 10206.7442
2024-04-30 13:55:42 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 13:55:44 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 13:55:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:55:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:56:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:56:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1617 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:56:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:56:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2419 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:56:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2824 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:56:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3227 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:56:31 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-04-30 13:56:46 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 103 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:57:03 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:57:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:57:40 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 13:57:45 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9892  0.9887  0.9886  0.9882  0.9881  0.9867  0.9857  0.9857  0.9857  0.9855  0.9841  0.9830  0.9830  0.9830  0.9828  0.9828
[17 : 32]:	0.9817  0.9817  0.9817  0.9817  0.9817  0.9816  0.9812  0.9805  0.9804  0.9803  0.9800  0.9798  0.9794  0.9794  0.9794  0.9793
[33 : 48]:	0.9793  0.9791  0.9791  0.9791  0.9791  0.9791  0.9787  0.9786  0.9786  0.9785  0.9785  0.9783  0.9783  0.9781  0.9780  0.9780
[49 : 64]:	0.9779  0.9777  0.9776  0.9773  0.9773  0.9772  0.9768  0.9766  0.9765  0.9764  0.9764  0.9764  0.9762  0.9760  0.9759  0.9757
2024-04-30 13:57:45 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 13:57:45 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #895: GFLOPs: 10103.0940. Time: 172.3539 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #896: GFLOPs: 10154.0041. Time: 171.4898 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #897: GFLOPs: 10101.8914. Time: 172.3744 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #898: GFLOPs: 10121.9015. Time: 172.0337 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #899: GFLOPs: 10037.3944. Time: 173.4821 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #900: GFLOPs: 10118.1301. Time: 172.0978 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #901: GFLOPs: 10035.5516. Time: 173.5139 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #902: GFLOPs: 10036.0043. Time: 173.5061 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #903: GFLOPs: 10044.0909. Time: 173.3664 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #904: GFLOPs: 10040.0853. Time: 173.4356 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #905: GFLOPs: 10033.5500. Time: 173.5485 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #906: GFLOPs: 10056.8587. Time: 173.1463 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #907: GFLOPs: 9895.6689. Time: 175.9667 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #908: GFLOPs: 9911.1249. Time: 175.6923 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #909: GFLOPs: 10001.9465. Time: 174.0969 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #910: GFLOPs: 9936.3428. Time: 175.2464 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #911: GFLOPs: 9877.3205. Time: 176.2935 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #912: GFLOPs: 9942.3001. Time: 175.1414 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #913: GFLOPs: 9816.5134. Time: 177.3856 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #914: GFLOPs: 10075.6610. Time: 172.8232 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #915: GFLOPs: 9998.8677. Time: 174.1505 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #916: GFLOPs: 9995.2399. Time: 174.2137 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #917: GFLOPs: 10008.0326. Time: 173.9910 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #918: GFLOPs: 9883.0789. Time: 176.1908 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #919: GFLOPs: 9896.1455. Time: 175.9582 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #920: GFLOPs: 10047.5625. Time: 173.3065 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #921: GFLOPs: 9895.8718. Time: 175.9631 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #922: GFLOPs: 9956.3547. Time: 174.8941 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #923: GFLOPs: 9827.3666. Time: 177.1897 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #924: GFLOPs: 9821.9441. Time: 177.2875 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #925: GFLOPs: 9961.1748. Time: 174.8095 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #926: GFLOPs: 10052.4231. Time: 173.2227 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #927: GFLOPs: 9766.3792. Time: 178.2962 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #928: GFLOPs: 9933.6379. Time: 175.2941 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #929: GFLOPs: 9934.9477. Time: 175.2710 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #930: GFLOPs: 9934.3005. Time: 175.2824 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #931: GFLOPs: 10046.2869. Time: 173.3285 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #932: GFLOPs: 9982.4973. Time: 174.4361 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #933: GFLOPs: 10026.4116. Time: 173.6721 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #934: GFLOPs: 9935.6086. Time: 175.2593 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #935: GFLOPs: 10053.1838. Time: 173.2096 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #936: GFLOPs: 10100.2285. Time: 172.4028 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #937: GFLOPs: 9952.3753. Time: 174.9641 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #938: GFLOPs: 10091.0778. Time: 172.5592 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #939: GFLOPs: 10041.1884. Time: 173.4165 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #940: GFLOPs: 10053.6081. Time: 173.2023 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #941: GFLOPs: 10059.0619. Time: 173.1084 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #942: GFLOPs: 10094.0866. Time: 172.5077 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #943: GFLOPs: 10098.6294. Time: 172.4301 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #944: GFLOPs: 10046.6322. Time: 173.3225 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #945: GFLOPs: 10035.8462. Time: 173.5088 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #946: GFLOPs: 10123.7896. Time: 172.0016 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #947: GFLOPs: 10095.6659. Time: 172.4807 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #948: GFLOPs: 9987.8157. Time: 174.3432 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #949: GFLOPs: 9587.6250. Time: 181.6204 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #950: GFLOPs: 10101.7944. Time: 172.3761 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #951: GFLOPs: 9989.4105. Time: 174.3154 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #952: GFLOPs: 9708.8691. Time: 179.3523 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #953: GFLOPs: 10115.1449. Time: 172.1486 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #954: GFLOPs: 10137.0847. Time: 171.7760 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #955: GFLOPs: 7153.2445. Time: 243.4291 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #956: GFLOPs: 52.2618. Time: 33318.9125 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #957: GFLOPs: 6381.8219. Time: 272.8544 us. Best GFLOPs: 10206.7442
2024-04-30 13:58:52 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #958: GFLOPs: 335.2118. Time: 5194.6495 us. Best GFLOPs: 10206.7442
2024-04-30 14:01:50 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 14:01:53 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 14:01:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:02:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:02:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1206 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:02:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1608 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:02:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2015 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:02:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2418 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:02:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2816 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:02:32 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-04-30 14:02:46 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:03:04 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:03:22 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:03:40 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:03:45 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0003  0.9923  0.9920  0.9919  0.9907  0.9892  0.9876  0.9870  0.9867  0.9867  0.9860  0.9858  0.9857  0.9843  0.9839  0.9839
[17 : 32]:	0.9839  0.9835  0.9831  0.9823  0.9823  0.9823  0.9820  0.9817  0.9806  0.9805  0.9805  0.9805  0.9801  0.9798  0.9793  0.9793
[33 : 48]:	0.9793  0.9791  0.9791  0.9790  0.9789  0.9789  0.9779  0.9775  0.9775  0.9775  0.9773  0.9770  0.9770  0.9767  0.9767  0.9766
[49 : 64]:	0.9766  0.9764  0.9760  0.9757  0.9755  0.9754  0.9754  0.9754  0.9754  0.9752  0.9751  0.9750  0.9747  0.9747  0.9743  0.9743
2024-04-30 14:03:45 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 14:03:45 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #959: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(7) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(7) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(32) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(7)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(7) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 32, 4, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 1, 4, 7, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[16, 32, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l130, l131, l132, l133, l134 = sch.get_loops(block=b97)
l135, l136 = sch.split(loop=l134, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l136, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l137, l138, l139, l140, l141 = sch.get_loops(block=b108)
l142, l143, l144 = sch.split(loop=l141, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l144)
sch.bind(loop=l143, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b150)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b152)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b154)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #960: GFLOPs: 10160.3835. Time: 171.3821 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #961: GFLOPs: 9906.9221. Time: 175.7668 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #962: GFLOPs: 9953.1856. Time: 174.9498 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #963: GFLOPs: 10099.8515. Time: 172.4093 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #964: GFLOPs: 10133.8683. Time: 171.8305 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #965: GFLOPs: 9914.8994. Time: 175.6254 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #966: GFLOPs: 10048.0827. Time: 173.2975 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #967: GFLOPs: 10088.5946. Time: 172.6016 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #968: GFLOPs: 10021.4416. Time: 173.7582 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #969: GFLOPs: 10014.8529. Time: 173.8725 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #970: GFLOPs: 10021.7011. Time: 173.7537 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #971: GFLOPs: 10048.8076. Time: 173.2850 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #972: GFLOPs: 10063.3695. Time: 173.0343 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #973: GFLOPs: 9941.8602. Time: 175.1491 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #974: GFLOPs: 10102.4785. Time: 172.3644 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #975: GFLOPs: 10063.5634. Time: 173.0309 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #976: GFLOPs: 9958.1511. Time: 174.8626 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #977: GFLOPs: 10021.0040. Time: 173.7658 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #978: GFLOPs: 9950.7605. Time: 174.9924 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #979: GFLOPs: 9779.5668. Time: 178.0557 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #980: GFLOPs: 9841.2523. Time: 176.9397 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #981: GFLOPs: 9753.7436. Time: 178.5271 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #982: GFLOPs: 9723.0088. Time: 179.0915 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #983: GFLOPs: 10028.1988. Time: 173.6411 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #984: GFLOPs: 9936.6228. Time: 175.2414 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #985: GFLOPs: 9990.6421. Time: 174.2939 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #986: GFLOPs: 9984.0132. Time: 174.4096 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #987: GFLOPs: 9924.5331. Time: 175.4549 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #988: GFLOPs: 9959.4822. Time: 174.8392 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #989: GFLOPs: 9875.8388. Time: 176.3200 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #990: GFLOPs: 9870.7053. Time: 176.4117 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #991: GFLOPs: 9945.9543. Time: 175.0770 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #992: GFLOPs: 9772.1150. Time: 178.1915 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #993: GFLOPs: 9771.0809. Time: 178.2104 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #994: GFLOPs: 9926.0952. Time: 175.4273 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #995: GFLOPs: 9828.7076. Time: 177.1655 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #996: GFLOPs: 9945.8749. Time: 175.0784 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #997: GFLOPs: 10011.2199. Time: 173.9356 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #998: GFLOPs: 10007.6309. Time: 173.9980 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #999: GFLOPs: 10064.2475. Time: 173.0192 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1000: GFLOPs: 10017.9616. Time: 173.8186 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1001: GFLOPs: 9835.5445. Time: 177.0423 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1002: GFLOPs: 9936.8251. Time: 175.2379 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1003: GFLOPs: 9644.0729. Time: 180.5573 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1004: GFLOPs: 10025.8954. Time: 173.6810 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1005: GFLOPs: 9992.9773. Time: 174.2532 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1006: GFLOPs: 9795.1996. Time: 177.7716 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1007: GFLOPs: 10030.1191. Time: 173.6079 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1008: GFLOPs: 9916.8508. Time: 175.5908 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1009: GFLOPs: 9857.5734. Time: 176.6467 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1010: GFLOPs: 9778.0915. Time: 178.0826 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1011: GFLOPs: 9960.7657. Time: 174.8167 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1012: GFLOPs: 10005.0296. Time: 174.0433 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1013: GFLOPs: 9675.4426. Time: 179.9719 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1014: GFLOPs: 9860.3572. Time: 176.5968 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1015: GFLOPs: 9915.2394. Time: 175.6193 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1016: GFLOPs: 9937.0857. Time: 175.2333 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1017: GFLOPs: 9869.9638. Time: 176.4250 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1018: GFLOPs: 9982.0762. Time: 174.4435 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1019: GFLOPs: 9989.1815. Time: 174.3194 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1020: GFLOPs: 665.5963. Time: 2616.1624 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1021: GFLOPs: 138.7990. Time: 12545.5360 us. Best GFLOPs: 10206.7442
2024-04-30 14:04:51 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1022: GFLOPs: 4210.7850. Time: 413.5352 us. Best GFLOPs: 10206.7442
2024-04-30 14:27:38 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 14:27:41 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 14:27:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:27:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:27:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:28:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:28:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2019 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:28:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2424 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:28:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2827 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:28:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3230 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:28:27 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-04-30 14:28:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:28:59 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 155 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:29:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 147 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:29:34 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 160 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:29:38 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1195  1.0947  0.9939  0.9936  0.9927  0.9919  0.9919  0.9903  0.9900  0.9900  0.9895  0.9894  0.9874  0.9862  0.9845  0.9841
[17 : 32]:	0.9841  0.9833  0.9833  0.9829  0.9829  0.9825  0.9820  0.9820  0.9817  0.9812  0.9808  0.9800  0.9788  0.9788  0.9786  0.9786
[33 : 48]:	0.9782  0.9781  0.9780  0.9780  0.9777  0.9776  0.9776  0.9773  0.9772  0.9770  0.9770  0.9768  0.9768  0.9767  0.9766  0.9764
[49 : 64]:	0.9764  0.9764  0.9761  0.9761  0.9761  0.9759  0.9757  0.9757  0.9757  0.9757  0.9756  0.9754  0.9754  0.9754  0.9753  0.9746
2024-04-30 14:29:38 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 14:29:38 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1023: GFLOPs: 528.9184. Time: 3292.2055 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1024: GFLOPs: 549.3945. Time: 3169.5039 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1025: GFLOPs: 10145.4683. Time: 171.6341 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1026: GFLOPs: 10115.4080. Time: 172.1441 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1027: GFLOPs: 10133.2536. Time: 171.8409 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1028: GFLOPs: 10037.4002. Time: 173.4820 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1029: GFLOPs: 10087.3253. Time: 172.6234 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1030: GFLOPs: 10065.3126. Time: 173.0009 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1031: GFLOPs: 10107.1307. Time: 172.2851 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1032: GFLOPs: 10037.6103. Time: 173.4783 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1033: GFLOPs: 9834.3374. Time: 177.0641 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1034: GFLOPs: 10048.1681. Time: 173.2961 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1035: GFLOPs: 10089.1086. Time: 172.5928 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1036: GFLOPs: 10012.3334. Time: 173.9163 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1037: GFLOPs: 9917.6527. Time: 175.5766 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1038: GFLOPs: 10074.8115. Time: 172.8378 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1039: GFLOPs: 10037.7863. Time: 173.4753 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1040: GFLOPs: 9940.0760. Time: 175.1805 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1041: GFLOPs: 9943.7505. Time: 175.1158 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1042: GFLOPs: 10078.3528. Time: 172.7770 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1043: GFLOPs: 10026.5094. Time: 173.6704 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1044: GFLOPs: 9938.2380. Time: 175.2129 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1045: GFLOPs: 10005.8076. Time: 174.0297 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1046: GFLOPs: 9926.1255. Time: 175.4267 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1047: GFLOPs: 10012.2737. Time: 173.9173 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1048: GFLOPs: 9864.9904. Time: 176.5139 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1049: GFLOPs: 9988.3974. Time: 174.3331 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1050: GFLOPs: 9898.0576. Time: 175.9242 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1051: GFLOPs: 10044.0736. Time: 173.3667 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1052: GFLOPs: 10005.4139. Time: 174.0366 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1053: GFLOPs: 9933.2027. Time: 175.3018 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1054: GFLOPs: 9940.2420. Time: 175.1776 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1055: GFLOPs: 9931.9887. Time: 175.3232 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1056: GFLOPs: 9927.4135. Time: 175.4040 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1057: GFLOPs: 10042.2032. Time: 173.3990 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1058: GFLOPs: 10050.5217. Time: 173.2555 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1059: GFLOPs: 10027.5913. Time: 173.6517 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1060: GFLOPs: 9830.6160. Time: 177.1311 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1061: GFLOPs: 9919.3037. Time: 175.5474 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1062: GFLOPs: 9741.5980. Time: 178.7497 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1063: GFLOPs: 9791.3172. Time: 177.8420 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1064: GFLOPs: 10011.6316. Time: 173.9285 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1065: GFLOPs: 10011.8001. Time: 173.9256 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1066: GFLOPs: 9976.6531. Time: 174.5383 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1067: GFLOPs: 9843.1805. Time: 176.9050 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1068: GFLOPs: 9976.0095. Time: 174.5495 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1069: GFLOPs: 9819.8866. Time: 177.3246 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1070: GFLOPs: 9830.9823. Time: 177.1245 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1071: GFLOPs: 9757.5254. Time: 178.4579 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1072: GFLOPs: 9747.7085. Time: 178.6377 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1073: GFLOPs: 9980.3452. Time: 174.4737 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1074: GFLOPs: 9977.5296. Time: 174.5230 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1075: GFLOPs: 9992.8514. Time: 174.2554 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1076: GFLOPs: 10035.3082. Time: 173.5181 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1077: GFLOPs: 9796.8782. Time: 177.7411 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1078: GFLOPs: 9914.7187. Time: 175.6286 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1079: GFLOPs: 9949.8580. Time: 175.0083 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1080: GFLOPs: 9815.7940. Time: 177.3986 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1081: GFLOPs: 10002.1359. Time: 174.0936 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1082: GFLOPs: 9886.6989. Time: 176.1263 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1083: GFLOPs: 9911.0538. Time: 175.6935 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1084: GFLOPs: 3196.8270. Time: 544.6988 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1085: GFLOPs: 59.4881. Time: 29271.5510 us. Best GFLOPs: 10206.7442
2024-04-30 14:30:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1086: GFLOPs: 4074.1287. Time: 427.4062 us. Best GFLOPs: 10206.7442
2024-04-30 14:34:43 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 14:34:45 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 14:34:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:34:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 801 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:35:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1205 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:35:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1611 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:35:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2013 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:35:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2417 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:35:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2822 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:35:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3225 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:35:31 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-04-30 14:35:45 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 105 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:36:03 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:36:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:36:39 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:36:44 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0157  0.9983  0.9914  0.9881  0.9867  0.9866  0.9860  0.9854  0.9853  0.9852  0.9852  0.9851  0.9850  0.9849  0.9849  0.9848
[17 : 32]:	0.9845  0.9843  0.9834  0.9832  0.9829  0.9829  0.9827  0.9826  0.9826  0.9825  0.9821  0.9819  0.9817  0.9817  0.9815  0.9813
[33 : 48]:	0.9812  0.9811  0.9807  0.9806  0.9805  0.9800  0.9799  0.9798  0.9797  0.9796  0.9791  0.9790  0.9788  0.9787  0.9784  0.9783
[49 : 64]:	0.9782  0.9780  0.9779  0.9779  0.9778  0.9774  0.9773  0.9772  0.9771  0.9768  0.9767  0.9767  0.9765  0.9764  0.9763  0.9763
2024-04-30 14:36:44 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 14:36:44 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1087: GFLOPs: 98.8191. Time: 17621.1623 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1088: GFLOPs: 10151.6250. Time: 171.5300 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1089: GFLOPs: 10106.8382. Time: 172.2901 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1090: GFLOPs: 9939.6017. Time: 175.1889 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1091: GFLOPs: 9947.5782. Time: 175.0484 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1092: GFLOPs: 10068.9406. Time: 172.9385 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1093: GFLOPs: 10041.7508. Time: 173.4068 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1094: GFLOPs: 9934.1231. Time: 175.2855 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1095: GFLOPs: 10040.2488. Time: 173.4327 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1096: GFLOPs: 9980.5888. Time: 174.4695 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1097: GFLOPs: 10110.9945. Time: 172.2193 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1098: GFLOPs: 10055.6894. Time: 173.1664 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1099: GFLOPs: 10110.1828. Time: 172.2331 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1100: GFLOPs: 9912.6719. Time: 175.6648 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1101: GFLOPs: 9942.0575. Time: 175.1456 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1102: GFLOPs: 10030.4522. Time: 173.6021 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1103: GFLOPs: 9969.4626. Time: 174.6642 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1104: GFLOPs: 9839.1252. Time: 176.9779 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1105: GFLOPs: 9753.4270. Time: 178.5329 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1106: GFLOPs: 10003.0141. Time: 174.0783 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1107: GFLOPs: 10026.1248. Time: 173.6771 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1108: GFLOPs: 10039.4530. Time: 173.4465 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1109: GFLOPs: 9875.7802. Time: 176.3210 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1110: GFLOPs: 9754.5942. Time: 178.5116 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1111: GFLOPs: 9878.7251. Time: 176.2685 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1112: GFLOPs: 9929.0510. Time: 175.3751 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1113: GFLOPs: 10060.8439. Time: 173.0777 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1114: GFLOPs: 9923.0368. Time: 175.4814 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1115: GFLOPs: 9879.1047. Time: 176.2617 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1116: GFLOPs: 9894.0486. Time: 175.9955 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1117: GFLOPs: 10023.9419. Time: 173.7149 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1118: GFLOPs: 8612.6398. Time: 202.1805 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1119: GFLOPs: 9931.9335. Time: 175.3242 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1120: GFLOPs: 9925.0094. Time: 175.4465 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1121: GFLOPs: 9949.6674. Time: 175.0117 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1122: GFLOPs: 10096.4977. Time: 172.4665 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1123: GFLOPs: 10093.4329. Time: 172.5189 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1124: GFLOPs: 10031.3401. Time: 173.5868 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1125: GFLOPs: 10044.5178. Time: 173.3590 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1126: GFLOPs: 9938.9964. Time: 175.1996 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1127: GFLOPs: 10055.6710. Time: 173.1668 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1128: GFLOPs: 9936.8460. Time: 175.2375 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1129: GFLOPs: 9864.0570. Time: 176.5306 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1130: GFLOPs: 10122.2956. Time: 172.0270 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1131: GFLOPs: 10045.7889. Time: 173.3371 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1132: GFLOPs: 10057.9955. Time: 173.1267 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1133: GFLOPs: 10032.5242. Time: 173.5663 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1134: GFLOPs: 10039.5499. Time: 173.4448 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1135: GFLOPs: 9868.9575. Time: 176.4429 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1136: GFLOPs: 10013.2892. Time: 173.8997 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1137: GFLOPs: 10078.6998. Time: 172.7711 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1138: GFLOPs: 9927.6559. Time: 175.3997 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1139: GFLOPs: 10026.5877. Time: 173.6690 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1140: GFLOPs: 10111.2003. Time: 172.2157 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1141: GFLOPs: 10150.2472. Time: 171.5533 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1142: GFLOPs: 10113.4837. Time: 172.1769 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1143: GFLOPs: 9849.3099. Time: 176.7949 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1144: GFLOPs: 10086.9108. Time: 172.6304 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1145: GFLOPs: 9935.9837. Time: 175.2527 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1146: GFLOPs: 10105.2809. Time: 172.3166 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1147: GFLOPs: 9978.0964. Time: 174.5130 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1148: GFLOPs: 3075.4463. Time: 566.1968 us. Best GFLOPs: 10206.7442
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1149: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(56), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(28) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), nu_3_init * T.int64(4) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(28) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(28) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(28) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512) // T.int64(128))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(28) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), nu_3 * T.int64(4) + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(28) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(28) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(4), ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(28) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 4])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 2, 16, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 28, 1, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 14:38:13 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1150: GFLOPs: 122.3103. Time: 14236.8001 us. Best GFLOPs: 10206.7442
2024-04-30 14:44:54 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 14:44:56 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 14:45:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1621 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2022 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2424 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2828 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3234 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3636 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:45:46 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-04-30 14:46:00 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 105 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:46:17 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:46:35 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 147 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:46:53 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 14:46:58 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9913  0.9867  0.9865  0.9865  0.9863  0.9858  0.9846  0.9842  0.9839  0.9837  0.9834  0.9833  0.9832  0.9829  0.9823  0.9817
[17 : 32]:	0.9814  0.9813  0.9813  0.9808  0.9808  0.9806  0.9806  0.9804  0.9804  0.9803  0.9801  0.9800  0.9796  0.9794  0.9793  0.9793
[33 : 48]:	0.9790  0.9790  0.9785  0.9784  0.9783  0.9782  0.9773  0.9773  0.9773  0.9769  0.9769  0.9769  0.9768  0.9765  0.9762  0.9761
[49 : 64]:	0.9757  0.9756  0.9755  0.9753  0.9753  0.9752  0.9751  0.9750  0.9749  0.9742  0.9741  0.9739  0.9739  0.9738  0.9737  0.9737
2024-04-30 14:46:58 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 14:46:58 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1151: GFLOPs: 10122.3983. Time: 172.0252 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1152: GFLOPs: 10065.8837. Time: 172.9911 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1153: GFLOPs: 10054.7300. Time: 173.1830 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1154: GFLOPs: 10153.2973. Time: 171.5017 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1155: GFLOPs: 9917.3305. Time: 175.5823 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1156: GFLOPs: 10041.1706. Time: 173.4168 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1157: GFLOPs: 10084.9832. Time: 172.6634 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1158: GFLOPs: 10097.8982. Time: 172.4426 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1159: GFLOPs: 10049.9403. Time: 173.2655 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1160: GFLOPs: 10126.1028. Time: 171.9623 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1161: GFLOPs: 10112.0838. Time: 172.2007 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1162: GFLOPs: 10150.3461. Time: 171.5516 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1163: GFLOPs: 10111.7848. Time: 172.2058 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1164: GFLOPs: 10153.2502. Time: 171.5025 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1165: GFLOPs: 10117.3753. Time: 172.1106 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1166: GFLOPs: 10114.8704. Time: 172.1533 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1167: GFLOPs: 10090.7432. Time: 172.5649 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1168: GFLOPs: 10115.4639. Time: 172.1432 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1169: GFLOPs: 9863.5123. Time: 176.5403 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1170: GFLOPs: 10150.7814. Time: 171.5442 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1171: GFLOPs: 9813.2914. Time: 177.4438 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1172: GFLOPs: 10095.0737. Time: 172.4909 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1173: GFLOPs: 10122.0983. Time: 172.0303 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1174: GFLOPs: 10162.2934. Time: 171.3499 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1175: GFLOPs: 9925.6164. Time: 175.4357 us. Best GFLOPs: 10206.7442
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1176: GFLOPs: 10207.8385. Time: 170.5854 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1177: GFLOPs: 10012.8314. Time: 173.9076 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1178: GFLOPs: 9990.1125. Time: 174.3031 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1179: GFLOPs: 10046.9706. Time: 173.3167 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1180: GFLOPs: 10092.8956. Time: 172.5281 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1181: GFLOPs: 9926.5968. Time: 175.4184 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1182: GFLOPs: 10030.3422. Time: 173.6040 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1183: GFLOPs: 10081.2945. Time: 172.7266 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1184: GFLOPs: 10125.7365. Time: 171.9685 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1185: GFLOPs: 9990.9891. Time: 174.2878 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1186: GFLOPs: 10052.2591. Time: 173.2255 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1187: GFLOPs: 10078.3706. Time: 172.7767 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1188: GFLOPs: 10082.3478. Time: 172.7086 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1189: GFLOPs: 10090.4922. Time: 172.5692 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1190: GFLOPs: 10088.9113. Time: 172.5962 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1191: GFLOPs: 9861.8674. Time: 176.5698 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1192: GFLOPs: 9859.3270. Time: 176.6153 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1193: GFLOPs: 9896.9523. Time: 175.9439 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1194: GFLOPs: 9821.9381. Time: 177.2876 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1195: GFLOPs: 9984.4672. Time: 174.4017 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1196: GFLOPs: 9906.7395. Time: 175.7700 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1197: GFLOPs: 10107.9416. Time: 172.2713 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1198: GFLOPs: 9930.0489. Time: 175.3574 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1199: GFLOPs: 10022.8933. Time: 173.7331 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1200: GFLOPs: 9959.5172. Time: 174.8386 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1201: GFLOPs: 9984.1679. Time: 174.4069 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1202: GFLOPs: 10020.4753. Time: 173.7750 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1203: GFLOPs: 10059.8434. Time: 173.0949 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1204: GFLOPs: 10084.0799. Time: 172.6789 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1205: GFLOPs: 9993.3346. Time: 174.2469 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1206: GFLOPs: 10109.5258. Time: 172.2443 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1207: GFLOPs: 10041.7816. Time: 173.4063 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1208: GFLOPs: 9847.2440. Time: 176.8320 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1209: GFLOPs: 9929.6368. Time: 175.3647 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1210: GFLOPs: 9875.4108. Time: 176.3276 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1211: GFLOPs: 9919.8855. Time: 175.5371 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1212: GFLOPs: 4187.7491. Time: 415.8100 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1213: GFLOPs: 35.8522. Time: 48569.0000 us. Best GFLOPs: 10207.8385
2024-04-30 14:48:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1214: GFLOPs: 283.2966. Time: 6146.5902 us. Best GFLOPs: 10207.8385
2024-04-30 15:02:24 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 15:02:27 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 15:02:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:02:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:02:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:02:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1613 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:02:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:03:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2415 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:03:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2818 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:03:06 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-04-30 15:03:21 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 103 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:03:39 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:03:56 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:04:15 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:04:19 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3029  0.9907  0.9893  0.9890  0.9869  0.9866  0.9857  0.9849  0.9840  0.9838  0.9837  0.9835  0.9833  0.9822  0.9819  0.9819
[17 : 32]:	0.9816  0.9816  0.9813  0.9813  0.9813  0.9811  0.9799  0.9796  0.9790  0.9790  0.9787  0.9784  0.9781  0.9779  0.9778  0.9778
[33 : 48]:	0.9777  0.9776  0.9773  0.9771  0.9769  0.9769  0.9769  0.9767  0.9765  0.9765  0.9764  0.9763  0.9758  0.9757  0.9753  0.9753
[49 : 64]:	0.9751  0.9750  0.9748  0.9745  0.9745  0.9743  0.9740  0.9740  0.9739  0.9738  0.9736  0.9736  0.9736  0.9734  0.9734  0.9733
2024-04-30 15:04:20 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 15:04:20 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1215: GFLOPs: 512.0828. Time: 3400.4421 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1216: GFLOPs: 9755.5978. Time: 178.4932 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1217: GFLOPs: 9867.1645. Time: 176.4750 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1218: GFLOPs: 10099.6148. Time: 172.4133 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1219: GFLOPs: 10086.3527. Time: 172.6400 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1220: GFLOPs: 10077.0133. Time: 172.8000 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1221: GFLOPs: 10044.1591. Time: 173.3652 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1222: GFLOPs: 10027.0162. Time: 173.6616 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1223: GFLOPs: 9870.0294. Time: 176.4238 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1224: GFLOPs: 9926.7659. Time: 175.4154 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1225: GFLOPs: 9954.4416. Time: 174.9277 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1226: GFLOPs: 9958.9332. Time: 174.8488 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1227: GFLOPs: 9910.6772. Time: 175.7002 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1228: GFLOPs: 9875.7003. Time: 176.3225 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1229: GFLOPs: 10031.5489. Time: 173.5832 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1230: GFLOPs: 7157.9581. Time: 243.2688 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1231: GFLOPs: 9990.6421. Time: 174.2939 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1232: GFLOPs: 9939.2696. Time: 175.1948 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1233: GFLOPs: 10082.0248. Time: 172.7141 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1234: GFLOPs: 9760.9036. Time: 178.3962 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1235: GFLOPs: 9958.2637. Time: 174.8606 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1236: GFLOPs: 9866.1388. Time: 176.4934 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1237: GFLOPs: 9929.1861. Time: 175.3727 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1238: GFLOPs: 10014.4668. Time: 173.8792 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1239: GFLOPs: 9926.7575. Time: 175.4156 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1240: GFLOPs: 10103.6707. Time: 172.3441 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1241: GFLOPs: 9936.9537. Time: 175.2356 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1242: GFLOPs: 10045.2359. Time: 173.3466 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1243: GFLOPs: 10052.5812. Time: 173.2200 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1244: GFLOPs: 9975.8564. Time: 174.5522 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1245: GFLOPs: 9983.0798. Time: 174.4259 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1246: GFLOPs: 9911.3690. Time: 175.6879 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1247: GFLOPs: 9948.8377. Time: 175.0263 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1248: GFLOPs: 9984.0505. Time: 174.4090 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1249: GFLOPs: 10086.4292. Time: 172.6387 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1250: GFLOPs: 9925.2332. Time: 175.4425 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1251: GFLOPs: 9860.9437. Time: 176.5863 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1252: GFLOPs: 9973.4878. Time: 174.5937 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1253: GFLOPs: 9955.5915. Time: 174.9075 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1254: GFLOPs: 9793.5182. Time: 177.8021 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1255: GFLOPs: 9915.2830. Time: 175.6186 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1256: GFLOPs: 9796.0901. Time: 177.7554 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1257: GFLOPs: 10098.6294. Time: 172.4301 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1258: GFLOPs: 9589.7734. Time: 181.5797 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1259: GFLOPs: 9721.1550. Time: 179.1256 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1260: GFLOPs: 10018.8931. Time: 173.8024 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1261: GFLOPs: 9947.7287. Time: 175.0458 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1262: GFLOPs: 9977.1614. Time: 174.5294 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1263: GFLOPs: 10050.1344. Time: 173.2622 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1264: GFLOPs: 9991.8601. Time: 174.2726 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1265: GFLOPs: 10084.4098. Time: 172.6733 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1266: GFLOPs: 9862.3534. Time: 176.5611 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1267: GFLOPs: 9999.9365. Time: 174.1319 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1268: GFLOPs: 9905.8018. Time: 175.7867 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1269: GFLOPs: 9819.3106. Time: 177.3350 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1270: GFLOPs: 9815.9015. Time: 177.3966 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1271: GFLOPs: 10094.8034. Time: 172.4955 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1272: GFLOPs: 9839.4431. Time: 176.9722 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1273: GFLOPs: 9866.7258. Time: 176.4829 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1274: GFLOPs: 9859.6954. Time: 176.6087 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1275: GFLOPs: 9676.8195. Time: 179.9463 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1276: GFLOPs: 139.1696. Time: 12512.1279 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1277: GFLOPs: 157.5013. Time: 11055.8351 us. Best GFLOPs: 10207.8385
2024-04-30 15:05:23 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1278: GFLOPs: 3972.8427. Time: 438.3028 us. Best GFLOPs: 10207.8385
2024-04-30 15:19:21 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 15:19:24 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 15:19:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:19:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:19:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:19:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:19:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2019 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:19:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2418 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:20:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2823 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:20:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3225 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:20:09 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-04-30 15:20:24 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:20:41 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:20:58 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 135 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:21:16 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:21:20 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0548  0.9876  0.9874  0.9869  0.9853  0.9846  0.9839  0.9830  0.9826  0.9813  0.9812  0.9805  0.9803  0.9802  0.9801  0.9801
[17 : 32]:	0.9801  0.9795  0.9793  0.9793  0.9791  0.9789  0.9786  0.9785  0.9784  0.9783  0.9783  0.9782  0.9782  0.9781  0.9781  0.9780
[33 : 48]:	0.9778  0.9778  0.9776  0.9776  0.9769  0.9769  0.9768  0.9768  0.9767  0.9764  0.9761  0.9761  0.9761  0.9760  0.9760  0.9758
[49 : 64]:	0.9757  0.9757  0.9756  0.9756  0.9755  0.9754  0.9754  0.9753  0.9753  0.9753  0.9752  0.9752  0.9747  0.9747  0.9746  0.9746
2024-04-30 15:21:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 15:21:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1279: GFLOPs: 5944.5042. Time: 292.9274 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1280: GFLOPs: 10090.1513. Time: 172.5750 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1281: GFLOPs: 10078.1846. Time: 172.7799 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1282: GFLOPs: 10131.7907. Time: 171.8658 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1283: GFLOPs: 10122.0000. Time: 172.0320 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1284: GFLOPs: 9888.1505. Time: 176.1005 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1285: GFLOPs: 9900.4150. Time: 175.8823 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1286: GFLOPs: 9940.2003. Time: 175.1784 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1287: GFLOPs: 10048.9142. Time: 173.2832 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1288: GFLOPs: 10099.1606. Time: 172.4211 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1289: GFLOPs: 9904.4985. Time: 175.8098 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1290: GFLOPs: 10048.7842. Time: 173.2854 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1291: GFLOPs: 9777.6247. Time: 178.0911 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1292: GFLOPs: 9998.3559. Time: 174.1594 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1293: GFLOPs: 10119.6570. Time: 172.0718 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1294: GFLOPs: 10078.3173. Time: 172.7776 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1295: GFLOPs: 9874.0953. Time: 176.3511 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1296: GFLOPs: 9937.0961. Time: 175.2331 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1297: GFLOPs: 9958.5420. Time: 174.8557 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1298: GFLOPs: 9930.0700. Time: 175.3571 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1299: GFLOPs: 9921.6370. Time: 175.5061 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1300: GFLOPs: 9992.1866. Time: 174.2670 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1301: GFLOPs: 10029.2355. Time: 173.6232 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1302: GFLOPs: 9997.9767. Time: 174.1660 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1303: GFLOPs: 9928.5687. Time: 175.3836 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1304: GFLOPs: 9911.2871. Time: 175.6894 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1305: GFLOPs: 10036.3759. Time: 173.4997 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1306: GFLOPs: 10016.4244. Time: 173.8453 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1307: GFLOPs: 10053.7333. Time: 173.2001 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1308: GFLOPs: 9846.1960. Time: 176.8508 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1309: GFLOPs: 10017.7711. Time: 173.8219 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1310: GFLOPs: 9978.0779. Time: 174.5134 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1311: GFLOPs: 9971.5051. Time: 174.6284 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1312: GFLOPs: 9885.6516. Time: 176.1450 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1313: GFLOPs: 10043.1712. Time: 173.3823 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1314: GFLOPs: 9893.1428. Time: 176.0116 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1315: GFLOPs: 10052.3699. Time: 173.2236 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1316: GFLOPs: 10048.5752. Time: 173.2890 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1317: GFLOPs: 9803.9269. Time: 177.6133 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1318: GFLOPs: 9843.6813. Time: 176.8960 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1319: GFLOPs: 9951.7195. Time: 174.9756 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1320: GFLOPs: 9985.3844. Time: 174.3857 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1321: GFLOPs: 9934.5936. Time: 175.2772 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1322: GFLOPs: 10004.0905. Time: 174.0596 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1323: GFLOPs: 9934.2595. Time: 175.2831 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1324: GFLOPs: 9813.3318. Time: 177.4431 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1325: GFLOPs: 10024.6426. Time: 173.7027 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1326: GFLOPs: 9896.4847. Time: 175.9522 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1327: GFLOPs: 9890.0044. Time: 176.0675 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1328: GFLOPs: 9791.4361. Time: 177.8399 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1329: GFLOPs: 9846.7796. Time: 176.8403 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1330: GFLOPs: 9998.0653. Time: 174.1645 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1331: GFLOPs: 9831.7839. Time: 177.1101 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1332: GFLOPs: 10019.5091. Time: 173.7917 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1333: GFLOPs: 10039.4299. Time: 173.4469 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1334: GFLOPs: 9967.2752. Time: 174.7025 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1335: GFLOPs: 9845.0468. Time: 176.8715 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1336: GFLOPs: 9842.1951. Time: 176.9227 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1337: GFLOPs: 10019.8415. Time: 173.7860 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1338: GFLOPs: 9644.6487. Time: 180.5465 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1339: GFLOPs: 9753.6167. Time: 178.5295 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1340: GFLOPs: 3654.5100. Time: 476.4819 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1341: GFLOPs: 2519.9474. Time: 691.0096 us. Best GFLOPs: 10207.8385
2024-04-30 15:22:22 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1342: GFLOPs: 72.5703. Time: 23994.7768 us. Best GFLOPs: 10207.8385
2024-04-30 15:36:02 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 15:36:04 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 15:36:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:36:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:36:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:36:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1612 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:36:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2014 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:36:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2422 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:36:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2829 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:36:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3230 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:36:51 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-04-30 15:37:06 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:37:24 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 115 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:37:43 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:38:02 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:38:06 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9930  0.9910  0.9884  0.9869  0.9869  0.9865  0.9852  0.9845  0.9840  0.9828  0.9824  0.9819  0.9815  0.9815  0.9811  0.9810
[17 : 32]:	0.9806  0.9805  0.9804  0.9803  0.9802  0.9798  0.9794  0.9794  0.9791  0.9788  0.9788  0.9788  0.9781  0.9775  0.9775  0.9775
[33 : 48]:	0.9773  0.9773  0.9773  0.9772  0.9771  0.9771  0.9770  0.9770  0.9769  0.9767  0.9767  0.9765  0.9765  0.9765  0.9765  0.9765
[49 : 64]:	0.9763  0.9763  0.9758  0.9754  0.9754  0.9752  0.9751  0.9750  0.9748  0.9748  0.9747  0.9745  0.9742  0.9741  0.9739  0.9739
2024-04-30 15:38:07 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 15:38:07 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1343: GFLOPs: 10097.5154. Time: 172.4491 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1344: GFLOPs: 9958.0895. Time: 174.8637 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1345: GFLOPs: 10113.7816. Time: 172.1718 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1346: GFLOPs: 10094.6874. Time: 172.4975 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1347: GFLOPs: 10095.4801. Time: 172.4839 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1348: GFLOPs: 10064.5676. Time: 173.0137 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1349: GFLOPs: 10055.5819. Time: 173.1683 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1350: GFLOPs: 10070.3663. Time: 172.9141 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1351: GFLOPs: 9778.0140. Time: 178.0840 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1352: GFLOPs: 9938.6853. Time: 175.2051 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1353: GFLOPs: 9847.9211. Time: 176.8198 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1354: GFLOPs: 9992.0365. Time: 174.2696 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1355: GFLOPs: 10090.0866. Time: 172.5761 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1356: GFLOPs: 9997.5892. Time: 174.1728 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1357: GFLOPs: 9980.2046. Time: 174.4762 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1358: GFLOPs: 9779.7552. Time: 178.0523 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1359: GFLOPs: 10097.1064. Time: 172.4561 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1360: GFLOPs: 10089.8433. Time: 172.5803 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1361: GFLOPs: 10015.9997. Time: 173.8526 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1362: GFLOPs: 9990.2350. Time: 174.3010 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1363: GFLOPs: 9763.6135. Time: 178.3467 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1364: GFLOPs: 9829.5515. Time: 177.1503 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1365: GFLOPs: 10096.1444. Time: 172.4726 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1366: GFLOPs: 9749.7534. Time: 178.6002 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1367: GFLOPs: 9766.5262. Time: 178.2935 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1368: GFLOPs: 9993.7323. Time: 174.2400 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1369: GFLOPs: 9847.0498. Time: 176.8355 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1370: GFLOPs: 9761.8210. Time: 178.3794 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1371: GFLOPs: 10028.1578. Time: 173.6419 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1372: GFLOPs: 9955.7768. Time: 174.9043 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1373: GFLOPs: 9963.7796. Time: 174.7638 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1374: GFLOPs: 9807.9554. Time: 177.5404 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1375: GFLOPs: 9913.2201. Time: 175.6551 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1376: GFLOPs: 9913.9274. Time: 175.6426 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1377: GFLOPs: 9936.8290. Time: 175.2378 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1378: GFLOPs: 9863.1679. Time: 176.5465 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1379: GFLOPs: 9776.7363. Time: 178.1073 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1380: GFLOPs: 9845.0636. Time: 176.8712 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1381: GFLOPs: 9939.1893. Time: 175.1962 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1382: GFLOPs: 9938.0063. Time: 175.2170 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1383: GFLOPs: 9950.2879. Time: 175.0008 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1384: GFLOPs: 9913.6310. Time: 175.6478 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1385: GFLOPs: 9943.7536. Time: 175.1158 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1386: GFLOPs: 10033.6477. Time: 173.5468 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1387: GFLOPs: 10067.6117. Time: 172.9614 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1388: GFLOPs: 9992.4766. Time: 174.2619 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1389: GFLOPs: 9878.6864. Time: 176.2692 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1390: GFLOPs: 8542.6067. Time: 203.8380 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1391: GFLOPs: 9940.0612. Time: 175.1808 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1392: GFLOPs: 10017.3821. Time: 173.8286 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1393: GFLOPs: 9761.2549. Time: 178.3898 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1394: GFLOPs: 9940.9676. Time: 175.1648 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1395: GFLOPs: 9875.6220. Time: 176.3239 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1396: GFLOPs: 9911.4528. Time: 175.6864 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1397: GFLOPs: 9958.3788. Time: 174.8586 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1398: GFLOPs: 9845.3453. Time: 176.8661 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1399: GFLOPs: 10003.9876. Time: 174.0614 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1400: GFLOPs: 10002.1462. Time: 174.0934 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1401: GFLOPs: 9998.1543. Time: 174.1629 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1402: GFLOPs: 9869.1794. Time: 176.4390 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1403: GFLOPs: 9935.5120. Time: 175.2610 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1404: GFLOPs: 162.7861. Time: 10696.9085 us. Best GFLOPs: 10207.8385
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1405: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(28), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(224), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(56) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused % T.int64(56) // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(28) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(128))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(56) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused % T.int64(56) // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(28) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(56) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(7) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused % T.int64(56) // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused % T.int64(28) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 2, 16, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 28, 1, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144 = sch.split(loop=l142, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184 = sch.get_loops(block=b150)
l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l185, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l185, ann_key="pragma_unroll_explicit", ann_val=1)
l199, l200, l201, l202, l203, l204, l205 = sch.get_loops(block=b152)
l206, l207, l208, l209, l210, l211, l212, l213 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l206, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l206, ann_key="pragma_unroll_explicit", ann_val=1)
l214, l215, l216, l217 = sch.get_loops(block=b154)
b218 = sch.get_block(name="data_pack", func_name="main")
l219, l220, l221, l222, l223, l224 = sch.get_loops(block=b218)
b225 = sch.decompose_reduction(block=b218, loop=l223)
b226 = sch.get_block(name="bgemm", func_name="main")
l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240 = sch.get_loops(block=b226)
b241 = sch.decompose_reduction(block=b226, loop=l230)
b242 = sch.get_block(name="inverse", func_name="main")
l243, l244, l245, l246, l247, l248, l249, l250 = sch.get_loops(block=b242)
b251 = sch.decompose_reduction(block=b242, loop=l249)
2024-04-30 15:39:36 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1406: GFLOPs: 1838.8992. Time: 946.9295 us. Best GFLOPs: 10207.8385
2024-04-30 15:48:22 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 15:48:24 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 15:48:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:48:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:48:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:48:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:48:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2017 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:48:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2423 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:49:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2827 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:49:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3229 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:49:08 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-04-30 15:49:23 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 118 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:49:40 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:49:58 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 158 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:50:16 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:50:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9860  0.9830  0.9827  0.9824  0.9824  0.9819  0.9818  0.9813  0.9808  0.9801  0.9800  0.9799  0.9797  0.9794  0.9794  0.9794
[17 : 32]:	0.9787  0.9785  0.9780  0.9778  0.9778  0.9778  0.9777  0.9774  0.9771  0.9765  0.9765  0.9764  0.9760  0.9758  0.9758  0.9755
[33 : 48]:	0.9753  0.9752  0.9750  0.9749  0.9748  0.9747  0.9747  0.9746  0.9746  0.9746  0.9745  0.9745  0.9744  0.9742  0.9742  0.9741
[49 : 64]:	0.9738  0.9737  0.9736  0.9733  0.9733  0.9729  0.9729  0.9728  0.9727  0.9725  0.9725  0.9724  0.9723  0.9722  0.9721  0.9720
2024-04-30 15:50:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 15:50:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1407: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(7) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(7) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(32) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(7)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(7) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 32, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 1, 4, 7, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[16, 32, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l130, l131, l132, l133, l134 = sch.get_loops(block=b97)
l135, l136 = sch.split(loop=l134, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l136, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l137, l138, l139, l140, l141 = sch.get_loops(block=b108)
l142, l143, l144 = sch.split(loop=l141, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l144)
sch.bind(loop=l143, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b150)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b152)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b154)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1408: GFLOPs: 10114.7564. Time: 172.1552 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1409: GFLOPs: 9775.7039. Time: 178.1261 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1410: GFLOPs: 10081.7646. Time: 172.7186 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1411: GFLOPs: 10141.2381. Time: 171.7057 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1412: GFLOPs: 10128.4073. Time: 171.9232 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1413: GFLOPs: 10063.4777. Time: 173.0324 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1414: GFLOPs: 10127.8812. Time: 171.9321 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1415: GFLOPs: 9939.5371. Time: 175.1900 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1416: GFLOPs: 10020.1295. Time: 173.7810 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1417: GFLOPs: 9987.2193. Time: 174.3536 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1418: GFLOPs: 10026.0699. Time: 173.6780 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1419: GFLOPs: 10021.4781. Time: 173.7576 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1420: GFLOPs: 10018.8142. Time: 173.8038 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1421: GFLOPs: 10060.8484. Time: 173.0776 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1422: GFLOPs: 10014.0649. Time: 173.8862 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1423: GFLOPs: 10015.2391. Time: 173.8658 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1424: GFLOPs: 9935.0867. Time: 175.2685 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1425: GFLOPs: 9917.4194. Time: 175.5807 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1426: GFLOPs: 9926.0300. Time: 175.4284 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1427: GFLOPs: 9943.1842. Time: 175.1258 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1428: GFLOPs: 9937.8475. Time: 175.2198 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1429: GFLOPs: 10014.0649. Time: 173.8862 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1430: GFLOPs: 9903.3119. Time: 175.8309 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1431: GFLOPs: 9991.2208. Time: 174.2838 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1432: GFLOPs: 10022.1837. Time: 173.7454 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1433: GFLOPs: 9891.9455. Time: 176.0329 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1434: GFLOPs: 10048.7744. Time: 173.2856 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1435: GFLOPs: 9982.4231. Time: 174.4374 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1436: GFLOPs: 9947.2234. Time: 175.0547 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1437: GFLOPs: 9845.5012. Time: 176.8633 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1438: GFLOPs: 9987.5834. Time: 174.3473 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1439: GFLOPs: 9783.7384. Time: 177.9798 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1440: GFLOPs: 10007.1497. Time: 174.0064 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1441: GFLOPs: 10086.7225. Time: 172.6337 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1442: GFLOPs: 9915.7632. Time: 175.6101 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1443: GFLOPs: 9839.7038. Time: 176.9675 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1444: GFLOPs: 9983.7568. Time: 174.4141 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1445: GFLOPs: 9992.4319. Time: 174.2627 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1446: GFLOPs: 9991.0683. Time: 174.2865 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1447: GFLOPs: 9912.9238. Time: 175.6604 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1448: GFLOPs: 10020.5718. Time: 173.7733 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1449: GFLOPs: 9900.2547. Time: 175.8852 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1450: GFLOPs: 10085.2975. Time: 172.6581 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1451: GFLOPs: 10034.3787. Time: 173.5342 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1452: GFLOPs: 9965.9784. Time: 174.7252 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1453: GFLOPs: 9945.0855. Time: 175.0923 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1454: GFLOPs: 9873.9316. Time: 176.3541 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1455: GFLOPs: 9877.3155. Time: 176.2936 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1456: GFLOPs: 10000.0388. Time: 174.1301 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1457: GFLOPs: 9947.7481. Time: 175.0454 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1458: GFLOPs: 9992.5169. Time: 174.2612 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1459: GFLOPs: 9878.1032. Time: 176.2796 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1460: GFLOPs: 9951.1207. Time: 174.9861 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1461: GFLOPs: 9921.1002. Time: 175.5156 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1462: GFLOPs: 9843.3294. Time: 176.9023 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1463: GFLOPs: 9864.6477. Time: 176.5200 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1464: GFLOPs: 9920.0051. Time: 175.5350 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1465: GFLOPs: 9940.7167. Time: 175.1693 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1466: GFLOPs: 9993.2797. Time: 174.2479 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1467: GFLOPs: 10087.8671. Time: 172.6141 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1468: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(2), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(256) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(49) * T.int64(32) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(49) * T.int64(4) + p_3_init * T.int64(4) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(256)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(392))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(392) // T.int64(196))
                                    v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(196))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(11)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(256))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 < T.int64(2048))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(32), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(256) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(49) * T.int64(32) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(49) * T.int64(4) + p_3 * T.int64(4) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(2) + ci_1 * T.int64(2) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(32), T.int64(4)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(256) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused // T.int64(49) * T.int64(32) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(49) * T.int64(4) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 2, 4, 32, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 49, 1, 4])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l130, l131, l132, l133, l134 = sch.get_loops(block=b97)
l135, l136 = sch.split(loop=l134, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l136, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l137, l138, l139, l140, l141 = sch.get_loops(block=b108)
l142, l143 = sch.split(loop=l141, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l143, thread_axis="threadIdx.x")
b144 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b144, ann_key="meta_schedule.unroll_explicit")
b145, b146, b147, b148, b149, b150, b151, b152, b153 = sch.get_child_blocks(b144)
l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b145)
l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b146)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b147)
l172, l173, l174, l175, l176, l177 = sch.get_loops(block=b148)
l178, l179, l180, l181, l182, l183 = sch.get_loops(block=b149)
l184, l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l184, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l184, ann_key="pragma_unroll_explicit", ann_val=1)
l198, l199, l200, l201, l202, l203, l204 = sch.get_loops(block=b151)
l205, l206, l207, l208, l209, l210, l211, l212 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l205, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l205, ann_key="pragma_unroll_explicit", ann_val=1)
l213, l214, l215, l216 = sch.get_loops(block=b153)
b217 = sch.get_block(name="data_pack", func_name="main")
l218, l219, l220, l221, l222, l223 = sch.get_loops(block=b217)
b224 = sch.decompose_reduction(block=b217, loop=l222)
b225 = sch.get_block(name="bgemm", func_name="main")
l226, l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239 = sch.get_loops(block=b225)
b240 = sch.decompose_reduction(block=b225, loop=l229)
b241 = sch.get_block(name="inverse", func_name="main")
l242, l243, l244, l245, l246, l247, l248, l249 = sch.get_loops(block=b241)
b250 = sch.decompose_reduction(block=b241, loop=l248)
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1469: GFLOPs: 10.0739. Time: 172853.2510 us. Best GFLOPs: 10207.8385
2024-04-30 15:51:58 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1470: GFLOPs: 14.3355. Time: 121468.2413 us. Best GFLOPs: 10207.8385
2024-04-30 15:55:54 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 15:55:57 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 15:56:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:56:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:56:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1213 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:56:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:56:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:56:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2417 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:56:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2817 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:56:36 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-04-30 15:56:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 118 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:57:08 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:57:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:57:44 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 15:57:49 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9883  0.9882  0.9856  0.9850  0.9844  0.9842  0.9840  0.9838  0.9832  0.9829  0.9826  0.9820  0.9819  0.9819  0.9815  0.9814
[17 : 32]:	0.9813  0.9813  0.9810  0.9805  0.9805  0.9799  0.9798  0.9797  0.9796  0.9791  0.9785  0.9785  0.9784  0.9784  0.9776  0.9775
[33 : 48]:	0.9774  0.9774  0.9774  0.9773  0.9771  0.9771  0.9770  0.9770  0.9769  0.9768  0.9767  0.9766  0.9766  0.9765  0.9764  0.9763
[49 : 64]:	0.9762  0.9761  0.9760  0.9759  0.9755  0.9754  0.9753  0.9752  0.9752  0.9751  0.9750  0.9750  0.9748  0.9747  0.9744  0.9744
2024-04-30 15:57:49 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 15:57:49 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1471: GFLOPs: 10014.5183. Time: 173.8783 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1472: GFLOPs: 10063.5634. Time: 173.0309 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1473: GFLOPs: 10137.1834. Time: 171.7743 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1474: GFLOPs: 10143.8671. Time: 171.6612 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1475: GFLOPs: 10104.5064. Time: 172.3298 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1476: GFLOPs: 10093.4041. Time: 172.5194 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1477: GFLOPs: 10032.6664. Time: 173.5638 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1478: GFLOPs: 10062.1068. Time: 173.0560 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1479: GFLOPs: 9865.7992. Time: 176.4994 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1480: GFLOPs: 10026.0288. Time: 173.6787 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1481: GFLOPs: 10092.7978. Time: 172.5298 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1482: GFLOPs: 10092.3068. Time: 172.5381 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1483: GFLOPs: 10018.4213. Time: 173.8106 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1484: GFLOPs: 10049.5423. Time: 173.2724 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1485: GFLOPs: 10028.4191. Time: 173.6373 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1486: GFLOPs: 10083.1869. Time: 172.6942 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1487: GFLOPs: 10033.3951. Time: 173.5512 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1488: GFLOPs: 9949.1728. Time: 175.0204 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1489: GFLOPs: 10053.4977. Time: 173.2042 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1490: GFLOPs: 10017.5539. Time: 173.8257 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1491: GFLOPs: 10073.9321. Time: 172.8529 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1492: GFLOPs: 9945.3874. Time: 175.0870 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1493: GFLOPs: 8519.7167. Time: 204.3857 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1494: GFLOPs: 10128.1935. Time: 171.9268 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1495: GFLOPs: 10075.4945. Time: 172.8260 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1496: GFLOPs: 9841.2843. Time: 176.9391 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1497: GFLOPs: 9842.5572. Time: 176.9162 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1498: GFLOPs: 9998.0244. Time: 174.1652 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1499: GFLOPs: 9870.0060. Time: 176.4242 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1500: GFLOPs: 10036.4623. Time: 173.4982 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1501: GFLOPs: 9914.6163. Time: 175.6304 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1502: GFLOPs: 8565.5821. Time: 203.2913 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1503: GFLOPs: 9916.3397. Time: 175.5999 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1504: GFLOPs: 9999.4636. Time: 174.1401 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1505: GFLOPs: 10029.3513. Time: 173.6212 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1506: GFLOPs: 10020.8326. Time: 173.7688 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1507: GFLOPs: 10006.4535. Time: 174.0185 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1508: GFLOPs: 10001.5867. Time: 174.1032 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1509: GFLOPs: 9958.7903. Time: 174.8513 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1510: GFLOPs: 9941.4572. Time: 175.1562 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1511: GFLOPs: 9971.2387. Time: 174.6331 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1512: GFLOPs: 10054.0981. Time: 173.1938 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1513: GFLOPs: 9941.1181. Time: 175.1622 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1514: GFLOPs: 9872.7988. Time: 176.3743 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1515: GFLOPs: 9801.5512. Time: 177.6564 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1516: GFLOPs: 10007.8955. Time: 173.9934 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1517: GFLOPs: 9837.2281. Time: 177.0120 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1518: GFLOPs: 9860.2982. Time: 176.5979 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1519: GFLOPs: 9942.3313. Time: 175.1408 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1520: GFLOPs: 9939.1630. Time: 175.1966 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1521: GFLOPs: 10010.8075. Time: 173.9428 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1522: GFLOPs: 9903.3426. Time: 175.8303 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1523: GFLOPs: 9876.5326. Time: 176.3076 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1524: GFLOPs: 9903.2059. Time: 175.8327 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1525: GFLOPs: 9896.5744. Time: 175.9506 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1526: GFLOPs: 9896.4897. Time: 175.9521 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1527: GFLOPs: 9927.7120. Time: 175.3987 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1528: GFLOPs: 10005.0228. Time: 174.0434 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1529: GFLOPs: 10021.9250. Time: 173.7498 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1530: GFLOPs: 9948.4824. Time: 175.0325 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1531: GFLOPs: 9895.2580. Time: 175.9740 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1532: GFLOPs: 149.7223. Time: 11630.2507 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1533: GFLOPs: 496.8353. Time: 3504.7989 us. Best GFLOPs: 10207.8385
2024-04-30 15:58:53 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1534: GFLOPs: 1571.8943. Time: 1107.7767 us. Best GFLOPs: 10207.8385
2024-04-30 16:02:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 16:02:33 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 16:02:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:02:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 802 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:02:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:02:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1612 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:03:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2017 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:03:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2418 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:03:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2821 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:03:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3223 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:03:19 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2024-04-30 16:03:34 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:03:52 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:04:10 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:04:28 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:04:33 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9861  0.9857  0.9853  0.9853  0.9850  0.9837  0.9829  0.9822  0.9821  0.9816  0.9816  0.9813  0.9811  0.9811  0.9798  0.9793
[17 : 32]:	0.9791  0.9790  0.9790  0.9787  0.9778  0.9776  0.9776  0.9775  0.9773  0.9773  0.9770  0.9770  0.9770  0.9769  0.9769  0.9768
[33 : 48]:	0.9768  0.9766  0.9766  0.9765  0.9760  0.9760  0.9758  0.9756  0.9754  0.9754  0.9751  0.9750  0.9749  0.9747  0.9746  0.9746
[49 : 64]:	0.9745  0.9744  0.9743  0.9743  0.9743  0.9742  0.9741  0.9740  0.9739  0.9739  0.9735  0.9734  0.9733  0.9732  0.9731  0.9728
2024-04-30 16:04:33 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 16:04:34 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1535: GFLOPs: 10120.3342. Time: 172.0603 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1536: GFLOPs: 10141.1348. Time: 171.7074 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1537: GFLOPs: 10124.7396. Time: 171.9855 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1538: GFLOPs: 10136.8146. Time: 171.7806 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1539: GFLOPs: 10095.1103. Time: 172.4902 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1540: GFLOPs: 10070.3824. Time: 172.9138 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1541: GFLOPs: 10048.5047. Time: 173.2903 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1542: GFLOPs: 10136.7666. Time: 171.7814 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1543: GFLOPs: 10049.6539. Time: 173.2704 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1544: GFLOPs: 10039.0827. Time: 173.4529 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1545: GFLOPs: 10155.7378. Time: 171.4605 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1546: GFLOPs: 10074.8880. Time: 172.8365 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1547: GFLOPs: 10137.2337. Time: 171.7735 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1548: GFLOPs: 10106.0104. Time: 172.3042 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1549: GFLOPs: 10117.0799. Time: 172.1157 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1550: GFLOPs: 9954.3786. Time: 174.9288 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1551: GFLOPs: 9944.9977. Time: 175.0938 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1552: GFLOPs: 10152.1656. Time: 171.5208 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1553: GFLOPs: 10110.9861. Time: 172.2194 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1554: GFLOPs: 9953.6576. Time: 174.9415 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1555: GFLOPs: 9949.6147. Time: 175.0126 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1556: GFLOPs: 10049.6894. Time: 173.2698 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1557: GFLOPs: 10038.0560. Time: 173.4706 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1558: GFLOPs: 10038.0560. Time: 173.4706 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1559: GFLOPs: 10055.9148. Time: 173.1626 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1560: GFLOPs: 10120.3375. Time: 172.0603 us. Best GFLOPs: 10207.8385
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1561: GFLOPs: 10220.5294. Time: 170.3736 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1562: GFLOPs: 9958.5970. Time: 174.8547 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1563: GFLOPs: 10011.4135. Time: 173.9323 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1564: GFLOPs: 9936.7624. Time: 175.2390 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1565: GFLOPs: 10023.9481. Time: 173.7148 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1566: GFLOPs: 9970.9782. Time: 174.6376 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1567: GFLOPs: 9950.9195. Time: 174.9896 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1568: GFLOPs: 10014.3665. Time: 173.8810 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1569: GFLOPs: 9949.8851. Time: 175.0078 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1570: GFLOPs: 10058.2358. Time: 173.1226 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1571: GFLOPs: 9939.9864. Time: 175.1821 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1572: GFLOPs: 10038.3277. Time: 173.4659 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1573: GFLOPs: 9920.3366. Time: 175.5291 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1574: GFLOPs: 10077.2330. Time: 172.7962 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1575: GFLOPs: 9930.4300. Time: 175.3507 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1576: GFLOPs: 9943.2696. Time: 175.1243 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1577: GFLOPs: 9986.9489. Time: 174.3583 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1578: GFLOPs: 10061.9131. Time: 173.0593 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1579: GFLOPs: 10031.6071. Time: 173.5821 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1580: GFLOPs: 10132.9303. Time: 171.8464 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1581: GFLOPs: 10056.5640. Time: 173.1514 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1582: GFLOPs: 10028.3668. Time: 173.6382 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1583: GFLOPs: 10054.5241. Time: 173.1865 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1584: GFLOPs: 10002.8204. Time: 174.0817 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1585: GFLOPs: 9873.9457. Time: 176.3538 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1586: GFLOPs: 9956.0084. Time: 174.9002 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1587: GFLOPs: 10066.2644. Time: 172.9845 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1588: GFLOPs: 10097.7045. Time: 172.4459 us. Best GFLOPs: 10220.5294
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1589: GFLOPs: 10230.8661. Time: 170.2014 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1590: GFLOPs: 10029.5245. Time: 173.6182 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1591: GFLOPs: 10111.7095. Time: 172.2071 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1592: GFLOPs: 9964.6734. Time: 174.7481 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1593: GFLOPs: 10028.9016. Time: 173.6290 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1594: GFLOPs: 10073.0859. Time: 172.8674 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1595: GFLOPs: 10071.0502. Time: 172.9023 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1596: GFLOPs: 300.1493. Time: 5801.4721 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1597: GFLOPs: 119.0943. Time: 14621.2571 us. Best GFLOPs: 10230.8661
2024-04-30 16:05:41 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1598: GFLOPs: 530.9126. Time: 3279.8390 us. Best GFLOPs: 10230.8661
2024-04-30 16:17:36 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 16:17:39 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 16:17:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:17:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:17:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:18:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1613 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:18:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2014 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:18:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2420 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:18:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2822 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:18:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3225 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:18:26 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-04-30 16:18:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:18:58 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:19:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:19:33 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:19:38 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9876  0.9865  0.9862  0.9859  0.9855  0.9845  0.9844  0.9801  0.9800  0.9799  0.9798  0.9794  0.9790  0.9788  0.9787  0.9784
[17 : 32]:	0.9783  0.9775  0.9772  0.9771  0.9769  0.9769  0.9769  0.9761  0.9758  0.9754  0.9754  0.9754  0.9754  0.9752  0.9752  0.9749
[33 : 48]:	0.9747  0.9746  0.9745  0.9745  0.9744  0.9743  0.9742  0.9739  0.9739  0.9739  0.9738  0.9738  0.9738  0.9736  0.9736  0.9735
[49 : 64]:	0.9732  0.9732  0.9731  0.9731  0.9731  0.9731  0.9730  0.9729  0.9726  0.9725  0.9721  0.9721  0.9720  0.9720  0.9720  0.9719
2024-04-30 16:19:38 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 16:19:38 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1599: GFLOPs: 10133.9440. Time: 171.8292 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1600: GFLOPs: 10101.3074. Time: 172.3844 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1601: GFLOPs: 10141.5994. Time: 171.6995 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1602: GFLOPs: 10036.4208. Time: 173.4989 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1603: GFLOPs: 10042.4284. Time: 173.3951 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1604: GFLOPs: 10136.3234. Time: 171.7889 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1605: GFLOPs: 10111.0483. Time: 172.2183 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1606: GFLOPs: 9990.5456. Time: 174.2956 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1607: GFLOPs: 9869.2951. Time: 176.4369 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1608: GFLOPs: 10056.3982. Time: 173.1542 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1609: GFLOPs: 10072.1520. Time: 172.8834 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1610: GFLOPs: 9948.7058. Time: 175.0286 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1611: GFLOPs: 10042.4932. Time: 173.3940 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1612: GFLOPs: 10026.0082. Time: 173.6791 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1613: GFLOPs: 10021.9597. Time: 173.7492 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1614: GFLOPs: 9998.4828. Time: 174.1572 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1615: GFLOPs: 9925.8315. Time: 175.4319 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1616: GFLOPs: 10080.3935. Time: 172.7421 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1617: GFLOPs: 9940.1472. Time: 175.1793 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1618: GFLOPs: 9984.5030. Time: 174.4011 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1619: GFLOPs: 9992.2307. Time: 174.2662 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1620: GFLOPs: 10017.4326. Time: 173.8278 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1621: GFLOPs: 10042.9777. Time: 173.3856 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1622: GFLOPs: 9888.2311. Time: 176.0990 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1623: GFLOPs: 9897.5014. Time: 175.9341 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1624: GFLOPs: 10014.6037. Time: 173.8769 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1625: GFLOPs: 9951.3291. Time: 174.9824 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1626: GFLOPs: 10014.7774. Time: 173.8738 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1627: GFLOPs: 9830.0219. Time: 177.1418 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1628: GFLOPs: 9913.6783. Time: 175.6470 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1629: GFLOPs: 10014.9308. Time: 173.8712 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1630: GFLOPs: 10040.3002. Time: 173.4319 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1631: GFLOPs: 9879.6710. Time: 176.2516 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1632: GFLOPs: 9981.7872. Time: 174.4485 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1633: GFLOPs: 10004.5629. Time: 174.0514 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1634: GFLOPs: 9934.1523. Time: 175.2850 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1635: GFLOPs: 9853.3291. Time: 176.7228 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1636: GFLOPs: 9856.7263. Time: 176.6619 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1637: GFLOPs: 9948.7869. Time: 175.0272 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1638: GFLOPs: 9968.4871. Time: 174.6813 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1639: GFLOPs: 9922.8268. Time: 175.4851 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1640: GFLOPs: 9936.0797. Time: 175.2510 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1641: GFLOPs: 9759.5797. Time: 178.4204 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1642: GFLOPs: 9851.0913. Time: 176.7629 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1643: GFLOPs: 10025.9495. Time: 173.6801 us. Best GFLOPs: 10230.8661
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1644: GFLOPs: 10455.9813. Time: 166.5370 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1645: GFLOPs: 10036.7092. Time: 173.4939 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1646: GFLOPs: 9813.2366. Time: 177.4448 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1647: GFLOPs: 9886.0329. Time: 176.1382 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1648: GFLOPs: 9924.6629. Time: 175.4526 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1649: GFLOPs: 9995.2652. Time: 174.2133 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1650: GFLOPs: 9712.6200. Time: 179.2830 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1651: GFLOPs: 9992.4872. Time: 174.2617 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1652: GFLOPs: 10044.2570. Time: 173.3635 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1653: GFLOPs: 9937.4889. Time: 175.2261 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1654: GFLOPs: 9835.0855. Time: 177.0506 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1655: GFLOPs: 9972.8610. Time: 174.6047 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1656: GFLOPs: 9882.4465. Time: 176.2021 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1657: GFLOPs: 9901.1383. Time: 175.8695 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1658: GFLOPs: 9980.3374. Time: 174.4739 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1659: GFLOPs: 9986.6426. Time: 174.3637 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1660: GFLOPs: 15.4463. Time: 112732.8387 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1661: GFLOPs: 2344.2663. Time: 742.7944 us. Best GFLOPs: 10455.9813
2024-04-30 16:21:01 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1662: GFLOPs: 642.0174. Time: 2712.2440 us. Best GFLOPs: 10455.9813
2024-04-30 16:25:04 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 16:25:07 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 16:25:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:25:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:25:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1208 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:25:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1611 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:25:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2012 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:25:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2412 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:25:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:25:48 [INFO] [evolutionary_search.cc:723] Sampled 64 candidate(s)
2024-04-30 16:26:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 135 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:26:21 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:26:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 132 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:26:57 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:27:02 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9700  0.9661  0.9625  0.9625  0.9621  0.9618  0.9615  0.9614  0.9612  0.9604  0.9603  0.9598  0.9597  0.9596  0.9595  0.9594
[17 : 32]:	0.9578  0.9577  0.9570  0.9558  0.9555  0.9549  0.9547  0.9546  0.9545  0.9543  0.9537  0.9537  0.9533  0.9532  0.9531  0.9530
[33 : 48]:	0.9530  0.9523  0.9522  0.9522  0.9521  0.9520  0.9519  0.9519  0.9517  0.9513  0.9512  0.9512  0.9512  0.9510  0.9509  0.9509
[49 : 64]:	0.9506  0.9505  0.9505  0.9504  0.9502  0.9501  0.9500  0.9500  0.9496  0.9496  0.9494  0.9492  0.9490  0.9485  0.9484  0.9484
2024-04-30 16:27:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 16:27:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1663: GFLOPs: 10556.0952. Time: 164.9576 us. Best GFLOPs: 10556.0952
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1664: GFLOPs: 10644.9871. Time: 163.5801 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1665: GFLOPs: 10091.4254. Time: 172.5532 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1666: GFLOPs: 9896.2417. Time: 175.9565 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1667: GFLOPs: 10068.5003. Time: 172.9461 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1668: GFLOPs: 10120.9190. Time: 172.0504 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1669: GFLOPs: 10027.0467. Time: 173.6611 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1670: GFLOPs: 10079.2715. Time: 172.7613 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1671: GFLOPs: 10114.0969. Time: 172.1664 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1672: GFLOPs: 10024.4564. Time: 173.7060 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1673: GFLOPs: 10121.7066. Time: 172.0370 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1674: GFLOPs: 10030.9285. Time: 173.5939 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1675: GFLOPs: 10066.4367. Time: 172.9816 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1676: GFLOPs: 9998.1442. Time: 174.1631 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1677: GFLOPs: 10508.8201. Time: 165.6997 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1678: GFLOPs: 9844.3215. Time: 176.8845 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1679: GFLOPs: 9900.7325. Time: 175.8767 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1680: GFLOPs: 10103.5482. Time: 172.3462 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1681: GFLOPs: 10072.3124. Time: 172.8806 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1682: GFLOPs: 9945.3774. Time: 175.0872 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1683: GFLOPs: 10026.7218. Time: 173.6667 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1684: GFLOPs: 10014.9875. Time: 173.8702 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1685: GFLOPs: 9959.6776. Time: 174.8358 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1686: GFLOPs: 10014.7951. Time: 173.8735 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1687: GFLOPs: 9963.3306. Time: 174.7717 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1688: GFLOPs: 9905.9940. Time: 175.7833 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1689: GFLOPs: 9893.7588. Time: 176.0006 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1690: GFLOPs: 9892.7091. Time: 176.0193 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1691: GFLOPs: 9970.3032. Time: 174.6494 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1692: GFLOPs: 9870.6847. Time: 176.4121 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1693: GFLOPs: 10122.5803. Time: 172.0221 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1694: GFLOPs: 9955.1911. Time: 174.9146 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1695: GFLOPs: 9862.2217. Time: 176.5635 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1696: GFLOPs: 9958.2913. Time: 174.8601 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1697: GFLOPs: 9891.0500. Time: 176.0488 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1698: GFLOPs: 9782.7595. Time: 177.9976 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1699: GFLOPs: 9741.9143. Time: 178.7439 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1700: GFLOPs: 9965.1588. Time: 174.7396 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1701: GFLOPs: 9733.8533. Time: 178.8919 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1702: GFLOPs: 9912.2285. Time: 175.6727 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1703: GFLOPs: 10092.0133. Time: 172.5432 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1704: GFLOPs: 10099.7379. Time: 172.4112 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1705: GFLOPs: 9916.3429. Time: 175.5998 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1706: GFLOPs: 10023.0172. Time: 173.7309 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1707: GFLOPs: 9972.2043. Time: 174.6161 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1708: GFLOPs: 10008.0076. Time: 173.9915 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1709: GFLOPs: 10057.1799. Time: 173.1408 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1710: GFLOPs: 10012.7501. Time: 173.9091 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1711: GFLOPs: 10051.3371. Time: 173.2414 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1712: GFLOPs: 10064.7172. Time: 173.0111 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1713: GFLOPs: 10091.2928. Time: 172.5555 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1714: GFLOPs: 9963.6215. Time: 174.7666 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1715: GFLOPs: 9922.8268. Time: 175.4851 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1716: GFLOPs: 10050.6181. Time: 173.2538 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1717: GFLOPs: 10094.8820. Time: 172.4941 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1718: GFLOPs: 9704.2000. Time: 179.4386 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1719: GFLOPs: 10057.9814. Time: 173.1270 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1720: GFLOPs: 10161.9650. Time: 171.3554 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1721: GFLOPs: 9890.7187. Time: 176.0547 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1722: GFLOPs: 10080.8256. Time: 172.7347 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1723: GFLOPs: 9735.9153. Time: 178.8541 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1724: GFLOPs: 389.7500. Time: 4467.7565 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1725: GFLOPs: 171.4918. Time: 10153.8818 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1726: GFLOPs: 191.7096. Time: 9083.0503 us. Best GFLOPs: 10644.9871
2024-04-30 16:28:06 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 16:28:08 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 16:28:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 397 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:28:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 801 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:28:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1204 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:28:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1602 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:28:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2003 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:28:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:28:44 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2024-04-30 16:28:59 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 120 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:29:17 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:29:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 146 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:29:55 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:30:00 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0077  1.0021  1.0011  0.9942  0.9919  0.9875  0.9873  0.9830  0.9776  0.9724  0.9709  0.9707  0.9675  0.9638  0.9618  0.9593
[17 : 32]:	0.9579  0.9566  0.9555  0.9546  0.9509  0.9505  0.9504  0.9504  0.9500  0.9493  0.9463  0.9457  0.9454  0.9453  0.9452  0.9448
[33 : 48]:	0.9444  0.9443  0.9443  0.9435  0.9434  0.9430  0.9424  0.9420  0.9416  0.9416  0.9409  0.9405  0.9403  0.9395  0.9394  0.9390
[49 : 64]:	0.9385  0.9382  0.9380  0.9379  0.9377  0.9376  0.9374  0.9374  0.9371  0.9368  0.9367  0.9364  0.9364  0.9363  0.9363  0.9359
2024-04-30 16:30:01 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 16:30:01 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1727: GFLOPs: 10617.9063. Time: 163.9973 us. Best GFLOPs: 10644.9871
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1728: GFLOPs: 10662.3369. Time: 163.3139 us. Best GFLOPs: 10662.3369
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1729: GFLOPs: 10505.0579. Time: 165.7590 us. Best GFLOPs: 10662.3369
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1730: GFLOPs: 10585.7075. Time: 164.4961 us. Best GFLOPs: 10662.3369
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1731: GFLOPs: 10597.0035. Time: 164.3208 us. Best GFLOPs: 10662.3369
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1732: GFLOPs: 10647.6787. Time: 163.5387 us. Best GFLOPs: 10662.3369
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1733: GFLOPs: 10585.0827. Time: 164.5058 us. Best GFLOPs: 10662.3369
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1734: GFLOPs: 10546.0911. Time: 165.1141 us. Best GFLOPs: 10662.3369
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1735: GFLOPs: 10352.1718. Time: 168.2070 us. Best GFLOPs: 10662.3369
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1736: GFLOPs: 10812.4587. Time: 161.0464 us. Best GFLOPs: 10812.4587
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1737: GFLOPs: 5807.3209. Time: 299.8470 us. Best GFLOPs: 10812.4587
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1738: GFLOPs: 4746.4633. Time: 366.8643 us. Best GFLOPs: 10812.4587
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1739: GFLOPs: 10560.3688. Time: 164.8908 us. Best GFLOPs: 10812.4587
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1740: GFLOPs: 10362.9882. Time: 168.0314 us. Best GFLOPs: 10812.4587
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1741: GFLOPs: 10372.0956. Time: 167.8839 us. Best GFLOPs: 10812.4587
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1742: GFLOPs: 10838.7632. Time: 160.6556 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1743: GFLOPs: 10607.8952. Time: 164.1521 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1744: GFLOPs: 10828.9062. Time: 160.8018 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1745: GFLOPs: 10635.2144. Time: 163.7304 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1746: GFLOPs: 10663.7613. Time: 163.2921 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1747: GFLOPs: 10530.1428. Time: 165.3641 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1748: GFLOPs: 10349.1139. Time: 168.2567 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1749: GFLOPs: 10415.7170. Time: 167.1808 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1750: GFLOPs: 10651.4103. Time: 163.4814 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1751: GFLOPs: 10531.2519. Time: 165.3467 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1752: GFLOPs: 10608.6345. Time: 164.1406 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1753: GFLOPs: 10199.1411. Time: 170.7308 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1754: GFLOPs: 10746.0895. Time: 162.0411 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1755: GFLOPs: 9905.9288. Time: 175.7844 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1756: GFLOPs: 10039.1243. Time: 173.4522 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1757: GFLOPs: 9921.9044. Time: 175.5014 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1758: GFLOPs: 10046.2096. Time: 173.3298 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1759: GFLOPs: 10082.0891. Time: 172.7130 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1760: GFLOPs: 10110.1188. Time: 172.2342 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1761: GFLOPs: 10050.1159. Time: 173.2625 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1762: GFLOPs: 10043.8717. Time: 173.3702 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1763: GFLOPs: 10700.1048. Time: 162.7375 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1764: GFLOPs: 10008.6619. Time: 173.9801 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1765: GFLOPs: 10027.9106. Time: 173.6461 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1766: GFLOPs: 9984.6176. Time: 174.3991 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1767: GFLOPs: 10027.8147. Time: 173.6478 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1768: GFLOPs: 9999.7352. Time: 174.1354 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1769: GFLOPs: 10013.7760. Time: 173.8912 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1770: GFLOPs: 9982.5060. Time: 174.4359 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1771: GFLOPs: 10022.9845. Time: 173.7315 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1772: GFLOPs: 10747.8360. Time: 162.0147 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1773: GFLOPs: 10031.3679. Time: 173.5863 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1774: GFLOPs: 10002.6307. Time: 174.0850 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1775: GFLOPs: 10004.2663. Time: 174.0565 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1776: GFLOPs: 9932.6148. Time: 175.3121 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1777: GFLOPs: 9989.6578. Time: 174.3111 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1778: GFLOPs: 10002.1537. Time: 174.0933 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1779: GFLOPs: 10007.4684. Time: 174.0008 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1780: GFLOPs: 9978.1890. Time: 174.5114 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1781: GFLOPs: 9811.9263. Time: 177.4685 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1782: GFLOPs: 10027.2677. Time: 173.6573 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1783: GFLOPs: 9963.2379. Time: 174.7733 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1784: GFLOPs: 9967.1675. Time: 174.7044 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1785: GFLOPs: 10016.7592. Time: 173.8395 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1786: GFLOPs: 9913.9478. Time: 175.6422 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1787: GFLOPs: 9884.1147. Time: 176.1724 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1788: GFLOPs: 6283.1420. Time: 277.1397 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1789: GFLOPs: 5839.5257. Time: 298.1934 us. Best GFLOPs: 10838.7632
2024-04-30 16:31:09 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1790: GFLOPs: 2286.2414. Time: 761.6466 us. Best GFLOPs: 10838.7632
2024-04-30 16:37:49 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 16:37:51 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 16:37:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:38:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:38:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:38:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:38:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:38:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2425 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:38:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2827 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:38:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3228 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:38:38 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-04-30 16:38:52 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:39:10 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:39:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 139 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:39:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:39:52 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9955  0.9943  0.9943  0.9938  0.9935  0.9935  0.9925  0.9916  0.9914  0.9907  0.9905  0.9902  0.9902  0.9902  0.9892  0.9888
[17 : 32]:	0.9888  0.9886  0.9885  0.9867  0.9867  0.9864  0.9856  0.9853  0.9850  0.9842  0.9836  0.9836  0.9835  0.9826  0.9821  0.9821
[33 : 48]:	0.9815  0.9813  0.9813  0.9806  0.9806  0.9803  0.9803  0.9796  0.9795  0.9795  0.9784  0.9780  0.9777  0.9776  0.9773  0.9771
[49 : 64]:	0.9771  0.9769  0.9763  0.9760  0.9760  0.9754  0.9752  0.9745  0.9739  0.9734  0.9732  0.9731  0.9731  0.9727  0.9723  0.9718
2024-04-30 16:39:52 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 16:39:52 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1791: GFLOPs: 10816.7183. Time: 160.9830 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1792: GFLOPs: 10817.4672. Time: 160.9719 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1793: GFLOPs: 10796.4362. Time: 161.2854 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1794: GFLOPs: 10836.3612. Time: 160.6912 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1795: GFLOPs: 10294.4192. Time: 169.1507 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1796: GFLOPs: 10096.0610. Time: 172.4740 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1797: GFLOPs: 10333.1172. Time: 168.5172 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1798: GFLOPs: 10805.5443. Time: 161.1495 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1799: GFLOPs: 10747.0218. Time: 162.0270 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1800: GFLOPs: 10729.0877. Time: 162.2979 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1801: GFLOPs: 10616.7220. Time: 164.0156 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1802: GFLOPs: 10722.3801. Time: 162.3994 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1803: GFLOPs: 10723.6754. Time: 162.3798 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1804: GFLOPs: 10722.2218. Time: 162.4018 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1805: GFLOPs: 10734.4779. Time: 162.2164 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1806: GFLOPs: 10593.1323. Time: 164.3808 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1807: GFLOPs: 10308.6191. Time: 168.9177 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1808: GFLOPs: 10085.7812. Time: 172.6498 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1809: GFLOPs: 10749.8186. Time: 161.9849 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1810: GFLOPs: 10512.4589. Time: 165.6423 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1811: GFLOPs: 10497.4910. Time: 165.8785 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1812: GFLOPs: 10356.5441. Time: 168.1360 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1813: GFLOPs: 10170.0386. Time: 171.2194 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1814: GFLOPs: 10296.1989. Time: 169.1214 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1815: GFLOPs: 10249.1050. Time: 169.8985 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1816: GFLOPs: 10193.9327. Time: 170.8181 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1817: GFLOPs: 10219.6811. Time: 170.3877 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1818: GFLOPs: 10256.8242. Time: 169.7707 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1819: GFLOPs: 10397.0433. Time: 167.4811 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1820: GFLOPs: 10552.5935. Time: 165.0123 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1821: GFLOPs: 10512.0331. Time: 165.6490 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1822: GFLOPs: 10533.3568. Time: 165.3137 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1823: GFLOPs: 10427.4637. Time: 166.9925 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1824: GFLOPs: 10656.7741. Time: 163.3992 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1825: GFLOPs: 10546.8455. Time: 165.1022 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1826: GFLOPs: 10613.5767. Time: 164.0642 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1827: GFLOPs: 10613.3706. Time: 164.0674 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1828: GFLOPs: 10456.7348. Time: 166.5250 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1829: GFLOPs: 10642.9043. Time: 163.6121 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1830: GFLOPs: 10468.7047. Time: 166.3346 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1831: GFLOPs: 10667.2463. Time: 163.2387 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1832: GFLOPs: 10669.3078. Time: 163.2072 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1833: GFLOPs: 10297.8999. Time: 169.0935 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1834: GFLOPs: 10297.2601. Time: 169.1040 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1835: GFLOPs: 10337.2142. Time: 168.4504 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1836: GFLOPs: 10336.8142. Time: 168.4569 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1837: GFLOPs: 10469.4078. Time: 166.3234 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1838: GFLOPs: 10268.5778. Time: 169.5763 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1839: GFLOPs: 10510.9287. Time: 165.6664 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1840: GFLOPs: 10629.8293. Time: 163.8133 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1841: GFLOPs: 10286.7559. Time: 169.2767 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1842: GFLOPs: 10398.6461. Time: 167.4553 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1843: GFLOPs: 10400.3374. Time: 167.4280 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1844: GFLOPs: 10387.2366. Time: 167.6392 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1845: GFLOPs: 10418.9079. Time: 167.1296 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1846: GFLOPs: 10492.4258. Time: 165.9586 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1847: GFLOPs: 10472.3239. Time: 166.2771 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1848: GFLOPs: 10615.9431. Time: 164.0276 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1849: GFLOPs: 10658.2957. Time: 163.3758 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1850: GFLOPs: 10661.9764. Time: 163.3194 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1851: GFLOPs: 10538.3669. Time: 165.2351 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1852: GFLOPs: 5107.6866. Time: 340.9191 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1853: GFLOPs: 653.2369. Time: 2665.6606 us. Best GFLOPs: 10838.7632
2024-04-30 16:40:57 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1854: GFLOPs: 42.7049. Time: 40775.3397 us. Best GFLOPs: 10838.7632
2024-04-30 16:56:41 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 16:56:43 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 16:56:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:56:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 804 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:57:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1203 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:57:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1608 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:57:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2009 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:57:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2411 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:57:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2810 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:57:22 [INFO] [evolutionary_search.cc:723] Sampled 60 candidate(s)
2024-04-30 16:57:37 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:57:55 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:58:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 106 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:58:29 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 16:58:34 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9835  0.9831  0.9831  0.9825  0.9815  0.9806  0.9794  0.9794  0.9789  0.9788  0.9788  0.9786  0.9771  0.9768  0.9767  0.9763
[17 : 32]:	0.9762  0.9762  0.9761  0.9760  0.9760  0.9759  0.9755  0.9755  0.9746  0.9746  0.9744  0.9744  0.9742  0.9742  0.9740  0.9739
[33 : 48]:	0.9738  0.9737  0.9735  0.9735  0.9735  0.9732  0.9732  0.9731  0.9731  0.9729  0.9729  0.9729  0.9728  0.9727  0.9727  0.9725
[49 : 64]:	0.9725  0.9725  0.9722  0.9722  0.9719  0.9719  0.9717  0.9716  0.9715  0.9712  0.9712  0.9712  0.9712  0.9711  0.9709  0.9709
2024-04-30 16:58:35 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 16:58:35 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1855: GFLOPs: 10394.6097. Time: 167.5203 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1856: GFLOPs: 10362.9449. Time: 168.0321 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1857: GFLOPs: 10368.3775. Time: 167.9441 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1858: GFLOPs: 10484.6795. Time: 166.0812 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1859: GFLOPs: 10343.3923. Time: 168.3498 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1860: GFLOPs: 10483.0930. Time: 166.1063 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1861: GFLOPs: 10654.3260. Time: 163.4367 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1862: GFLOPs: 10816.4954. Time: 160.9863 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1863: GFLOPs: 10724.9220. Time: 162.3609 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1864: GFLOPs: 10579.4387. Time: 164.5936 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1865: GFLOPs: 10766.6598. Time: 161.7315 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1866: GFLOPs: 10657.5347. Time: 163.3875 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1867: GFLOPs: 10634.0683. Time: 163.7480 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1868: GFLOPs: 10639.0625. Time: 163.6712 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1869: GFLOPs: 10360.2771. Time: 168.0754 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1870: GFLOPs: 10291.6876. Time: 169.1956 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1871: GFLOPs: 10541.8542. Time: 165.1804 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1872: GFLOPs: 10622.1485. Time: 163.9318 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1873: GFLOPs: 10544.3259. Time: 165.1417 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1874: GFLOPs: 10411.4018. Time: 167.2501 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1875: GFLOPs: 10733.6357. Time: 162.2291 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1876: GFLOPs: 10361.9768. Time: 168.0478 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1877: GFLOPs: 10666.6590. Time: 163.2477 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1878: GFLOPs: 10665.0062. Time: 163.2730 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1879: GFLOPs: 10631.6241. Time: 163.7857 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1880: GFLOPs: 10630.9802. Time: 163.7956 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1881: GFLOPs: 10359.9401. Time: 168.0809 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1882: GFLOPs: 10434.9838. Time: 166.8721 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1883: GFLOPs: 10536.7690. Time: 165.2601 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1884: GFLOPs: 10395.0481. Time: 167.5132 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1885: GFLOPs: 10501.5549. Time: 165.8143 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1886: GFLOPs: 10634.4690. Time: 163.7419 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1887: GFLOPs: 10399.4021. Time: 167.4431 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1888: GFLOPs: 10746.4974. Time: 162.0349 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1889: GFLOPs: 10428.4053. Time: 166.9774 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1890: GFLOPs: 10597.7254. Time: 164.3096 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1891: GFLOPs: 10386.3934. Time: 167.6528 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1892: GFLOPs: 10676.1628. Time: 163.1024 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1893: GFLOPs: 10607.9164. Time: 164.1517 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1894: GFLOPs: 10594.7399. Time: 164.3559 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1895: GFLOPs: 10684.1349. Time: 162.9807 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1896: GFLOPs: 10529.2365. Time: 165.3784 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1897: GFLOPs: 10465.2459. Time: 166.3896 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1898: GFLOPs: 10398.3125. Time: 167.4606 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1899: GFLOPs: 10603.0609. Time: 164.2269 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1900: GFLOPs: 10301.2649. Time: 169.0383 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1901: GFLOPs: 10516.6418. Time: 165.5764 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1902: GFLOPs: 10536.7466. Time: 165.2605 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1903: GFLOPs: 10573.5082. Time: 164.6859 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1904: GFLOPs: 10612.2308. Time: 164.0850 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1905: GFLOPs: 10388.6994. Time: 167.6156 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1906: GFLOPs: 10454.6814. Time: 166.5577 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1907: GFLOPs: 10552.6647. Time: 165.0112 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1908: GFLOPs: 10689.5166. Time: 162.8987 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1909: GFLOPs: 10468.3476. Time: 166.3403 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1910: GFLOPs: 10349.9749. Time: 168.2427 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1911: GFLOPs: 10736.3881. Time: 162.1875 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1912: GFLOPs: 10436.7462. Time: 166.8439 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1913: GFLOPs: 10583.6283. Time: 164.5284 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1914: GFLOPs: 10375.7308. Time: 167.8251 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1915: GFLOPs: 10741.2609. Time: 162.1139 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1916: GFLOPs: 2542.9594. Time: 684.7565 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1917: GFLOPs: 778.7897. Time: 2235.9154 us. Best GFLOPs: 10838.7632
2024-04-30 16:59:40 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1918: GFLOPs: 52.4250. Time: 33215.2327 us. Best GFLOPs: 10838.7632
2024-04-30 17:19:28 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 17:19:31 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 17:19:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:19:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:19:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:19:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1615 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:19:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:20:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2422 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:20:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2822 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:20:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3226 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:20:16 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-04-30 17:20:31 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:20:48 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:21:05 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:21:23 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:21:28 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9831  0.9824  0.9824  0.9819  0.9818  0.9818  0.9818  0.9815  0.9814  0.9813  0.9812  0.9810  0.9805  0.9804  0.9803  0.9801
[17 : 32]:	0.9801  0.9800  0.9799  0.9797  0.9797  0.9797  0.9792  0.9791  0.9789  0.9788  0.9787  0.9786  0.9786  0.9785  0.9784  0.9784
[33 : 48]:	0.9780  0.9779  0.9779  0.9779  0.9777  0.9776  0.9775  0.9773  0.9770  0.9769  0.9767  0.9767  0.9765  0.9763  0.9762  0.9761
[49 : 64]:	0.9760  0.9758  0.9757  0.9757  0.9755  0.9754  0.9754  0.9754  0.9753  0.9752  0.9750  0.9750  0.9749  0.9748  0.9746  0.9745
2024-04-30 17:21:28 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 17:21:28 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1919: GFLOPs: 10845.8175. Time: 160.5511 us. Best GFLOPs: 10845.8175
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1920: GFLOPs: 10803.0170. Time: 161.1872 us. Best GFLOPs: 10845.8175
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1921: GFLOPs: 10541.4482. Time: 165.1868 us. Best GFLOPs: 10845.8175
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1922: GFLOPs: 10400.7884. Time: 167.4208 us. Best GFLOPs: 10845.8175
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1923: GFLOPs: 10787.6844. Time: 161.4163 us. Best GFLOPs: 10845.8175
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1924: GFLOPs: 10832.8500. Time: 160.7433 us. Best GFLOPs: 10845.8175
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1925: GFLOPs: 10911.0165. Time: 159.5917 us. Best GFLOPs: 10911.0165
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1926: GFLOPs: 10910.0802. Time: 159.6054 us. Best GFLOPs: 10911.0165
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1927: GFLOPs: 10698.2461. Time: 162.7657 us. Best GFLOPs: 10911.0165
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1928: GFLOPs: 10934.2206. Time: 159.2530 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1929: GFLOPs: 10924.7313. Time: 159.3914 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1930: GFLOPs: 10923.9573. Time: 159.4027 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1931: GFLOPs: 10914.2903. Time: 159.5439 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1932: GFLOPs: 10900.8264. Time: 159.7409 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1933: GFLOPs: 10594.3127. Time: 164.3625 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1934: GFLOPs: 10814.2521. Time: 161.0197 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1935: GFLOPs: 10600.1832. Time: 164.2715 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1936: GFLOPs: 10784.2471. Time: 161.4677 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1937: GFLOPs: 10761.2855. Time: 161.8123 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1938: GFLOPs: 10772.0149. Time: 161.6511 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1939: GFLOPs: 10734.2731. Time: 162.2195 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1940: GFLOPs: 10850.9915. Time: 160.4745 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1941: GFLOPs: 10583.6907. Time: 164.5275 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1942: GFLOPs: 10796.2264. Time: 161.2886 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1943: GFLOPs: 10624.2972. Time: 163.8986 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1944: GFLOPs: 10772.8499. Time: 161.6386 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1945: GFLOPs: 10785.4220. Time: 161.4501 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1946: GFLOPs: 10777.3386. Time: 161.5712 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1947: GFLOPs: 10922.1500. Time: 159.4290 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1948: GFLOPs: 10709.2805. Time: 162.5980 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1949: GFLOPs: 10589.2853. Time: 164.4406 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1950: GFLOPs: 10637.7310. Time: 163.6917 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1951: GFLOPs: 10634.3664. Time: 163.7435 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1952: GFLOPs: 10858.5770. Time: 160.3624 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1953: GFLOPs: 10638.9602. Time: 163.6728 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1954: GFLOPs: 10532.2310. Time: 165.3313 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1955: GFLOPs: 10450.3334. Time: 166.6270 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1956: GFLOPs: 10777.2344. Time: 161.5728 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1957: GFLOPs: 10463.3075. Time: 166.4204 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1958: GFLOPs: 10560.7543. Time: 164.8848 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1959: GFLOPs: 10617.1264. Time: 164.0093 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1960: GFLOPs: 10807.7147. Time: 161.1171 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1961: GFLOPs: 10822.0632. Time: 160.9035 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1962: GFLOPs: 10777.4661. Time: 161.5693 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1963: GFLOPs: 10587.3873. Time: 164.4700 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1964: GFLOPs: 10453.0606. Time: 166.5835 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1965: GFLOPs: 10671.9163. Time: 163.1673 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1966: GFLOPs: 10615.5893. Time: 164.0331 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1967: GFLOPs: 10643.1110. Time: 163.6089 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1968: GFLOPs: 10648.5150. Time: 163.5259 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1969: GFLOPs: 10681.9005. Time: 163.0148 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1970: GFLOPs: 10510.4940. Time: 165.6733 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1971: GFLOPs: 10744.1900. Time: 162.0697 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1972: GFLOPs: 10598.5340. Time: 164.2971 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1973: GFLOPs: 10750.7099. Time: 161.9714 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1974: GFLOPs: 10622.8589. Time: 163.9208 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1975: GFLOPs: 10743.0874. Time: 162.0864 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1976: GFLOPs: 10576.5250. Time: 164.6389 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1977: GFLOPs: 10874.2289. Time: 160.1316 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1978: GFLOPs: 10865.8408. Time: 160.2552 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1979: GFLOPs: 10562.5977. Time: 164.8560 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1980: GFLOPs: 214.6903. Time: 8110.7888 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1981: GFLOPs: 5336.0013. Time: 326.3320 us. Best GFLOPs: 10934.2206
2024-04-30 17:22:33 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1982: GFLOPs: 152.5762. Time: 11412.7078 us. Best GFLOPs: 10934.2206
2024-04-30 17:35:27 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 17:35:30 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 17:35:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:35:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 804 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:35:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1208 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:35:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1609 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:35:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2010 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:36:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2411 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:36:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2814 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:36:11 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2024-04-30 17:36:26 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:36:44 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:37:02 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 145 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:37:21 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:37:26 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9861  0.9842  0.9829  0.9813  0.9802  0.9802  0.9793  0.9793  0.9793  0.9785  0.9783  0.9777  0.9776  0.9772  0.9770  0.9769
[17 : 32]:	0.9767  0.9767  0.9766  0.9765  0.9762  0.9760  0.9760  0.9756  0.9755  0.9755  0.9753  0.9752  0.9751  0.9750  0.9750  0.9747
[33 : 48]:	0.9747  0.9746  0.9744  0.9744  0.9743  0.9742  0.9740  0.9738  0.9738  0.9737  0.9736  0.9735  0.9735  0.9734  0.9730  0.9730
[49 : 64]:	0.9730  0.9729  0.9729  0.9727  0.9727  0.9727  0.9727  0.9727  0.9726  0.9725  0.9725  0.9724  0.9724  0.9723  0.9722  0.9720
2024-04-30 17:37:26 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 17:37:26 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1983: GFLOPs: 10798.7046. Time: 161.2516 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1984: GFLOPs: 10820.6606. Time: 160.9244 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1985: GFLOPs: 10827.4684. Time: 160.8232 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1986: GFLOPs: 10807.6156. Time: 161.1186 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1987: GFLOPs: 10795.2022. Time: 161.3039 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1988: GFLOPs: 10830.8750. Time: 160.7726 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1989: GFLOPs: 10675.2211. Time: 163.1168 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1990: GFLOPs: 10666.0779. Time: 163.2566 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1991: GFLOPs: 10795.2562. Time: 161.3031 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1992: GFLOPs: 10619.4834. Time: 163.9729 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1993: GFLOPs: 10502.7087. Time: 165.7961 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1994: GFLOPs: 10755.6429. Time: 161.8971 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1995: GFLOPs: 10598.2922. Time: 164.3008 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1996: GFLOPs: 10570.9490. Time: 164.7258 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1997: GFLOPs: 10614.9131. Time: 164.0435 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1998: GFLOPs: 10651.8550. Time: 163.4746 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #1999: GFLOPs: 10612.2549. Time: 164.0846 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2000: GFLOPs: 10603.6627. Time: 164.2176 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2001: GFLOPs: 10564.7035. Time: 164.8232 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2002: GFLOPs: 10389.7956. Time: 167.5979 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2003: GFLOPs: 10728.2765. Time: 162.3101 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2004: GFLOPs: 10802.7240. Time: 161.1916 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2005: GFLOPs: 10673.9518. Time: 163.1362 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2006: GFLOPs: 10470.8509. Time: 166.3005 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2007: GFLOPs: 10596.2210. Time: 164.3329 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2008: GFLOPs: 10640.9984. Time: 163.6414 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2009: GFLOPs: 10659.4337. Time: 163.3584 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2010: GFLOPs: 10579.1112. Time: 164.5987 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2011: GFLOPs: 10602.8940. Time: 164.2295 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2012: GFLOPs: 10597.9552. Time: 164.3060 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2013: GFLOPs: 10646.0973. Time: 163.5630 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2014: GFLOPs: 10405.9243. Time: 167.3381 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2015: GFLOPs: 10728.8331. Time: 162.3017 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2016: GFLOPs: 10582.8388. Time: 164.5407 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2017: GFLOPs: 10258.6378. Time: 169.7407 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2018: GFLOPs: 10599.8792. Time: 164.2762 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2019: GFLOPs: 8670.8602. Time: 200.8230 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2020: GFLOPs: 10575.8519. Time: 164.6494 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2021: GFLOPs: 10593.4385. Time: 164.3761 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2022: GFLOPs: 10639.5547. Time: 163.6636 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2023: GFLOPs: 10564.9576. Time: 164.8192 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2024: GFLOPs: 10687.7988. Time: 162.9248 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2025: GFLOPs: 10399.3795. Time: 167.4434 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2026: GFLOPs: 10423.8437. Time: 167.0505 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2027: GFLOPs: 10601.5957. Time: 164.2496 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2028: GFLOPs: 10598.1471. Time: 164.3031 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2029: GFLOPs: 9265.8912. Time: 187.9267 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2030: GFLOPs: 10461.5732. Time: 166.4480 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2031: GFLOPs: 10762.2238. Time: 161.7982 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2032: GFLOPs: 10746.4433. Time: 162.0357 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2033: GFLOPs: 10471.6559. Time: 166.2877 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2034: GFLOPs: 10472.6469. Time: 166.2720 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2035: GFLOPs: 10591.9633. Time: 164.3990 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2036: GFLOPs: 9340.9495. Time: 186.4166 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2037: GFLOPs: 10629.8453. Time: 163.8131 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2038: GFLOPs: 10635.1890. Time: 163.7308 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2039: GFLOPs: 10620.8932. Time: 163.9512 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2040: GFLOPs: 10592.8911. Time: 164.3846 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2041: GFLOPs: 10555.6317. Time: 164.9648 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2042: GFLOPs: 10606.6707. Time: 164.1710 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2043: GFLOPs: 10637.9765. Time: 163.6879 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2044: GFLOPs: 1509.9331. Time: 1153.2351 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2045: GFLOPs: 5169.7066. Time: 336.8292 us. Best GFLOPs: 10934.2206
2024-04-30 17:38:34 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2046: GFLOPs: 10.8181. Time: 160961.8733 us. Best GFLOPs: 10934.2206
2024-04-30 17:48:45 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 17:48:48 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 17:48:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:48:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 803 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:49:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:49:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1613 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:49:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2014 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:49:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2419 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:49:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2822 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:49:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3226 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:49:32 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-04-30 17:49:46 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 115 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:50:04 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:50:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:50:39 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 17:50:44 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2188  1.1933  1.1819  1.1698  1.1672  1.1630  1.1559  1.0583  1.0467  0.9907  0.9877  0.9855  0.9853  0.9852  0.9845  0.9840
[17 : 32]:	0.9836  0.9832  0.9828  0.9823  0.9821  0.9814  0.9813  0.9813  0.9799  0.9798  0.9794  0.9790  0.9788  0.9788  0.9780  0.9779
[33 : 48]:	0.9776  0.9776  0.9775  0.9771  0.9766  0.9751  0.9750  0.9749  0.9748  0.9747  0.9744  0.9743  0.9742  0.9740  0.9740  0.9736
[49 : 64]:	0.9735  0.9734  0.9732  0.9731  0.9730  0.9724  0.9724  0.9724  0.9724  0.9724  0.9722  0.9720  0.9719  0.9719  0.9717  0.9716
2024-04-30 17:50:44 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 17:50:44 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2047: GFLOPs: 93.9202. Time: 18540.2889 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2048: GFLOPs: 61.3499. Time: 28383.2320 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2049: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(7), T.int64(1), T.int64(1), T.int64(64), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + co_3_init * T.int64(64) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(392))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                        v3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + co_3 * T.int64(64) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(128), T.int64(7)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 64])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 14, 7, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2050: GFLOPs: 122.6976. Time: 14191.8716 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2051: GFLOPs: 34.6310. Time: 50281.8143 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2052: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(32), T.int64(7), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + co_3_init * T.int64(4) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(98))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(98))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(392))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                        v3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + co_3 * T.int64(4) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(128), T.int64(7)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 1, 4, 32, 4])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 14, 7, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2053: GFLOPs: 94.0791. Time: 18508.9708 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2054: GFLOPs: 122.5461. Time: 14209.4078 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:121] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2055: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(32), T.int64(7), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + co_3_init * T.int64(4) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(98))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(98))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(392))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                        v3 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + co_3 * T.int64(4) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(128), T.int64(7)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(56) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_2_nu_2_co_2_p_2_fused % T.int64(56) // T.int64(14) * T.int64(128) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(98) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(7) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 1, 4, 32, 4])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 14, 7, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2056: GFLOPs: 10864.5385. Time: 160.2744 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2057: GFLOPs: 10818.1933. Time: 160.9611 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2058: GFLOPs: 10847.5908. Time: 160.5249 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2059: GFLOPs: 10819.6467. Time: 160.9394 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2060: GFLOPs: 10821.8698. Time: 160.9064 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2061: GFLOPs: 10742.9423. Time: 162.0885 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2062: GFLOPs: 10770.1493. Time: 161.6791 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2063: GFLOPs: 10748.4135. Time: 162.0060 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2064: GFLOPs: 10801.9958. Time: 161.2024 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2065: GFLOPs: 10554.0706. Time: 164.9892 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2066: GFLOPs: 10763.4560. Time: 161.7796 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2067: GFLOPs: 10743.5280. Time: 162.0797 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2068: GFLOPs: 10608.1892. Time: 164.1475 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2069: GFLOPs: 10627.9997. Time: 163.8415 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2070: GFLOPs: 10801.0144. Time: 161.2171 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2071: GFLOPs: 10795.8177. Time: 161.2947 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2072: GFLOPs: 10604.1643. Time: 164.2098 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2073: GFLOPs: 10754.5832. Time: 161.9131 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2074: GFLOPs: 10601.9570. Time: 164.2440 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2075: GFLOPs: 10623.9691. Time: 163.9037 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2076: GFLOPs: 10579.7100. Time: 164.5894 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2077: GFLOPs: 10747.2246. Time: 162.0240 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2078: GFLOPs: 10721.8144. Time: 162.4079 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2079: GFLOPs: 10490.8365. Time: 165.9837 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2080: GFLOPs: 10655.8662. Time: 163.4131 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2081: GFLOPs: 10545.7115. Time: 165.1200 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2082: GFLOPs: 10588.3124. Time: 164.4557 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2083: GFLOPs: 10745.5291. Time: 162.0495 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2084: GFLOPs: 10516.9880. Time: 165.5710 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2085: GFLOPs: 10718.9557. Time: 162.4513 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2086: GFLOPs: 10679.9488. Time: 163.0446 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2087: GFLOPs: 10618.3466. Time: 163.9905 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2088: GFLOPs: 10524.1090. Time: 165.4589 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2089: GFLOPs: 10621.9718. Time: 163.9345 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2090: GFLOPs: 10626.9911. Time: 163.8571 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2091: GFLOPs: 10743.5405. Time: 162.0795 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2092: GFLOPs: 10655.9415. Time: 163.4119 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2093: GFLOPs: 10746.7011. Time: 162.0319 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2094: GFLOPs: 10482.3304. Time: 166.1184 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2095: GFLOPs: 10634.5553. Time: 163.7405 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2096: GFLOPs: 10552.8942. Time: 165.0076 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2097: GFLOPs: 10575.9054. Time: 164.6486 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2098: GFLOPs: 10671.3159. Time: 163.1765 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2099: GFLOPs: 10587.2479. Time: 164.4722 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2100: GFLOPs: 10550.6162. Time: 165.0432 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2101: GFLOPs: 10549.3217. Time: 165.0635 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2102: GFLOPs: 10597.0035. Time: 164.3208 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2103: GFLOPs: 10593.5386. Time: 164.3745 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2104: GFLOPs: 10593.5906. Time: 164.3737 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2105: GFLOPs: 10734.6039. Time: 162.2145 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2106: GFLOPs: 10649.7372. Time: 163.5071 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2107: GFLOPs: 10735.0881. Time: 162.2071 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2108: GFLOPs: 127.6158. Time: 13644.9279 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2109: GFLOPs: 3009.9710. Time: 578.5132 us. Best GFLOPs: 10934.2206
2024-04-30 17:52:07 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2110: GFLOPs: 1590.5736. Time: 1094.7673 us. Best GFLOPs: 10934.2206
2024-04-30 18:03:37 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 18:03:40 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 18:03:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:03:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:03:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:04:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1614 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:04:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2018 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:04:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2422 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:04:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2821 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:04:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3221 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:04:26 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2024-04-30 18:04:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 116 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:05:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:05:18 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:05:35 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 109 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:05:40 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9850  0.9850  0.9838  0.9838  0.9837  0.9836  0.9819  0.9817  0.9812  0.9796  0.9790  0.9784  0.9782  0.9777  0.9776  0.9775
[17 : 32]:	0.9772  0.9772  0.9771  0.9771  0.9768  0.9766  0.9762  0.9761  0.9757  0.9757  0.9756  0.9756  0.9755  0.9740  0.9738  0.9738
[33 : 48]:	0.9738  0.9733  0.9733  0.9732  0.9731  0.9731  0.9730  0.9729  0.9728  0.9726  0.9726  0.9726  0.9726  0.9725  0.9724  0.9721
[49 : 64]:	0.9720  0.9720  0.9720  0.9718  0.9718  0.9717  0.9717  0.9716  0.9715  0.9715  0.9713  0.9713  0.9713  0.9713  0.9708  0.9708
2024-04-30 18:05:40 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 18:05:41 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2111: GFLOPs: 10818.4836. Time: 160.9567 us. Best GFLOPs: 10934.2206
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2112: GFLOPs: 10820.4060. Time: 160.9281 us. Best GFLOPs: 10934.2206
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2113: GFLOPs: 10796.6452. Time: 161.2823 us. Best GFLOPs: 10934.2206
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2114: GFLOPs: 10816.3033. Time: 160.9892 us. Best GFLOPs: 10934.2206
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2115: GFLOPs: 10939.0081. Time: 159.1833 us. Best GFLOPs: 10939.0081
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2116: GFLOPs: 10859.9452. Time: 160.3422 us. Best GFLOPs: 10939.0081
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2117: GFLOPs: 10639.5612. Time: 163.6635 us. Best GFLOPs: 10939.0081
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2118: GFLOPs: 10931.6307. Time: 159.2908 us. Best GFLOPs: 10939.0081
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2119: GFLOPs: 10672.9147. Time: 163.1520 us. Best GFLOPs: 10939.0081
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2120: GFLOPs: 10910.6374. Time: 159.5973 us. Best GFLOPs: 10939.0081
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2121: GFLOPs: 10979.9826. Time: 158.5893 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2122: GFLOPs: 10857.9055. Time: 160.3724 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2123: GFLOPs: 10864.8426. Time: 160.2700 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2124: GFLOPs: 10828.8818. Time: 160.8022 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2125: GFLOPs: 10865.8937. Time: 160.2545 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2126: GFLOPs: 10800.9563. Time: 161.2179 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2127: GFLOPs: 10844.2861. Time: 160.5738 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2128: GFLOPs: 10728.4769. Time: 162.3071 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2129: GFLOPs: 10832.9623. Time: 160.7416 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2130: GFLOPs: 10848.9595. Time: 160.5046 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2131: GFLOPs: 10840.8350. Time: 160.6249 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2132: GFLOPs: 10778.6234. Time: 161.5520 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2133: GFLOPs: 10850.7966. Time: 160.4774 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2134: GFLOPs: 10831.2891. Time: 160.7665 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2135: GFLOPs: 10845.9658. Time: 160.5489 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2136: GFLOPs: 10806.4311. Time: 161.1363 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2137: GFLOPs: 10799.2300. Time: 161.2437 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2138: GFLOPs: 10750.5049. Time: 161.9745 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2139: GFLOPs: 10784.8012. Time: 161.4594 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2140: GFLOPs: 10869.2198. Time: 160.2054 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2141: GFLOPs: 10730.5877. Time: 162.2752 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2142: GFLOPs: 10855.9318. Time: 160.4015 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2143: GFLOPs: 10799.8786. Time: 161.2340 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2144: GFLOPs: 10633.7908. Time: 163.7523 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2145: GFLOPs: 10871.2733. Time: 160.1752 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2146: GFLOPs: 10862.6251. Time: 160.3027 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2147: GFLOPs: 10866.8283. Time: 160.2407 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2148: GFLOPs: 10849.5715. Time: 160.4955 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2149: GFLOPs: 10868.5961. Time: 160.2146 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2150: GFLOPs: 10854.9940. Time: 160.4154 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2151: GFLOPs: 10886.1812. Time: 159.9558 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2152: GFLOPs: 10876.4008. Time: 160.0996 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2153: GFLOPs: 10866.8857. Time: 160.2398 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2154: GFLOPs: 10866.5655. Time: 160.2445 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2155: GFLOPs: 10853.3980. Time: 160.4390 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2156: GFLOPs: 10882.0067. Time: 160.0172 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2157: GFLOPs: 10841.8327. Time: 160.6101 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2158: GFLOPs: 10861.3218. Time: 160.3219 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2159: GFLOPs: 10839.2760. Time: 160.6480 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2160: GFLOPs: 10841.3919. Time: 160.6166 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2161: GFLOPs: 10730.3327. Time: 162.2790 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2162: GFLOPs: 10834.8005. Time: 160.7143 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2163: GFLOPs: 10872.6536. Time: 160.1548 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2164: GFLOPs: 10867.0867. Time: 160.2369 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2165: GFLOPs: 10872.0714. Time: 160.1634 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2166: GFLOPs: 10751.3265. Time: 161.9621 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2167: GFLOPs: 10778.9603. Time: 161.5469 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2168: GFLOPs: 10627.4829. Time: 163.8495 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2169: GFLOPs: 10751.1888. Time: 161.9642 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2170: GFLOPs: 10584.4783. Time: 164.5152 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2171: GFLOPs: 10732.5345. Time: 162.2457 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2172: GFLOPs: 3035.8861. Time: 573.5748 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2173: GFLOPs: 249.9970. Time: 6965.3162 us. Best GFLOPs: 10979.9826
2024-04-30 18:06:49 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2174: GFLOPs: 8126.6237. Time: 214.2720 us. Best GFLOPs: 10979.9826
2024-04-30 18:19:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 18:20:02 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 18:20:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:20:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 808 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:20:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:20:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 1611 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:20:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2016 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:20:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2416 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:20:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 2821 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:20:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 3222 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:20:48 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2024-04-30 18:21:02 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 105 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:21:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:21:38 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:21:55 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xefc9588)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xe6f8ad8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xe6f8f28)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0xb29deb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44c1498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xe6d9098)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xe6f8ef8)]: 0 failure(s)
2024-04-30 18:22:00 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0855  1.0852  1.0752  0.9863  0.9863  0.9853  0.9846  0.9845  0.9833  0.9830  0.9823  0.9822  0.9821  0.9821  0.9820  0.9819
[17 : 32]:	0.9814  0.9811  0.9810  0.9810  0.9809  0.9809  0.9808  0.9806  0.9804  0.9797  0.9797  0.9794  0.9793  0.9793  0.9789  0.9774
[33 : 48]:	0.9773  0.9773  0.9773  0.9771  0.9768  0.9768  0.9765  0.9764  0.9764  0.9763  0.9763  0.9762  0.9762  0.9761  0.9761  0.9757
[49 : 64]:	0.9756  0.9756  0.9756  0.9754  0.9754  0.9753  0.9752  0.9752  0.9752  0.9751  0.9749  0.9749  0.9748  0.9748  0.9748  0.9748
2024-04-30 18:22:00 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 18:22:00 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2175: GFLOPs: 286.1343. Time: 6085.6317 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2176: GFLOPs: 279.6977. Time: 6225.6788 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2177: GFLOPs: 286.6990. Time: 6073.6452 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2178: GFLOPs: 3890.3414. Time: 447.5977 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2179: GFLOPs: 3467.7105. Time: 502.1492 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2180: GFLOPs: 10674.3583. Time: 163.1300 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2181: GFLOPs: 10653.4555. Time: 163.4501 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2182: GFLOPs: 10673.6735. Time: 163.1405 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2183: GFLOPs: 10837.2054. Time: 160.6787 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2184: GFLOPs: 10560.9668. Time: 164.8815 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2185: GFLOPs: 10952.9023. Time: 158.9814 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2186: GFLOPs: 10931.4903. Time: 159.2928 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2187: GFLOPs: 10962.5147. Time: 158.8420 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2188: GFLOPs: 10897.2591. Time: 159.7932 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2189: GFLOPs: 10906.9716. Time: 159.6509 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2190: GFLOPs: 1938.5104. Time: 898.2711 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2191: GFLOPs: 10914.2923. Time: 159.5438 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2192: GFLOPs: 10889.4889. Time: 159.9072 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2193: GFLOPs: 10901.1483. Time: 159.7362 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2194: GFLOPs: 10946.4471. Time: 159.0752 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2195: GFLOPs: 10904.3051. Time: 159.6899 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2196: GFLOPs: 10902.8390. Time: 159.7114 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2197: GFLOPs: 10882.2030. Time: 160.0143 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2198: GFLOPs: 10924.1698. Time: 159.3996 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2199: GFLOPs: 10902.8526. Time: 159.7112 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2200: GFLOPs: 10917.2125. Time: 159.5011 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2201: GFLOPs: 10918.3500. Time: 159.4845 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2202: GFLOPs: 2881.2201. Time: 604.3648 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2203: GFLOPs: 10911.2868. Time: 159.5878 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2204: GFLOPs: 10578.5131. Time: 164.6080 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2205: GFLOPs: 10881.6494. Time: 160.0224 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2206: GFLOPs: 10710.0391. Time: 162.5865 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2207: GFLOPs: 10862.8707. Time: 160.2991 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2208: GFLOPs: 10867.3984. Time: 160.2323 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2209: GFLOPs: 10876.2187. Time: 160.1023 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2210: GFLOPs: 10875.3596. Time: 160.1150 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2211: GFLOPs: 10863.6492. Time: 160.2876 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2212: GFLOPs: 10854.2644. Time: 160.4262 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2213: GFLOPs: 10868.3069. Time: 160.2189 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2214: GFLOPs: 10839.0752. Time: 160.6510 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2215: GFLOPs: 10866.8755. Time: 160.2400 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2216: GFLOPs: 10868.5003. Time: 160.2160 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2217: GFLOPs: 10787.4129. Time: 161.4203 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2218: GFLOPs: 2879.2275. Time: 604.7830 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2219: GFLOPs: 10912.8547. Time: 159.5648 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2220: GFLOPs: 10886.5124. Time: 159.9509 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2221: GFLOPs: 10852.5012. Time: 160.4522 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2222: GFLOPs: 10721.2228. Time: 162.4169 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2223: GFLOPs: 10776.4804. Time: 161.5841 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2224: GFLOPs: 10825.7523. Time: 160.8487 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2225: GFLOPs: 10875.1516. Time: 160.1180 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2226: GFLOPs: 10880.4642. Time: 160.0399 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2227: GFLOPs: 10727.0850. Time: 162.3282 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2228: GFLOPs: 10853.0756. Time: 160.4437 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2229: GFLOPs: 10767.5894. Time: 161.7175 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2230: GFLOPs: 10757.9836. Time: 161.8619 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2231: GFLOPs: 10894.2386. Time: 159.8375 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2232: GFLOPs: 10854.8569. Time: 160.4174 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2233: GFLOPs: 10857.5449. Time: 160.3777 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2234: GFLOPs: 10860.5946. Time: 160.3326 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2235: GFLOPs: 10865.7366. Time: 160.2568 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2236: GFLOPs: 198.7741. Time: 8760.2347 us. Best GFLOPs: 10979.9826
2024-04-30 18:23:06 [INFO] [task_scheduler.cc:131] [Task #10: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_7] Trial #2237: GFLOPs: 1400.2254. Time: 1243.5911 us. Best GFLOPs: 10979.9826
