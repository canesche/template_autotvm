2024-04-29 01:54:22 [INFO] [task_scheduler.cc:160] Initializing Task #32: "fused_nn_avg_pool2d_3"
2024-04-29 01:54:22 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0, ax1, ax2, ax3, rv0, rv1 in T.grid(T.int64(1), T.int64(288), T.int64(25), T.int64(25), T.int64(3), T.int64(3)):
            with T.block("pool_sum"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_rv0, v_rv1 = T.axis.remap("SSSSRR", [ax0, ax1, ax2, ax3, rv0, rv1])
                T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                with T.init():
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(288), T.int64(25), T.int64(25)):
            with T.block("pool_avg"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                T.block_attr({"schedule_rule": "meta_schedule.pool_avg"})
                pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
2024-04-29 01:54:22 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2024-04-29 01:54:22 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
            for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                        with T.block("pool_sum"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                            v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                            v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                            v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                            T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                            T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_avg"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
2024-04-29 01:54:22 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
            for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x"):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                    for ax4_ax5_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                        with T.block("pool_sum"):
                            v_ax0 = T.axis.spatial(T.int64(1), ax0)
                            v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                            v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                            v_ax3 = T.axis.spatial(T.int64(25), ax3)
                            v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) // T.int64(3))
                            v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) % T.int64(3))
                            T.where(ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1 < T.int64(9))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                            T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_avg"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                        T.where(T.Mul(T.int64(0), T.int64(128)) + ax3_1 < T.int64(25))
                        T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
2024-04-29 02:52:40 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 02:52:40 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-29 02:52:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5267318)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7c1a918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb243248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x63f46f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4001568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xb23ae18)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xb23a348)]: 0 failure(s)
2024-04-29 02:52:41 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-04-29 02:52:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5267318)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7c1a918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb243248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x63f46f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4001568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xb23ae18)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xb23a348)]: 0 failure(s)
2024-04-29 02:52:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5267318)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7c1a918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb243248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x63f46f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4001568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xb23ae18)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xb23a348)]: 0 failure(s)
2024-04-29 02:52:43 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5267318)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7c1a918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb243248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x63f46f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4001568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xb23ae18)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xb23a348)]: 0 failure(s)
2024-04-29 02:52:43 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5267318)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7c1a918)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb243248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x63f46f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x4001568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xb23ae18)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0xb23a348)]: 0 failure(s)
2024-04-29 02:52:43 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9997  0.9974  0.9941  0.9893  0.9883  0.9796  0.9745  0.9685  0.9661  0.9645  0.9599  0.9588  0.9579  0.9516  0.9464  0.9446
[17 : 32]:	0.9437  0.9426  0.9395  0.9392  0.9313  0.9265  0.9186  0.9157  0.9119  0.9062  0.9000  0.8928  0.8927  0.8791  0.8782  0.8772
[33 : 48]:	0.8647  0.8636  0.8618  0.8554  0.8512  0.8495  0.8403  0.8393  0.8354  0.8348  0.8346  0.8322  0.8298  0.8251  0.8245  0.8214
[49 : 64]:	0.8189  0.8144  0.8067  0.8038  0.8032  0.7956  0.7922  0.7918  0.7828  0.7810  0.7715  0.7685  0.7390  0.7309  0.7239  0.7149
2024-04-29 02:52:43 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 02:52:43 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #1: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #2: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(512)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #3: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #4: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #5: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #6: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14400), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #7: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(128)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #8: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(128)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #9: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #10: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(256)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #11: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #12: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14400), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #13: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #14: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #15: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #16: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #17: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #18: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #19: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #20: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(512)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #21: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #22: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(50400), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(3)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(175) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(175) // T.int64(7) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(175))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(175) // T.int64(7))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #23: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #24: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #25: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #26: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14400), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #27: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #28: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #29: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(512)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #30: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #31: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #32: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #33: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #34: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #35: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(50400), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(3)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(175) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(175) // T.int64(7) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(175))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(175) // T.int64(7))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #36: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #37: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #38: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #39: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #40: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #41: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #42: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #43: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #44: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(256)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #45: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #46: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #47: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(32)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #48: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #49: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x"):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(64)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #50: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #51: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #52: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #53: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(28800), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(100) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(100) // T.int64(4) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(100))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(100) // T.int64(4))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #54: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #55: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #56: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #57: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #58: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(28800), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(100) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(100) // T.int64(4) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(100))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(100) // T.int64(4))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #59: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(2813), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(352), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #60: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #61: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #62: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(176), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #63: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(64)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-04-29 04:01:53 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #64: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("pool_sum_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                    T.reads()
                    T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_sum_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(5625), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                    v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                    T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24 = sch.get_child_blocks(b22)
l25, l26, l27, l28 = sch.get_loops(block=b23)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l29, l30 = sch.get_loops(block=b24)
b31 = sch.get_block(name="pool_sum", func_name="main")
l32, l33, l34, l35 = sch.get_loops(block=b31)
b36 = sch.decompose_reduction(block=b31, loop=l34)
2024-05-01 15:09:59 [INFO] [task_scheduler.cc:160] Initializing Task #32: "fused_nn_avg_pool2d_3"
2024-05-01 15:09:59 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
        for ax0, ax1, ax2, ax3, rv0, rv1 in T.grid(T.int64(1), T.int64(288), T.int64(25), T.int64(25), T.int64(3), T.int64(3)):
            with T.block("pool_sum"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_rv0, v_rv1 = T.axis.remap("SSSSRR", [ax0, ax1, ax2, ax3, rv0, rv1])
                T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                with T.init():
                    pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(288), T.int64(25), T.int64(25)):
            with T.block("pool_avg"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                T.block_attr({"schedule_rule": "meta_schedule.pool_avg"})
                pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
2024-05-01 15:09:59 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2024-05-01 15:09:59 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            pool_sum = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)))
            for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(704), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                        with T.block("pool_sum"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                            v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                            v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                            v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                            T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                            T.writes(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                            pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1407), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_avg"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(625))
                        v_ax2 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(625) // T.int64(25))
                        v_ax3 = T.axis.spatial(T.int64(25), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(25))
                        T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(180000))
                        T.reads(pool_sum[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4, l5, l6, l7 = sch.get_loops(block=b1)
l8 = sch.fuse(l4, l5, l6, l7, preserve_unit_iters=True)
v9 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l10, l11 = sch.split(loop=l8, factors=[None, v9], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="blockIdx.x")
sch.bind(loop=l11, thread_axis="threadIdx.x")
l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b0)
l18 = sch.fuse(l12, l13, l14, l15, preserve_unit_iters=True)
v19 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l20, l21 = sch.split(loop=l18, factors=[None, v19], preserve_unit_iters=True)
sch.bind(loop=l20, thread_axis="blockIdx.x")
sch.bind(loop=l21, thread_axis="threadIdx.x")
2024-05-01 15:09:59 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 1024})
            pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
            for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14400), thread="blockIdx.x"):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                    for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                        with T.block("pool_sum"):
                            v_ax0 = T.axis.spatial(T.int64(1), ax0)
                            v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50) + ax1)
                            v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2) + ax2)
                            v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3)
                            v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                            v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                            T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                            T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
                for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_avg"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50))
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2))
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1)
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1 < T.int64(25))
                        T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
2024-05-01 15:52:49 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-05-01 15:52:49 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-05-01 15:52:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb481a68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xb0d5a78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb48c1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x6316d38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x763e088)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x870c548)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x3595f08)]: 0 failure(s)
2024-05-01 15:52:50 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-05-01 15:52:50 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb481a68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xb0d5a78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb48c1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x6316d38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x763e088)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x870c548)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x3595f08)]: 0 failure(s)
2024-05-01 15:52:51 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb481a68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xb0d5a78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb48c1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x6316d38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x763e088)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x870c548)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x3595f08)]: 0 failure(s)
2024-05-01 15:52:51 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb481a68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xb0d5a78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb48c1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x6316d38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x763e088)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x870c548)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x3595f08)]: 0 failure(s)
2024-05-01 15:52:51 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xb481a68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0xb0d5a78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0xb48c1d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x6316d38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x763e088)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x870c548)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x3595f08)]: 0 failure(s)
2024-05-01 15:52:52 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9941  0.9891  0.9889  0.9880  0.9880  0.9803  0.9718  0.9647  0.9557  0.9528  0.9526  0.9458  0.9430  0.9394  0.9340  0.9275
[17 : 32]:	0.9274  0.9267  0.9213  0.9133  0.9061  0.9006  0.8904  0.8888  0.8872  0.8804  0.8779  0.8767  0.8714  0.8702  0.8699  0.8661
[33 : 48]:	0.8564  0.8518  0.8468  0.8420  0.8380  0.8335  0.8311  0.8199  0.8192  0.8175  0.8150  0.8102  0.8096  0.8089  0.8070  0.8060
[49 : 64]:	0.8028  0.8011  0.7943  0.7864  0.7861  0.7822  0.7725  0.7666  0.7662  0.7656  0.7646  0.7517  0.7488  0.7479  0.7466  0.7442
2024-05-01 15:52:52 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-05-01 15:52:52 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #1: GFLOPs: 505.7829. Time: 8.5412 us. Best GFLOPs: 505.7829
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #2: GFLOPs: 409.7094. Time: 10.5441 us. Best GFLOPs: 505.7829
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #3: GFLOPs: 532.6512. Time: 8.1104 us. Best GFLOPs: 532.6512
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #4: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(128)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #5: GFLOPs: 432.1277. Time: 9.9970 us. Best GFLOPs: 532.6512
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #6: GFLOPs: 522.8695. Time: 8.2621 us. Best GFLOPs: 532.6512
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #7: GFLOPs: 556.7694. Time: 7.7590 us. Best GFLOPs: 556.7694
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #8: GFLOPs: 373.9142. Time: 11.5535 us. Best GFLOPs: 556.7694
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #9: GFLOPs: 439.9908. Time: 9.8184 us. Best GFLOPs: 556.7694
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #10: GFLOPs: 432.1373. Time: 9.9968 us. Best GFLOPs: 556.7694
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #11: GFLOPs: 630.5929. Time: 6.8507 us. Best GFLOPs: 630.5929
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #12: GFLOPs: 415.2946. Time: 10.4023 us. Best GFLOPs: 630.5929
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #13: GFLOPs: 55.7360. Time: 77.5082 us. Best GFLOPs: 630.5929
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #14: GFLOPs: 409.4014. Time: 10.5520 us. Best GFLOPs: 630.5929
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #15: GFLOPs: 371.8901. Time: 11.6163 us. Best GFLOPs: 630.5929
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #16: GFLOPs: 572.4545. Time: 7.5465 us. Best GFLOPs: 630.5929
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #17: GFLOPs: 643.6409. Time: 6.7118 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #18: GFLOPs: 413.9408. Time: 10.4363 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #19: GFLOPs: 96.5033. Time: 44.7653 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #20: GFLOPs: 556.6746. Time: 7.7604 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #21: GFLOPs: 556.4824. Time: 7.7630 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #22: GFLOPs: 610.4689. Time: 7.0765 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #23: GFLOPs: 518.1854. Time: 8.3368 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #24: GFLOPs: 522.4810. Time: 8.2682 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #25: GFLOPs: 458.0169. Time: 9.4320 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #26: GFLOPs: 510.1454. Time: 8.4682 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #27: GFLOPs: 373.8716. Time: 11.5548 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #28: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(64)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #29: GFLOPs: 643.3709. Time: 6.7146 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #30: GFLOPs: 626.8084. Time: 6.8921 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #31: GFLOPs: 626.7935. Time: 6.8922 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #32: GFLOPs: 601.5702. Time: 7.1812 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #33: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14400), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #34: GFLOPs: 518.1245. Time: 8.3378 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #35: GFLOPs: 462.3624. Time: 9.3433 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #36: GFLOPs: 476.9566. Time: 9.0574 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #37: GFLOPs: 414.9725. Time: 10.4103 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #38: GFLOPs: 409.3798. Time: 10.5525 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #39: GFLOPs: 516.0902. Time: 8.3706 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #40: GFLOPs: 29.3047. Time: 147.4168 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #41: GFLOPs: 597.1958. Time: 7.2338 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #42: GFLOPs: 432.1397. Time: 9.9968 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #43: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14400), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(50))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(50) // T.int64(2))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(16) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #44: GFLOPs: 524.2209. Time: 8.2408 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #45: GFLOPs: 637.5397. Time: 6.7760 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #46: GFLOPs: 459.1913. Time: 9.4078 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #47: GFLOPs: 586.7817. Time: 7.3622 us. Best GFLOPs: 643.6409
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #48: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(28800), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(100) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(100) // T.int64(4) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(100))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(100) // T.int64(4))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #49: GFLOPs: 649.5741. Time: 6.6505 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #50: GFLOPs: 615.7494. Time: 7.0158 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #51: GFLOPs: 627.5369. Time: 6.8841 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #52: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(28800), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(100) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(100) // T.int64(4) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(100))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(100) // T.int64(4))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(8) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #53: GFLOPs: 518.7970. Time: 8.3270 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #54: GFLOPs: 458.4104. Time: 9.4239 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #55: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7200), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(25), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(25))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(25))
                    v_ax3 = T.axis.spatial(T.int64(25), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(32)) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #56: GFLOPs: 373.9371. Time: 11.5527 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #57: GFLOPs: 583.0753. Time: 7.4090 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #58: GFLOPs: 597.1678. Time: 7.2341 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #59: GFLOPs: 267.6657. Time: 16.1395 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #60: GFLOPs: 518.3681. Time: 8.3338 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #61: GFLOPs: 314.2435. Time: 13.7473 us. Best GFLOPs: 649.5741
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:121] [Task #32: fused_nn_avg_pool2d_3] Trial #62: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  44: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  43: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  42: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  41: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  40: tvm::transform::Pass::operator()(tvm::IRModule) const
  39: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  38: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  37: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  36: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  35: _ZN3tvm7runtime13PackedFun
  34: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  30: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  26: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  25: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorI
  23: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  22: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  21: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorI
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorI
  15: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  14: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  13: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  12: _ZZN3tvm3tir11StmtFunctorIFNS
  11: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  10: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  7: _ZZN3tvm3tir11StmtFunctorI
  6: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  5: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  4: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  3: _ZZN3tvm3tir11StmtFunctorIFNS
  2: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  1: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(288), T.int64(27), T.int64(27)), "float32"), pool_avg: T.Buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_sum_shared = T.alloc_buffer((T.int64(1), T.int64(288), T.int64(25), T.int64(25)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(50400), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(3)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_sum"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(175) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(175) // T.int64(7) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3 < T.int64(25) and ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1])
                        T.writes(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(0)
                        pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p0[v_ax0, v_ax1, v_ax2 + v_rv0, v_ax3 + v_rv1]
            for ax3_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                with T.block("pool_avg"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(288), ax0_ax1_ax2_ax3_0_fused // T.int64(175))
                    v_ax2 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(175) // T.int64(7))
                    v_ax3 = T.axis.spatial(T.int64(25), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(4) + ax3_1 < T.int64(25))
                    T.reads(pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                    T.writes(pool_avg[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_avg[v_ax0, v_ax1, v_ax2, v_ax3] = pool_sum_shared[v_ax0, v_ax1, v_ax2, v_ax3] / T.Cast("float32", T.max((T.min(T.int64(2), T.int64(26) - v_ax2) + T.int64(1) - T.max(T.int64(0) - v_ax2, T.int64(0))) * (T.min(T.int64(2), T.int64(26) - v_ax3) + T.int64(1) - T.max(T.int64(0) - v_ax3, T.int64(0))), T.int64(1)))
b0 = sch.get_block(name="pool_sum", func_name="main")
b1 = sch.get_block(name="pool_avg", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
b3, = sch.get_consumers(block=b0)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l9, l10 = sch.split(loop=l7, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b0)
l21 = sch.fuse(l19, l20, preserve_unit_iters=True)
l22, l23 = sch.split(loop=l21, factors=[None, v8], preserve_unit_iters=True)
sch.bind(loop=l23, thread_axis="threadIdx.x")
v24 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v24)
l25, l26, l27, l28, l29 = sch.get_loops(block=b1)
l30 = sch.fuse(l25, l26, l27, l28, preserve_unit_iters=True)
sch.bind(loop=l30, thread_axis="blockIdx.x")
sch.enter_postproc()
b31 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b31, ann_key="meta_schedule.unroll_explicit")
b32, b33 = sch.get_child_blocks(b31)
l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b32)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l41, l42 = sch.get_loops(block=b33)
2024-05-01 16:37:39 [INFO] [task_scheduler.cc:131] [Task #32: fused_nn_avg_pool2d_3] Trial #63: GFLOPs: 421.4639. Time: 10.2500 us. Best GFLOPs: 649.5741
