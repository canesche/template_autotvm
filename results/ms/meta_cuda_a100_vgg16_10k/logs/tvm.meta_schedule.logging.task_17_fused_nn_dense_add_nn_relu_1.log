2024-04-30 01:35:01 [INFO] [task_scheduler.cc:160] Initializing Task #17: "fused_nn_dense_add_nn_relu_1"
2024-04-30 01:35:01 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT = T.alloc_buffer((T.int64(1), T.int64(4096)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(4096)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("T_matmul_NT"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(p0[v_i0, v_k], p1[v_i1, v_k])
                T.writes(T_matmul_NT[v_i0, v_i1])
                with T.init():
                    T_matmul_NT[v_i0, v_i1] = T.float32(0)
                T_matmul_NT[v_i0, v_i1] = T_matmul_NT[v_i0, v_i1] + p0[v_i0, v_k] * p1[v_i1, v_k]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(4096)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_matmul_NT[v_ax0, v_ax1], p2[v_ax0, v_ax1])
                T.writes(T_add[v_ax0, v_ax1])
                T_add[v_ax0, v_ax1] = T_matmul_NT[v_ax0, v_ax1] + p2[v_ax0, v_ax1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(4096)):
            with T.block("T_relu"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_add[v_ax0, v_ax1])
                T.writes(T_relu[v_ax0, v_ax1])
                T_relu[v_ax0, v_ax1] = T.max(T_add[v_ax0, v_ax1], T.float32(0))
2024-04-30 01:35:01 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-30 01:35:01 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                        for k_0 in range(T.int64(64)):
                            for ax0_ax1_fused in range(T.int64(64)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(262144)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), ax0_ax1_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(64) + ax0_ax1_fused % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(64) + k_1 + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(2) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_relu[v0, v1])
                                T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 8, 256, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
2024-04-30 01:35:01 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                        for k_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_fused in range(T.int64(64)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(262144)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), ax0_ax1_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + ax0_ax1_fused % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(64) + k_1 + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(2) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_relu[v0, v1])
                                T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 8, 256, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
2024-04-30 01:35:01 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                        for k_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_fused in range(T.int64(64)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(262144)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), ax0_ax1_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(64) + ax0_ax1_fused % T.int64(64))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                                with T.block("T_matmul_NT"):
                                    v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                    v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                    v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(64) + k_1 + k_2)
                                    T.reads(p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                    T.writes(T_matmul_NT_local[v_i0, v_i1])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                                    T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(2) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_relu[v0, v1])
                                T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 8, 256, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 64, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
2024-04-30 02:14:29 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 02:14:29 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-30 02:14:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 502 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1009 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1515 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2530 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3040 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3548 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 4055 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 4563 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 5071 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 5581 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:37 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-04-30 02:14:39 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 120 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:40 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 90 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:41 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 105 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:42 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 88 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:14:43 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9996  0.9986  0.9985  0.9963  0.9953  0.9941  0.9934  0.9925  0.9916  0.9915  0.9907  0.9902  0.9884  0.9873  0.9868  0.9867
[17 : 32]:	0.9858  0.9854  0.9848  0.9831  0.9826  0.9809  0.9803  0.9800  0.9798  0.9796  0.9792  0.9791  0.9787  0.9772  0.9772  0.9771
[33 : 48]:	0.9754  0.9751  0.9743  0.9741  0.9729  0.9727  0.9723  0.9715  0.9711  0.9708  0.9696  0.9695  0.9690  0.9669  0.9660  0.9658
[49 : 64]:	0.9655  0.9646  0.9643  0.9631  0.9584  0.9580  0.9563  0.9529  0.9515  0.9513  0.9506  0.9504  0.9502  0.9484  0.9476  0.9475
2024-04-30 02:14:43 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 02:14:43 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #1: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(16))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 2, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #2: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 4, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 8, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #3: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(16))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 4, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #4: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 64, 4, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 4, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #5: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(8))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 64, 4, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 1, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #6: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + i1_3_init * T.int64(16) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + i1_3 * T.int64(16) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(16)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(16) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 4, 32, 1, 16])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #7: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 4, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 4, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #8: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 512, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 512], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 512, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #9: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 4, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #10: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  313: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  312: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  311: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  310: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  309: tvm::transform::Pass::operator()(tvm::IRModule) const
  308: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  307: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: _ZN3tvm7runtime13PackedFun
  303: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  302: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  301: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  299: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  298: _ZZN3tvm3tir11StmtFunctorI
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorI
  294: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  293: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  292: _ZZN3tvm3tir11StmtFunctorI
  291: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  290: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  289: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  288: _ZZN3tvm3tir11StmtFunctorI
  287: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  286: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  285: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  284: _ZZN3tvm3tir11StmtFunctorI
  283: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  282: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  281: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  280: _ZZN3tvm3tir11StmtFunctorI
  279: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  278: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  277: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  276: _ZZN3tvm3tir11StmtFunctorI
  275: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  274: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  273: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  272: _ZZN3tvm3tir11StmtFunctorI
  271: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorI
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorI
  263: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  262: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  261: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  260: _ZZN3tvm3tir11StmtFunctorI
  259: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  258: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  257: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  256: _ZZN3tvm3tir11StmtFunctorI
  255: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  254: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  253: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  252: _ZZN3tvm3tir11StmtFunctorI
  251: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  250: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  249: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  248: _ZZN3tvm3tir11StmtFunctorI
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorI
  244: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  243: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  242: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  241: _ZZN3tvm3tir11StmtFunctorI
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorI
  237: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  236: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  235: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  234: _ZZN3tvm3tir11StmtFunctorI
  233: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  232: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  231: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  230: _ZZN3tvm3tir11StmtFunctorI
  229: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  228: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  227: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  226: _ZZN3tvm3tir11StmtFunctorI
  225: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  224: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  223: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  222: _ZZN3tvm3tir11StmtFunctorI
  221: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  220: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  219: _ZZN3tvm3tir11StmtFunctorI
  218: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  217: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  216: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  215: _ZZN3tvm3tir11StmtFunctorI
  214: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  213: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  212: _ZZN3tvm3tir11StmtFunctorI
  211: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  210: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  209: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  208: _ZZN3tvm3tir11StmtFunctorI
  207: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  206: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  205: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  204: _ZZN3tvm3tir11StmtFunctorI
  203: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  200: _ZZN3tvm3tir11StmtFunctorI
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  197: _ZZN3tvm3tir11StmtFunctorI
  196: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  195: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  194: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  193: _ZZN3tvm3tir11StmtFunctorI
  192: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  191: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  190: _ZZN3tvm3tir11StmtFunctorI
  189: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  188: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  187: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  186: _ZZN3tvm3tir11StmtFunctorI
  185: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorI
  181: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  180: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  179: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  178: _ZZN3tvm3tir11StmtFunctorI
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorI
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorI
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorI
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorI
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorI
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorI
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorI
  155: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  154: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  153: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorI
  151: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  150: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  149: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  148: _ZZN3tvm3tir11StmtFunctorI
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorI
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorI
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorI
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorI
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS
  132: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  131: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  129: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  128: _ZZN3tvm3tir11StmtFunctorI
  127: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  126: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  125: _ZZN3tvm3tir11StmtFunctorI
  124: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  123: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  122: _ZZN3tvm3tir11StmtFunctorI
  121: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  120: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  119: _ZZN3tvm3tir11StmtFunctorI
  118: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  117: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  116: _ZZN3tvm3tir11StmtFunctorI
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorI
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorI
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS
  106: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  105: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  103: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  102: _ZZN3tvm3tir11StmtFunctorI
  101: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  100: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  99: _ZZN3tvm3tir11StmtFunctorI
  98: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  97: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  96: _ZZN3tvm3tir11StmtFunctorI
  95: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  94: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  93: _ZZN3tvm3tir11StmtFunctorI
  92: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  91: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  90: _ZZN3tvm3tir11StmtFunctorI
  89: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  88: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  87: _ZZN3tvm3tir11StmtFunctorI
  86: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  85: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  84: _ZZN3tvm3tir11StmtFunctorI
  83: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  82: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  81: _ZZN3tvm3tir11StmtFunctorI
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS
  77: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  76: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  74: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  73: _ZZN3tvm3tir11StmtFunctorI
  72: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  71: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  70: _ZZN3tvm3tir11StmtFunctorI
  69: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  68: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  67: _ZZN3tvm3tir11StmtFunctorI
  66: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  65: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  64: _ZZN3tvm3tir11StmtFunctorI
  63: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  62: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  61: _ZZN3tvm3tir11StmtFunctorI
  60: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  59: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  58: _ZZN3tvm3tir11StmtFunctorI
  57: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  56: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  55: _ZZN3tvm3tir11StmtFunctorI
  54: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  53: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  52: _ZZN3tvm3tir11StmtFunctorI
  51: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  50: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  49: _ZZN3tvm3tir11StmtFunctorIFNS
  48: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  47: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorI
  43: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorI
  40: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  39: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: _ZZN3tvm3tir11StmtFunctorI
  37: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: _ZZN3tvm3tir11StmtFunctorI
  34: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  30: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorI
  28: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  27: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: _ZZN3tvm3tir11StmtFunctorI
  25: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  24: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS
  22: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  21: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  19: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: _ZZN3tvm3tir11StmtFunctorI
  17: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorI
  13: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  12: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  11: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  10: _ZZN3tvm3tir11StmtFunctorI
  9: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  8: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  7: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  6: _ZZN3tvm3tir11StmtFunctorIFNS
  5: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  4: _ZZN3tvm3tir11ExprFunctorI
  3: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  2: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  1: tvm::tir::APIType(tvm::runtime::DataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 8, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #11: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(1024), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(4))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(4))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 2, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 2, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71 = sch.split(loop=l69, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #12: GFLOPs: 40.4502. Time: 829.7261 us. Best GFLOPs: 40.4502
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #13: GFLOPs: 23.4790. Time: 1429.4752 us. Best GFLOPs: 40.4502
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #14: GFLOPs: 64.7026. Time: 518.7210 us. Best GFLOPs: 64.7026
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #15: GFLOPs: 18.8529. Time: 1780.2330 us. Best GFLOPs: 64.7026
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #16: GFLOPs: 76.0010. Time: 441.6077 us. Best GFLOPs: 76.0010
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #17: GFLOPs: 38.4789. Time: 872.2343 us. Best GFLOPs: 76.0010
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #18: GFLOPs: 105.2526. Time: 318.8768 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #19: GFLOPs: 17.5586. Time: 1911.4602 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #20: GFLOPs: 7.8553. Time: 4272.5973 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #21: GFLOPs: 4.6480. Time: 7220.8094 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #22: GFLOPs: 38.2220. Time: 878.0979 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #23: GFLOPs: 7.3593. Time: 4560.5843 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #24: GFLOPs: 8.7172. Time: 3850.1613 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #25: GFLOPs: 13.3451. Time: 2514.9696 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #26: GFLOPs: 59.8842. Time: 560.4587 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #27: GFLOPs: 77.2671. Time: 434.3715 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #28: GFLOPs: 1.9318. Time: 17374.0373 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #29: GFLOPs: 38.4510. Time: 872.8665 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #30: GFLOPs: 16.3406. Time: 2053.9350 us. Best GFLOPs: 105.2526
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #31: GFLOPs: 150.6283. Time: 222.8176 us. Best GFLOPs: 150.6283
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #32: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 1, 64, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #33: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(16) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 4, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 16, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71 = sch.split(loop=l69, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #34: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 1, 256, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #35: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 2, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #36: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(4096)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.where(ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 256, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 256, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #37: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 64, 4, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 4, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #38: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(16))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(16) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 4, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 16, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #39: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(1024)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(4))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(4))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 1, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #40: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 512, 4, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 512], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 512, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #41: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 128, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #42: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 1, 32, 2, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 4, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #43: GFLOPs: 96.8905. Time: 346.3974 us. Best GFLOPs: 150.6283
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #44: GFLOPs: 79.3971. Time: 422.7185 us. Best GFLOPs: 150.6283
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #45: GFLOPs: 20.0438. Time: 1674.4619 us. Best GFLOPs: 150.6283
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #46: GFLOPs: 7.2758. Time: 4612.8873 us. Best GFLOPs: 150.6283
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #47: GFLOPs: 7.1498. Time: 4694.2020 us. Best GFLOPs: 150.6283
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #48: GFLOPs: 4.6366. Time: 7238.5826 us. Best GFLOPs: 150.6283
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #49: GFLOPs: 13.4829. Time: 2489.2691 us. Best GFLOPs: 150.6283
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #50: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 16, 64, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #51: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(32) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 2, 128, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 4, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #52: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  313: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  312: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  311: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  310: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  309: tvm::transform::Pass::operator()(tvm::IRModule) const
  308: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  307: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: _ZN3tvm7runtime13PackedFun
  303: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  302: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  301: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  299: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  298: _ZZN3tvm3tir11StmtFunctorI
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorI
  294: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  293: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  292: _ZZN3tvm3tir11StmtFunctorI
  291: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  290: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  289: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  288: _ZZN3tvm3tir11StmtFunctorI
  287: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  286: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  285: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  284: _ZZN3tvm3tir11StmtFunctorI
  283: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  282: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  281: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  280: _ZZN3tvm3tir11StmtFunctorI
  279: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  278: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  277: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  276: _ZZN3tvm3tir11StmtFunctorI
  275: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  274: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  273: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  272: _ZZN3tvm3tir11StmtFunctorI
  271: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorI
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorI
  263: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  262: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  261: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  260: _ZZN3tvm3tir11StmtFunctorI
  259: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  258: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  257: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  256: _ZZN3tvm3tir11StmtFunctorI
  255: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  254: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  253: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  252: _ZZN3tvm3tir11StmtFunctorI
  251: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  250: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  249: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  248: _ZZN3tvm3tir11StmtFunctorI
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorI
  244: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  243: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  242: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  241: _ZZN3tvm3tir11StmtFunctorI
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorI
  237: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  236: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  235: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  234: _ZZN3tvm3tir11StmtFunctorI
  233: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  232: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  231: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  230: _ZZN3tvm3tir11StmtFunctorI
  229: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  228: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  227: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  226: _ZZN3tvm3tir11StmtFunctorI
  225: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  224: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  223: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  222: _ZZN3tvm3tir11StmtFunctorI
  221: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  220: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  219: _ZZN3tvm3tir11StmtFunctorI
  218: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  217: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  216: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  215: _ZZN3tvm3tir11StmtFunctorI
  214: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  213: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  212: _ZZN3tvm3tir11StmtFunctorI
  211: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  210: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  209: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  208: _ZZN3tvm3tir11StmtFunctorI
  207: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  206: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  205: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  204: _ZZN3tvm3tir11StmtFunctorI
  203: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  200: _ZZN3tvm3tir11StmtFunctorI
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  197: _ZZN3tvm3tir11StmtFunctorI
  196: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  195: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  194: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  193: _ZZN3tvm3tir11StmtFunctorI
  192: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  191: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  190: _ZZN3tvm3tir11StmtFunctorI
  189: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  188: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  187: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  186: _ZZN3tvm3tir11StmtFunctorI
  185: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorI
  181: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  180: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  179: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  178: _ZZN3tvm3tir11StmtFunctorI
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorI
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorI
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorI
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorI
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorI
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorI
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorI
  155: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  154: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  153: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorI
  151: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  150: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  149: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  148: _ZZN3tvm3tir11StmtFunctorI
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorI
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorI
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorI
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorI
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS
  132: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  131: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  129: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  128: _ZZN3tvm3tir11StmtFunctorI
  127: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  126: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  125: _ZZN3tvm3tir11StmtFunctorI
  124: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  123: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  122: _ZZN3tvm3tir11StmtFunctorI
  121: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  120: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  119: _ZZN3tvm3tir11StmtFunctorI
  118: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  117: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  116: _ZZN3tvm3tir11StmtFunctorI
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorI
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorI
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS
  106: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  105: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  103: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  102: _ZZN3tvm3tir11StmtFunctorI
  101: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  100: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  99: _ZZN3tvm3tir11StmtFunctorI
  98: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  97: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  96: _ZZN3tvm3tir11StmtFunctorI
  95: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  94: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  93: _ZZN3tvm3tir11StmtFunctorI
  92: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  91: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  90: _ZZN3tvm3tir11StmtFunctorI
  89: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  88: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  87: _ZZN3tvm3tir11StmtFunctorI
  86: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  85: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  84: _ZZN3tvm3tir11StmtFunctorI
  83: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  82: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  81: _ZZN3tvm3tir11StmtFunctorI
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS
  77: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  76: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  74: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  73: _ZZN3tvm3tir11StmtFunctorI
  72: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  71: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  70: _ZZN3tvm3tir11StmtFunctorI
  69: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  68: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  67: _ZZN3tvm3tir11StmtFunctorI
  66: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  65: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  64: _ZZN3tvm3tir11StmtFunctorI
  63: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  62: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  61: _ZZN3tvm3tir11StmtFunctorI
  60: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  59: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  58: _ZZN3tvm3tir11StmtFunctorI
  57: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  56: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  55: _ZZN3tvm3tir11StmtFunctorI
  54: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  53: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  52: _ZZN3tvm3tir11StmtFunctorI
  51: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  50: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  49: _ZZN3tvm3tir11StmtFunctorIFNS
  48: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  47: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorI
  43: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorI
  40: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  39: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: _ZZN3tvm3tir11StmtFunctorI
  37: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: _ZZN3tvm3tir11StmtFunctorI
  34: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  30: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorI
  28: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  27: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: _ZZN3tvm3tir11StmtFunctorI
  25: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  24: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS
  22: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  21: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  19: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: _ZZN3tvm3tir11StmtFunctorI
  17: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorI
  13: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  12: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  11: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  10: _ZZN3tvm3tir11StmtFunctorI
  9: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  8: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  7: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  6: _ZZN3tvm3tir11StmtFunctorIFNS
  5: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  4: _ZZN3tvm3tir11ExprFunctorI
  3: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  2: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  1: tvm::tir::APIType(tvm::runtime::DataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 1, 32, 8, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #53: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 512, 2, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 512], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 512], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #54: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 64, 4, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 8, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #55: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(1024)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(4))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(128)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(4))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(4) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 8, 32, 4, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 4, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #56: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(1024)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(4))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(4))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 4, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 1, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #57: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(1024), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(4))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(4))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 32, 2, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 2, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #58: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(4096)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 64, 4, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #59: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(1024), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(4))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(4))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(4) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 1, 64, 4, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 2, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #60: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 4, 64, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #61: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(16))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(16) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 1, 64, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 4, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #62: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(16) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 4, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[256, 16, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #63: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 2, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 2, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63, l64 = sch.split(loop=l61, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l64)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l65, l66, l67, l68, l69 = sch.get_loops(block=b46)
l70, l71, l72 = sch.split(loop=l69, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l72)
sch.bind(loop=l71, thread_axis="threadIdx.x")
b73 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.unroll_explicit")
b74, b75, b76, b77 = sch.get_child_blocks(b73)
l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b75)
l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106 = sch.get_loops(block=b77)
b107 = sch.get_block(name="T_matmul_NT", func_name="main")
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b107)
b118 = sch.decompose_reduction(block=b107, loop=l111)
2024-04-30 02:17:33 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #64: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 1, 32, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 02:36:25 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 02:36:25 [INFO] [evolutionary_search.cc:715] Picked top 27 candidate(s) from database
2024-04-30 02:36:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 477 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 958 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1439 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1916 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2878 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3360 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3836 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 4314 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:31 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2024-04-30 02:36:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 123 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:38 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 99 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 02:36:39 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.3391  1.3391  1.3352  1.3352  1.3293  1.3254  1.3190  1.3190  1.3117  1.3093  1.3071  1.3071  1.3056  1.3056  1.2979  1.2979
[17 : 32]:	1.2955  1.2938  1.2922  1.2910  1.2909  1.2894  1.2894  1.2791  1.2791  1.2728  1.2701  1.2653  1.2653  1.2635  1.2635  1.2635
[33 : 48]:	1.2635  1.2635  1.2621  1.2616  1.2609  1.2553  1.2516  1.2510  1.2475  1.2436  1.2397  1.2397  1.2361  1.2361  1.2361  1.2361
[49 : 64]:	1.2328  1.2172  1.2172  1.2150  1.2150  1.2111  1.2109  1.2109  1.2097  1.2097  1.2094  1.2094  1.2059  1.2055  1.2055  1.2041
2024-04-30 02:36:39 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 02:36:39 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #65: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 8, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #66: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 1, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #67: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 8, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #68: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 1, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #69: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 1, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #70: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 1, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #71: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 8, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #72: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(128) + i0_1_i1_1_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[32, 2, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 1, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #73: GFLOPs: 107.9300. Time: 310.9666 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #74: GFLOPs: 94.1515. Time: 356.4746 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #75: GFLOPs: 107.9567. Time: 310.8896 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #76: GFLOPs: 107.9341. Time: 310.9547 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #77: GFLOPs: 107.9367. Time: 310.9474 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #78: GFLOPs: 107.9622. Time: 310.8738 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #79: GFLOPs: 127.2761. Time: 263.6993 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #80: GFLOPs: 127.2911. Time: 263.6682 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #81: GFLOPs: 88.6718. Time: 378.5041 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #82: GFLOPs: 107.9513. Time: 310.9054 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #83: GFLOPs: 107.9367. Time: 310.9474 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #84: GFLOPs: 107.9369. Time: 310.9468 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #85: GFLOPs: 89.2357. Time: 376.1121 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #86: GFLOPs: 92.1620. Time: 364.1700 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #87: GFLOPs: 92.1691. Time: 364.1418 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #88: GFLOPs: 88.6073. Time: 378.7793 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #89: GFLOPs: 89.0545. Time: 376.8775 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #90: GFLOPs: 88.6815. Time: 378.4627 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #91: GFLOPs: 92.0482. Time: 364.6203 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #92: GFLOPs: 88.8866. Time: 377.5894 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #93: GFLOPs: 89.2375. Time: 376.1047 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #94: GFLOPs: 89.1020. Time: 376.6765 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #95: GFLOPs: 89.1122. Time: 376.6334 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #96: GFLOPs: 89.3113. Time: 375.7937 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #97: GFLOPs: 88.6644. Time: 378.5354 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #98: GFLOPs: 89.2284. Time: 376.1429 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #99: GFLOPs: 88.9540. Time: 377.3033 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #100: GFLOPs: 96.2921. Time: 348.5501 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #101: GFLOPs: 75.8308. Time: 442.5991 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #102: GFLOPs: 89.0579. Time: 376.8631 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #103: GFLOPs: 75.5628. Time: 444.1684 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #104: GFLOPs: 98.0172. Time: 342.4156 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #105: GFLOPs: 68.6108. Time: 489.1741 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #106: GFLOPs: 68.4809. Time: 490.1019 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #107: GFLOPs: 88.6590. Time: 378.5587 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #108: GFLOPs: 89.1122. Time: 376.6334 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #109: GFLOPs: 79.7780. Time: 420.7005 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #110: GFLOPs: 34.4813. Time: 973.3567 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #111: GFLOPs: 33.9089. Time: 989.7871 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #112: GFLOPs: 33.8995. Time: 990.0635 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #113: GFLOPs: 61.9831. Time: 541.4801 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #114: GFLOPs: 62.2176. Time: 539.4391 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #115: GFLOPs: 62.2255. Time: 539.3706 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #116: GFLOPs: 36.5643. Time: 917.9080 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #117: GFLOPs: 34.7547. Time: 965.7010 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #118: GFLOPs: 75.8270. Time: 442.6212 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #119: GFLOPs: 62.2285. Time: 539.3452 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #120: GFLOPs: 62.2323. Time: 539.3122 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #121: GFLOPs: 34.6887. Time: 967.5381 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #122: GFLOPs: 33.4962. Time: 1001.9840 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #123: GFLOPs: 33.4992. Time: 1001.8919 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #124: GFLOPs: 34.6469. Time: 968.7040 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #125: GFLOPs: 61.2886. Time: 547.6162 us. Best GFLOPs: 150.6283
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #126: GFLOPs: 216.1050. Time: 155.3070 us. Best GFLOPs: 216.1050
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #127: GFLOPs: 1.3731. Time: 24442.4696 us. Best GFLOPs: 216.1050
2024-04-30 02:37:56 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #128: GFLOPs: 37.3698. Time: 898.1212 us. Best GFLOPs: 216.1050
2024-04-30 03:21:29 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 03:21:30 [INFO] [evolutionary_search.cc:715] Picked top 83 candidate(s) from database
2024-04-30 03:21:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 424 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 848 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1272 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1698 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2117 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2539 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2961 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3386 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:35 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-04-30 03:21:37 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 116 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:39 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 94 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:40 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:42 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 116 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:21:43 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1175  1.0834  1.0576  1.0381  0.9852  0.9852  0.9852  0.9802  0.9802  0.9802  0.9761  0.9761  0.9761  0.9761  0.9712  0.9712
[17 : 32]:	0.9712  0.9712  0.9712  0.9704  0.9704  0.9643  0.9636  0.9636  0.9616  0.9616  0.9616  0.9614  0.9614  0.9614  0.9580  0.9580
[33 : 48]:	0.9527  0.9527  0.9526  0.9526  0.9526  0.9526  0.9420  0.9420  0.9420  0.9420  0.9420  0.9393  0.9384  0.9384  0.9384  0.9370
[49 : 64]:	0.9370  0.9370  0.9370  0.9370  0.9370  0.9361  0.9361  0.9361  0.9339  0.9339  0.9295  0.9295  0.9295  0.9184  0.9184  0.8983
2024-04-30 03:21:43 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 03:21:43 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #129: GFLOPs: 43.3859. Time: 773.5840 us. Best GFLOPs: 216.1050
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #130: GFLOPs: 43.3917. Time: 773.4794 us. Best GFLOPs: 216.1050
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #131: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[128, 1, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #132: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #133: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 1, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #134: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 32, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #135: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 4, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #136: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 16, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #137: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 8, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #138: GFLOPs: 212.4993. Time: 157.9423 us. Best GFLOPs: 216.1050
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #139: GFLOPs: 219.9746. Time: 152.5750 us. Best GFLOPs: 219.9746
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #140: GFLOPs: 220.0027. Time: 152.5555 us. Best GFLOPs: 220.0027
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #141: GFLOPs: 219.9942. Time: 152.5614 us. Best GFLOPs: 220.0027
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #142: GFLOPs: 220.0254. Time: 152.5398 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #143: GFLOPs: 214.9451. Time: 156.1451 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #144: GFLOPs: 214.9410. Time: 156.1481 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #145: GFLOPs: 214.9993. Time: 156.1058 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #146: GFLOPs: 214.9342. Time: 156.1530 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #147: GFLOPs: 214.9743. Time: 156.1239 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #148: GFLOPs: 215.9573. Time: 155.4132 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #149: GFLOPs: 215.9583. Time: 155.4125 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #150: GFLOPs: 43.3468. Time: 774.2819 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #151: GFLOPs: 41.2310. Time: 814.0134 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #152: GFLOPs: 46.1591. Time: 727.1068 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #153: GFLOPs: 215.9487. Time: 155.4194 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #154: GFLOPs: 217.7689. Time: 154.1204 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #155: GFLOPs: 212.4947. Time: 157.9457 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #156: GFLOPs: 214.9711. Time: 156.1262 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #157: GFLOPs: 180.2131. Time: 186.2385 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #158: GFLOPs: 214.9636. Time: 156.1316 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #159: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #160: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(4096)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0)
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #161: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(4096)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0)
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #162: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #163: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 2, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #164: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 16, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #165: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 16, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #166: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(128)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(32) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(32) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 4, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #167: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 2, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b73)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b75)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #168: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 1, 32])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b73)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b75)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #169: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 16, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b73)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b75)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #170: GFLOPs: 203.0656. Time: 165.2797 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #171: GFLOPs: 203.1367. Time: 165.2219 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #172: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #173: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(32))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(32) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 64, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[128, 4, 8])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b73)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b75)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #174: GFLOPs: 202.6955. Time: 165.5815 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #175: GFLOPs: 208.2968. Time: 161.1289 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #176: GFLOPs: 202.7304. Time: 165.5530 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #177: GFLOPs: 203.1907. Time: 165.1779 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #178: GFLOPs: 208.3101. Time: 161.1186 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #179: GFLOPs: 208.3189. Time: 161.1117 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #180: GFLOPs: 203.2063. Time: 165.1653 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #181: GFLOPs: 202.7381. Time: 165.5467 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #182: GFLOPs: 41.2437. Time: 813.7636 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #183: GFLOPs: 46.1700. Time: 726.9361 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #184: GFLOPs: 41.2348. Time: 813.9385 us. Best GFLOPs: 220.0254
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #185: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #186: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(4096)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #187: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(4096)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    v1 = T.axis.spatial(T.int64(4096), k_0)
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #188: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b73)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b75)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #189: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1)
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 2, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b73)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b75)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #190: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(4) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(4)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(4) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(2048) + i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 2, 128, 2, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69 = sch.split(loop=l67, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2024-04-30 03:22:57 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #191: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(64) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[64, 1, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 2, 4])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 03:54:00 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 03:54:01 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 03:54:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1620 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2027 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2434 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2841 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3248 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3652 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 4050 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:06 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-04-30 03:54:08 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 133 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:10 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 141 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:14 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 135 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 03:54:15 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1271  1.0471  1.0246  1.0246  1.0246  1.0246  1.0205  1.0205  1.0205  1.0084  1.0068  1.0068  1.0043  0.9958  0.9958  0.9958
[17 : 32]:	0.9885  0.9878  0.9878  0.9878  0.9878  0.9878  0.9878  0.9861  0.9847  0.9844  0.9844  0.9844  0.9844  0.9844  0.9841  0.9841
[33 : 48]:	0.9841  0.9841  0.9841  0.9828  0.9828  0.9799  0.9799  0.9783  0.9783  0.9777  0.9777  0.9777  0.9777  0.9777  0.9750  0.9746
[49 : 64]:	0.9746  0.9745  0.9745  0.9743  0.9743  0.9743  0.9743  0.9743  0.9743  0.9739  0.9739  0.9739  0.9739  0.9739  0.9739  0.9739
2024-04-30 03:54:15 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 03:54:15 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #192: GFLOPs: 315.7716. Time: 106.2876 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #193: GFLOPs: 95.0615. Time: 353.0622 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #194: GFLOPs: 127.3036. Time: 263.6424 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #195: GFLOPs: 127.3093. Time: 263.6306 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #196: GFLOPs: 95.0560. Time: 353.0828 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #197: GFLOPs: 83.5511. Time: 401.7017 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #198: GFLOPs: 110.7515. Time: 303.0444 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #199: GFLOPs: 98.1405. Time: 341.9855 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #200: GFLOPs: 98.1199. Time: 342.0574 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #201: GFLOPs: 123.6925. Time: 271.3393 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #202: GFLOPs: 91.0247. Time: 368.7199 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #203: GFLOPs: 91.0510. Time: 368.6136 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #204: GFLOPs: 254.7255. Time: 131.7600 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #205: GFLOPs: 127.2883. Time: 263.6742 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #206: GFLOPs: 95.0946. Time: 352.9394 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #207: GFLOPs: 83.5555. Time: 401.6806 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #208: GFLOPs: 215.3892. Time: 155.8231 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #209: GFLOPs: 220.2900. Time: 152.3566 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #210: GFLOPs: 215.3823. Time: 155.8281 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #211: GFLOPs: 215.3847. Time: 155.8264 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #212: GFLOPs: 180.6523. Time: 185.7857 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #213: GFLOPs: 215.3743. Time: 155.8339 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #214: GFLOPs: 180.6866. Time: 185.7505 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #215: GFLOPs: 110.7304. Time: 303.1021 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #216: GFLOPs: 213.1469. Time: 157.4624 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #217: GFLOPs: 127.2512. Time: 263.7510 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #218: GFLOPs: 91.0389. Time: 368.6626 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #219: GFLOPs: 91.0346. Time: 368.6798 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #220: GFLOPs: 85.2929. Time: 393.4982 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #221: GFLOPs: 127.2568. Time: 263.7393 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #222: GFLOPs: 218.0494. Time: 153.9221 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #223: GFLOPs: 213.1265. Time: 157.4775 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #224: GFLOPs: 216.2956. Time: 155.1701 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #225: GFLOPs: 218.0273. Time: 153.9377 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #226: GFLOPs: 218.0254. Time: 153.9390 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #227: GFLOPs: 127.3012. Time: 263.6473 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #228: GFLOPs: 84.3462. Time: 397.9151 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #229: GFLOPs: 254.7499. Time: 131.7474 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #230: GFLOPs: 254.7482. Time: 131.7482 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #231: GFLOPs: 220.2742. Time: 152.3675 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #232: GFLOPs: 180.6767. Time: 185.7607 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #233: GFLOPs: 180.6962. Time: 185.7406 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #234: GFLOPs: 215.3943. Time: 155.8195 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #235: GFLOPs: 220.2888. Time: 152.3574 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #236: GFLOPs: 180.6964. Time: 185.7404 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #237: GFLOPs: 220.2759. Time: 152.3663 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #238: GFLOPs: 213.1264. Time: 157.4776 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #239: GFLOPs: 218.0417. Time: 153.9276 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #240: GFLOPs: 216.3081. Time: 155.1612 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #241: GFLOPs: 216.3150. Time: 155.1563 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #242: GFLOPs: 213.1142. Time: 157.4866 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #243: GFLOPs: 213.1169. Time: 157.4846 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #244: GFLOPs: 213.0784. Time: 157.5130 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #245: GFLOPs: 213.1049. Time: 157.4934 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #246: GFLOPs: 218.0502. Time: 153.9216 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #247: GFLOPs: 216.3246. Time: 155.1494 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #248: GFLOPs: 216.3157. Time: 155.1557 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #249: GFLOPs: 218.0240. Time: 153.9400 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #250: GFLOPs: 213.1039. Time: 157.4942 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #251: GFLOPs: 216.3199. Time: 155.1528 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #252: GFLOPs: 216.3116. Time: 155.1587 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #253: GFLOPs: 34.2014. Time: 981.3233 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #254: GFLOPs: 24.6233. Time: 1363.0408 us. Best GFLOPs: 315.7716
2024-04-30 03:55:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #255: GFLOPs: 103.0829. Time: 325.5887 us. Best GFLOPs: 315.7716
2024-04-30 04:32:05 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 04:32:05 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 04:32:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 404 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 803 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1613 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2016 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2416 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2822 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3224 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:10 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2024-04-30 04:32:11 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:14 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 151 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:18 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 148 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 04:32:18 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0052  0.9883  0.9883  0.9883  0.9883  0.9867  0.9867  0.9727  0.8685  0.8670  0.8463  0.8370  0.8370  0.8349  0.8278  0.8062
[17 : 32]:	0.8062  0.8062  0.8055  0.8055  0.8055  0.8055  0.8054  0.8054  0.8054  0.8054  0.8054  0.7917  0.7909  0.7545  0.7537  0.7537
[33 : 48]:	0.7537  0.7169  0.7169  0.7065  0.7065  0.7062  0.7047  0.7047  0.7047  0.7047  0.7047  0.7047  0.7043  0.7043  0.7043  0.7036
[49 : 64]:	0.7029  0.7028  0.6939  0.6939  0.6939  0.6921  0.6921  0.6921  0.6921  0.6921  0.6921  0.6921  0.6921  0.6805  0.6805  0.6805
2024-04-30 04:32:19 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 04:32:19 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #256: GFLOPs: 314.9026. Time: 106.5810 us. Best GFLOPs: 315.7716
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #257: GFLOPs: 315.5205. Time: 106.3723 us. Best GFLOPs: 315.7716
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #258: GFLOPs: 319.3661. Time: 105.0914 us. Best GFLOPs: 319.3661
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #259: GFLOPs: 315.5851. Time: 106.3505 us. Best GFLOPs: 319.3661
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #260: GFLOPs: 315.5379. Time: 106.3664 us. Best GFLOPs: 319.3661
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #261: GFLOPs: 315.5427. Time: 106.3648 us. Best GFLOPs: 319.3661
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #262: GFLOPs: 315.5313. Time: 106.3686 us. Best GFLOPs: 319.3661
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #263: GFLOPs: 315.5213. Time: 106.3720 us. Best GFLOPs: 319.3661
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #264: GFLOPs: 321.2610. Time: 104.4715 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #265: GFLOPs: 317.9896. Time: 105.5463 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #266: GFLOPs: 318.0673. Time: 105.5205 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #267: GFLOPs: 318.0897. Time: 105.5131 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #268: GFLOPs: 319.9014. Time: 104.9155 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #269: GFLOPs: 319.9712. Time: 104.8926 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #270: GFLOPs: 318.1120. Time: 105.5057 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #271: GFLOPs: 253.9217. Time: 132.1770 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #272: GFLOPs: 253.9609. Time: 132.1566 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #273: GFLOPs: 253.9214. Time: 132.1772 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #274: GFLOPs: 253.9761. Time: 132.1488 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #275: GFLOPs: 253.9617. Time: 132.1562 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #276: GFLOPs: 253.9607. Time: 132.1568 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #277: GFLOPs: 253.9545. Time: 132.1600 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #278: GFLOPs: 253.9980. Time: 132.1374 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #279: GFLOPs: 253.9331. Time: 132.1711 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #280: GFLOPs: 253.9739. Time: 132.1499 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #281: GFLOPs: 253.9701. Time: 132.1519 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #282: GFLOPs: 253.9509. Time: 132.1618 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #283: GFLOPs: 253.9645. Time: 132.1548 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #284: GFLOPs: 253.9399. Time: 132.1676 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #285: GFLOPs: 253.9715. Time: 132.1512 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #286: GFLOPs: 253.9095. Time: 132.1834 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #287: GFLOPs: 253.9480. Time: 132.1634 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #288: GFLOPs: 253.9493. Time: 132.1627 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #289: GFLOPs: 114.9223. Time: 292.0463 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #290: GFLOPs: 114.9662. Time: 291.9348 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #291: GFLOPs: 157.1805. Time: 213.5292 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #292: GFLOPs: 147.5730. Time: 227.4307 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #293: GFLOPs: 105.5628. Time: 317.9399 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #294: GFLOPs: 157.1660. Time: 213.5489 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #295: GFLOPs: 147.5767. Time: 227.4249 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #296: GFLOPs: 147.5784. Time: 227.4224 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #297: GFLOPs: 148.9146. Time: 225.3817 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #298: GFLOPs: 147.5900. Time: 227.4044 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #299: GFLOPs: 157.1845. Time: 213.5237 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #300: GFLOPs: 114.9514. Time: 291.9722 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #301: GFLOPs: 105.5440. Time: 317.9965 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #302: GFLOPs: 114.9353. Time: 292.0132 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #303: GFLOPs: 253.4491. Time: 132.4235 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #304: GFLOPs: 147.5891. Time: 227.4059 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #305: GFLOPs: 253.4554. Time: 132.4202 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #306: GFLOPs: 147.6151. Time: 227.3658 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #307: GFLOPs: 157.1780. Time: 213.5326 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #308: GFLOPs: 157.1590. Time: 213.5585 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #309: GFLOPs: 147.6029. Time: 227.3847 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #310: GFLOPs: 157.1890. Time: 213.5176 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #311: GFLOPs: 157.1690. Time: 213.5448 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #312: GFLOPs: 157.1617. Time: 213.5547 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #313: GFLOPs: 147.5917. Time: 227.4019 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #314: GFLOPs: 157.1675. Time: 213.5468 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #315: GFLOPs: 148.9209. Time: 225.3722 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #316: GFLOPs: 157.1895. Time: 213.5169 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #317: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  313: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  312: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  311: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  310: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  309: tvm::transform::Pass::operator()(tvm::IRModule) const
  308: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  307: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: _ZN3tvm7runtime13PackedFun
  303: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  302: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  301: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  299: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  298: _ZZN3tvm3tir11StmtFunctorI
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorI
  294: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  293: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  292: _ZZN3tvm3tir11StmtFunctorI
  291: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  290: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  289: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  288: _ZZN3tvm3tir11StmtFunctorI
  287: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  286: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  285: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  284: _ZZN3tvm3tir11StmtFunctorI
  283: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  282: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  281: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  280: _ZZN3tvm3tir11StmtFunctorI
  279: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  278: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  277: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  276: _ZZN3tvm3tir11StmtFunctorI
  275: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  274: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  273: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  272: _ZZN3tvm3tir11StmtFunctorI
  271: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorI
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorI
  263: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  262: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  261: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  260: _ZZN3tvm3tir11StmtFunctorI
  259: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  258: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  257: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  256: _ZZN3tvm3tir11StmtFunctorI
  255: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  254: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  253: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  252: _ZZN3tvm3tir11StmtFunctorI
  251: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  250: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  249: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  248: _ZZN3tvm3tir11StmtFunctorI
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorI
  244: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  243: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  242: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  241: _ZZN3tvm3tir11StmtFunctorI
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorI
  237: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  236: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  235: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  234: _ZZN3tvm3tir11StmtFunctorI
  233: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  232: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  231: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  230: _ZZN3tvm3tir11StmtFunctorI
  229: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  228: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  227: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  226: _ZZN3tvm3tir11StmtFunctorI
  225: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  224: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  223: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  222: _ZZN3tvm3tir11StmtFunctorI
  221: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  220: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  219: _ZZN3tvm3tir11StmtFunctorI
  218: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  217: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  216: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  215: _ZZN3tvm3tir11StmtFunctorI
  214: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  213: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  212: _ZZN3tvm3tir11StmtFunctorI
  211: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  210: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  209: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  208: _ZZN3tvm3tir11StmtFunctorI
  207: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  206: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  205: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  204: _ZZN3tvm3tir11StmtFunctorI
  203: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  200: _ZZN3tvm3tir11StmtFunctorI
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  197: _ZZN3tvm3tir11StmtFunctorI
  196: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  195: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  194: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  193: _ZZN3tvm3tir11StmtFunctorI
  192: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  191: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  190: _ZZN3tvm3tir11StmtFunctorI
  189: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  188: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  187: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  186: _ZZN3tvm3tir11StmtFunctorI
  185: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorI
  181: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  180: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  179: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  178: _ZZN3tvm3tir11StmtFunctorI
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorI
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorI
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorI
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorI
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorI
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorI
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorI
  155: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  154: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  153: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorI
  151: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  150: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  149: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  148: _ZZN3tvm3tir11StmtFunctorI
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorI
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorI
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorI
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorI
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS
  132: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  131: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  129: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  128: _ZZN3tvm3tir11StmtFunctorI
  127: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  126: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  125: _ZZN3tvm3tir11StmtFunctorI
  124: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  123: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  122: _ZZN3tvm3tir11StmtFunctorI
  121: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  120: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  119: _ZZN3tvm3tir11StmtFunctorI
  118: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  117: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  116: _ZZN3tvm3tir11StmtFunctorI
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorI
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorI
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS
  106: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  105: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  103: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  102: _ZZN3tvm3tir11StmtFunctorI
  101: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  100: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  99: _ZZN3tvm3tir11StmtFunctorI
  98: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  97: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  96: _ZZN3tvm3tir11StmtFunctorI
  95: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  94: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  93: _ZZN3tvm3tir11StmtFunctorI
  92: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  91: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  90: _ZZN3tvm3tir11StmtFunctorI
  89: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  88: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  87: _ZZN3tvm3tir11StmtFunctorI
  86: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  85: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  84: _ZZN3tvm3tir11StmtFunctorI
  83: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  82: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  81: _ZZN3tvm3tir11StmtFunctorI
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS
  77: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  76: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  74: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  73: _ZZN3tvm3tir11StmtFunctorI
  72: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  71: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  70: _ZZN3tvm3tir11StmtFunctorI
  69: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  68: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  67: _ZZN3tvm3tir11StmtFunctorI
  66: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  65: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  64: _ZZN3tvm3tir11StmtFunctorI
  63: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  62: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  61: _ZZN3tvm3tir11StmtFunctorI
  60: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  59: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  58: _ZZN3tvm3tir11StmtFunctorI
  57: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  56: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  55: _ZZN3tvm3tir11StmtFunctorI
  54: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  53: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  52: _ZZN3tvm3tir11StmtFunctorI
  51: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  50: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  49: _ZZN3tvm3tir11StmtFunctorIFNS
  48: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  47: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorI
  43: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorI
  40: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  39: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: _ZZN3tvm3tir11StmtFunctorI
  37: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: _ZZN3tvm3tir11StmtFunctorI
  34: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  30: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorI
  28: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  27: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: _ZZN3tvm3tir11StmtFunctorI
  25: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  24: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS
  22: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  21: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  19: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: _ZZN3tvm3tir11StmtFunctorI
  17: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorI
  13: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  12: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  11: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  10: _ZZN3tvm3tir11StmtFunctorI
  9: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  8: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  7: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  6: _ZZN3tvm3tir11StmtFunctorIFNS
  5: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  4: _ZZN3tvm3tir11ExprFunctorI
  3: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  2: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  1: tvm::tir::APIType(tvm::runtime::DataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 32, 4, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #318: GFLOPs: 11.3715. Time: 2951.4692 us. Best GFLOPs: 321.2610
2024-04-30 04:33:42 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #319: GFLOPs: 4.8513. Time: 6918.2805 us. Best GFLOPs: 321.2610
2024-04-30 05:28:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 05:29:00 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 05:29:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 813 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1220 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1623 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2025 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2429 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2829 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3234 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3636 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:05 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-04-30 05:29:06 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:08 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 158 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:10 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:12 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 05:29:13 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.7715  1.7640  1.7640  1.7497  1.7435  1.7243  1.7205  1.7062  1.6987  1.6868  1.6845  1.6713  1.6686  1.6650  1.6614  1.6567
[17 : 32]:	1.6508  1.6494  1.6493  1.6468  1.6384  1.6347  1.6274  1.6238  1.6083  1.6018  1.5937  1.5862  1.4459  1.4459  1.4332  1.4316
[33 : 48]:	1.4316  1.4233  1.4165  1.4165  1.4066  1.4057  1.4036  1.3995  1.3995  1.3977  1.3958  1.3931  1.3925  1.3896  1.3890  1.3890
[49 : 64]:	1.3878  1.3832  1.3826  1.3817  1.3806  1.3798  1.3791  1.3785  1.3759  1.3751  1.3749  1.3749  1.3725  1.3725  1.3699  1.3697
2024-04-30 05:29:13 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 05:29:13 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #320: GFLOPs: 325.6460. Time: 103.0647 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #321: GFLOPs: 276.9305. Time: 121.1951 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #322: GFLOPs: 276.8808. Time: 121.2169 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #323: GFLOPs: 296.6438. Time: 113.1412 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #324: GFLOPs: 325.4251. Time: 103.1347 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #325: GFLOPs: 276.9295. Time: 121.1956 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #326: GFLOPs: 325.4962. Time: 103.1122 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #327: GFLOPs: 325.6088. Time: 103.0765 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #328: GFLOPs: 276.9171. Time: 121.2010 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #329: GFLOPs: 325.4542. Time: 103.1255 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #330: GFLOPs: 296.6733. Time: 113.1299 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #331: GFLOPs: 325.4555. Time: 103.1251 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #332: GFLOPs: 276.9057. Time: 121.2060 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #333: GFLOPs: 276.9313. Time: 121.1948 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #334: GFLOPs: 325.4649. Time: 103.1221 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #335: GFLOPs: 276.9224. Time: 121.1987 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #336: GFLOPs: 296.6581. Time: 113.1357 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #337: GFLOPs: 276.9214. Time: 121.1991 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #338: GFLOPs: 325.6135. Time: 103.0750 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #339: GFLOPs: 276.9100. Time: 121.2041 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #340: GFLOPs: 325.4429. Time: 103.1291 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #341: GFLOPs: 296.6583. Time: 113.1356 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #342: GFLOPs: 296.6438. Time: 113.1412 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #343: GFLOPs: 276.9198. Time: 121.1998 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #344: GFLOPs: 325.4555. Time: 103.1251 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #345: GFLOPs: 296.6808. Time: 113.1270 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #346: GFLOPs: 276.8976. Time: 121.2095 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #347: GFLOPs: 325.6103. Time: 103.0760 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #348: GFLOPs: 319.0349. Time: 105.2005 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #349: GFLOPs: 318.9735. Time: 105.2207 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #350: GFLOPs: 325.5894. Time: 103.0827 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #351: GFLOPs: 320.2171. Time: 104.8121 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #352: GFLOPs: 320.2160. Time: 104.8125 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #353: GFLOPs: 323.3358. Time: 103.8011 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #354: GFLOPs: 322.4834. Time: 104.0755 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #355: GFLOPs: 325.4417. Time: 103.1295 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #356: GFLOPs: 322.5009. Time: 104.0699 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #357: GFLOPs: 294.0511. Time: 114.1387 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #358: GFLOPs: 319.0504. Time: 105.1954 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #359: GFLOPs: 325.4542. Time: 103.1255 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #360: GFLOPs: 322.4851. Time: 104.0750 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #361: GFLOPs: 322.4768. Time: 104.0776 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #362: GFLOPs: 293.9316. Time: 114.1852 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #363: GFLOPs: 325.6110. Time: 103.0758 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #364: GFLOPs: 323.2927. Time: 103.8150 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #365: GFLOPs: 322.4459. Time: 104.0876 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #366: GFLOPs: 291.4581. Time: 115.1542 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #367: GFLOPs: 294.0649. Time: 114.1334 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #368: GFLOPs: 322.4809. Time: 104.0763 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #369: GFLOPs: 323.3403. Time: 103.7997 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #370: GFLOPs: 323.3098. Time: 103.8095 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #371: GFLOPs: 291.5225. Time: 115.1287 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #372: GFLOPs: 319.0123. Time: 105.2079 us. Best GFLOPs: 325.6460
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #373: GFLOPs: 325.7623. Time: 103.0280 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #374: GFLOPs: 291.4073. Time: 115.1743 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #375: GFLOPs: 323.3217. Time: 103.8057 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #376: GFLOPs: 175.9925. Time: 190.7049 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #377: GFLOPs: 323.5663. Time: 103.7272 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #378: GFLOPs: 293.2549. Time: 114.4487 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #379: GFLOPs: 293.2423. Time: 114.4535 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #380: GFLOPs: 322.4871. Time: 104.0743 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #381: GFLOPs: 55.5985. Time: 603.6603 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #382: GFLOPs: 4.6732. Time: 7181.9703 us. Best GFLOPs: 325.7623
2024-04-30 05:30:38 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #383: GFLOPs: 64.9318. Time: 516.8903 us. Best GFLOPs: 325.7623
2024-04-30 06:00:13 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 06:00:13 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 06:00:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 812 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1215 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2427 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2830 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3237 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3643 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 4046 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:19 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-04-30 06:00:20 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 174 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:22 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 191 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:24 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:26 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 165 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 06:00:27 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1240  1.1033  1.1018  1.1008  1.0925  1.0923  1.0921  1.0892  1.0838  1.0459  1.0270  1.0232  1.0231  1.0231  1.0212  1.0176
[17 : 32]:	1.0174  1.0104  1.0093  1.0081  1.0078  1.0048  1.0038  1.0022  1.0018  1.0017  1.0015  1.0003  0.9996  0.9986  0.9977  0.9975
[33 : 48]:	0.9975  0.9973  0.9973  0.9973  0.9970  0.9970  0.9961  0.9942  0.9942  0.9939  0.9939  0.9936  0.9933  0.9927  0.9927  0.9924
[49 : 64]:	0.9924  0.9917  0.9904  0.9904  0.9901  0.9901  0.9901  0.9898  0.9898  0.9888  0.9887  0.9887  0.9883  0.9875  0.9874  0.9873
2024-04-30 06:00:27 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 06:00:27 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #384: GFLOPs: 169.9113. Time: 197.5303 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #385: GFLOPs: 171.4532. Time: 195.7538 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #386: GFLOPs: 177.4591. Time: 189.1288 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #387: GFLOPs: 89.5299. Time: 374.8760 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #388: GFLOPs: 89.5274. Time: 374.8864 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #389: GFLOPs: 89.5263. Time: 374.8914 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #390: GFLOPs: 108.8165. Time: 308.4331 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #391: GFLOPs: 89.4212. Time: 375.3317 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #392: GFLOPs: 108.8234. Time: 308.4136 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #393: GFLOPs: 171.4396. Time: 195.7694 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #394: GFLOPs: 89.4320. Time: 375.2865 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #395: GFLOPs: 198.0259. Time: 169.4861 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #396: GFLOPs: 299.0545. Time: 112.2291 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #397: GFLOPs: 299.0564. Time: 112.2284 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #398: GFLOPs: 299.0737. Time: 112.2219 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #399: GFLOPs: 295.3434. Time: 113.6393 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #400: GFLOPs: 295.3604. Time: 113.6328 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #401: GFLOPs: 320.2957. Time: 104.7864 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #402: GFLOPs: 325.5270. Time: 103.1024 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #403: GFLOPs: 323.3445. Time: 103.7984 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #404: GFLOPs: 325.5998. Time: 103.0794 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #405: GFLOPs: 323.3341. Time: 103.8017 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #406: GFLOPs: 325.5517. Time: 103.0946 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #407: GFLOPs: 323.3252. Time: 103.8045 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #408: GFLOPs: 325.6068. Time: 103.0772 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #409: GFLOPs: 319.0152. Time: 105.2070 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #410: GFLOPs: 325.5693. Time: 103.0890 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #411: GFLOPs: 325.7581. Time: 103.0293 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #412: GFLOPs: 322.4974. Time: 104.0710 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #413: GFLOPs: 325.4916. Time: 103.1136 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #414: GFLOPs: 325.6286. Time: 103.0703 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #415: GFLOPs: 325.6522. Time: 103.0628 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #416: GFLOPs: 325.6461. Time: 103.0647 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #417: GFLOPs: 322.4652. Time: 104.0814 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #418: GFLOPs: 322.4283. Time: 104.0933 us. Best GFLOPs: 325.7623
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #419: GFLOPs: 325.7634. Time: 103.0276 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #420: GFLOPs: 322.4830. Time: 104.0756 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #421: GFLOPs: 322.4809. Time: 104.0763 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #422: GFLOPs: 320.2196. Time: 104.8113 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #423: GFLOPs: 320.2442. Time: 104.8032 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #424: GFLOPs: 320.2207. Time: 104.8109 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #425: GFLOPs: 320.2612. Time: 104.7976 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #426: GFLOPs: 320.2299. Time: 104.8079 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #427: GFLOPs: 89.4252. Time: 375.3150 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #428: GFLOPs: 323.5734. Time: 103.7249 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #429: GFLOPs: 323.3320. Time: 103.8024 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #430: GFLOPs: 319.1329. Time: 105.1682 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #431: GFLOPs: 323.3635. Time: 103.7922 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #432: GFLOPs: 323.3276. Time: 103.8038 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #433: GFLOPs: 317.0603. Time: 105.8557 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #434: GFLOPs: 323.3507. Time: 103.7964 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #435: GFLOPs: 323.3279. Time: 103.8037 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #436: GFLOPs: 320.2299. Time: 104.8079 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #437: GFLOPs: 320.2242. Time: 104.8098 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #438: GFLOPs: 325.4500. Time: 103.1268 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #439: GFLOPs: 320.2654. Time: 104.7963 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #440: GFLOPs: 320.2253. Time: 104.8094 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #441: GFLOPs: 317.0954. Time: 105.8439 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #442: GFLOPs: 325.4283. Time: 103.1337 us. Best GFLOPs: 325.7634
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #443: GFLOPs: 325.7706. Time: 103.0253 us. Best GFLOPs: 325.7706
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #444: GFLOPs: 323.3074. Time: 103.8103 us. Best GFLOPs: 325.7706
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #445: GFLOPs: 199.6359. Time: 168.1192 us. Best GFLOPs: 325.7706
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #446: GFLOPs: 155.7089. Time: 215.5472 us. Best GFLOPs: 325.7706
2024-04-30 06:01:52 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #447: GFLOPs: 3.1089. Time: 10795.6222 us. Best GFLOPs: 325.7706
2024-04-30 07:30:13 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 07:30:14 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 07:30:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 802 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1607 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2012 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2413 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2815 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:18 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2024-04-30 07:30:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:21 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 141 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:24 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 07:30:25 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0841  1.0810  1.0810  1.0788  1.0705  1.0674  1.0674  1.0651  1.0422  1.0391  1.0308  1.0275  1.0256  1.0235  1.0083  0.9971
[17 : 32]:	0.9955  0.9948  0.9938  0.9924  0.9922  0.9919  0.9918  0.9917  0.9917  0.9912  0.9912  0.9909  0.9892  0.9890  0.9889  0.9882
[33 : 48]:	0.9882  0.9882  0.9870  0.9866  0.9866  0.9866  0.9864  0.9860  0.9859  0.9855  0.9855  0.9854  0.9854  0.9847  0.9844  0.9840
[49 : 64]:	0.9839  0.9834  0.9833  0.9829  0.9829  0.9824  0.9819  0.9809  0.9809  0.9809  0.9801  0.9801  0.9800  0.9797  0.9786  0.9783
2024-04-30 07:30:25 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 07:30:25 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #448: GFLOPs: 326.9269. Time: 102.6609 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #449: GFLOPs: 326.8627. Time: 102.6811 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #450: GFLOPs: 326.9248. Time: 102.6616 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #451: GFLOPs: 326.8947. Time: 102.6711 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #452: GFLOPs: 320.0351. Time: 104.8717 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #453: GFLOPs: 319.9938. Time: 104.8852 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #454: GFLOPs: 320.0618. Time: 104.8630 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #455: GFLOPs: 320.0226. Time: 104.8758 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #456: GFLOPs: 253.5220. Time: 132.3854 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #457: GFLOPs: 320.0597. Time: 104.8636 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #458: GFLOPs: 320.0761. Time: 104.8583 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #459: GFLOPs: 253.5186. Time: 132.3872 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #460: GFLOPs: 253.4927. Time: 132.4008 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #461: GFLOPs: 253.5077. Time: 132.3929 us. Best GFLOPs: 326.9269
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #462: GFLOPs: 327.7746. Time: 102.3954 us. Best GFLOPs: 327.7746
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #463: GFLOPs: 324.4672. Time: 103.4392 us. Best GFLOPs: 327.7746
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #464: GFLOPs: 327.7769. Time: 102.3947 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #465: GFLOPs: 324.5128. Time: 103.4247 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #466: GFLOPs: 327.6134. Time: 102.4458 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #467: GFLOPs: 324.5232. Time: 103.4213 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #468: GFLOPs: 324.4690. Time: 103.4386 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #469: GFLOPs: 317.8583. Time: 105.5899 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #470: GFLOPs: 324.4880. Time: 103.4326 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #471: GFLOPs: 324.7077. Time: 103.3626 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #472: GFLOPs: 324.6730. Time: 103.3736 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #473: GFLOPs: 324.2759. Time: 103.5002 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #474: GFLOPs: 324.2626. Time: 103.5045 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #475: GFLOPs: 326.8815. Time: 102.6752 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #476: GFLOPs: 324.2385. Time: 103.5121 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #477: GFLOPs: 324.2456. Time: 103.5099 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #478: GFLOPs: 199.7848. Time: 167.9938 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #479: GFLOPs: 317.8879. Time: 105.5801 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #480: GFLOPs: 317.8494. Time: 105.5929 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #481: GFLOPs: 317.8482. Time: 105.5933 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #482: GFLOPs: 324.2560. Time: 103.5066 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #483: GFLOPs: 326.8773. Time: 102.6765 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #484: GFLOPs: 326.8975. Time: 102.6702 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #485: GFLOPs: 326.9003. Time: 102.6693 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #486: GFLOPs: 324.6896. Time: 103.3683 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #487: GFLOPs: 324.2509. Time: 103.5082 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #488: GFLOPs: 317.9062. Time: 105.5740 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #489: GFLOPs: 317.9082. Time: 105.5733 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #490: GFLOPs: 317.8725. Time: 105.5852 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #491: GFLOPs: 321.1467. Time: 104.5087 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #492: GFLOPs: 317.9245. Time: 105.5679 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #493: GFLOPs: 327.6057. Time: 102.4482 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #494: GFLOPs: 326.9185. Time: 102.6636 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #495: GFLOPs: 324.2270. Time: 103.5158 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #496: GFLOPs: 317.9855. Time: 105.5477 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #497: GFLOPs: 316.8662. Time: 105.9205 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #498: GFLOPs: 317.9011. Time: 105.5757 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #499: GFLOPs: 317.9171. Time: 105.5704 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #500: GFLOPs: 317.8745. Time: 105.5845 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #501: GFLOPs: 199.7289. Time: 168.0409 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #502: GFLOPs: 326.9072. Time: 102.6671 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #503: GFLOPs: 317.9102. Time: 105.5726 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #504: GFLOPs: 316.9040. Time: 105.9079 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #505: GFLOPs: 320.7886. Time: 104.6254 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #506: GFLOPs: 320.7971. Time: 104.6226 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #507: GFLOPs: 318.0031. Time: 105.5418 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #508: GFLOPs: 320.7434. Time: 104.6401 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #509: GFLOPs: 89.5809. Time: 374.6625 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #510: GFLOPs: 22.4250. Time: 1496.6600 us. Best GFLOPs: 327.7769
2024-04-30 07:31:50 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #511: GFLOPs: 9.9950. Time: 3357.9348 us. Best GFLOPs: 327.7769
2024-04-30 08:15:13 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 08:15:14 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 08:15:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1219 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1624 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2030 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2435 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2838 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3244 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3646 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 4053 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 4458 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:20 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-04-30 08:15:22 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 180 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:24 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 165 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 159 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:27 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 08:15:28 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1312  1.1269  1.1266  1.1266  1.0949  1.0913  1.0913  1.0483  1.0337  1.0337  1.0092  1.0031  1.0026  1.0018  1.0018  0.9970
[17 : 32]:	0.9903  0.9898  0.9882  0.9882  0.9867  0.9860  0.9834  0.9816  0.9816  0.9816  0.9816  0.9808  0.9807  0.9756  0.9740  0.9740
[33 : 48]:	0.9727  0.9724  0.9697  0.9697  0.9697  0.9697  0.9696  0.9696  0.9695  0.9694  0.9692  0.9682  0.9681  0.9680  0.9680  0.9658
[49 : 64]:	0.9658  0.9646  0.9646  0.9646  0.9642  0.9642  0.9642  0.9633  0.9633  0.9626  0.9623  0.9623  0.9623  0.9623  0.9620  0.9620
2024-04-30 08:15:28 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 08:15:28 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #512: GFLOPs: 295.8707. Time: 113.4368 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #513: GFLOPs: 299.7148. Time: 111.9819 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #514: GFLOPs: 296.1137. Time: 113.3437 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #515: GFLOPs: 296.1466. Time: 113.3311 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #516: GFLOPs: 253.5671. Time: 132.3619 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #517: GFLOPs: 253.5361. Time: 132.3781 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #518: GFLOPs: 253.5482. Time: 132.3718 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #519: GFLOPs: 296.1466. Time: 113.3311 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #520: GFLOPs: 299.6999. Time: 111.9874 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #521: GFLOPs: 299.7358. Time: 111.9740 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #522: GFLOPs: 296.6069. Time: 113.1553 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #523: GFLOPs: 327.7537. Time: 102.4020 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #524: GFLOPs: 253.5202. Time: 132.3864 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #525: GFLOPs: 300.1239. Time: 111.8292 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #526: GFLOPs: 300.1342. Time: 111.8254 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #527: GFLOPs: 253.5418. Time: 132.3751 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #528: GFLOPs: 326.9017. Time: 102.6689 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #529: GFLOPs: 327.5598. Time: 102.4626 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #530: GFLOPs: 324.6580. Time: 103.3784 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #531: GFLOPs: 324.6463. Time: 103.3821 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #532: GFLOPs: 324.6824. Time: 103.3706 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #533: GFLOPs: 321.1872. Time: 104.4955 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #534: GFLOPs: 324.6431. Time: 103.3831 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #535: GFLOPs: 324.6849. Time: 103.3698 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #536: GFLOPs: 324.7176. Time: 103.3594 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #537: GFLOPs: 324.6522. Time: 103.3803 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #538: GFLOPs: 324.6589. Time: 103.3781 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #539: GFLOPs: 321.1296. Time: 104.5143 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #540: GFLOPs: 176.4061. Time: 190.2577 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #541: GFLOPs: 317.8929. Time: 105.5784 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #542: GFLOPs: 317.9241. Time: 105.5680 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #543: GFLOPs: 317.9184. Time: 105.5699 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #544: GFLOPs: 320.8160. Time: 104.6164 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #545: GFLOPs: 320.7603. Time: 104.6346 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #546: GFLOPs: 320.7783. Time: 104.6287 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #547: GFLOPs: 320.7578. Time: 104.6354 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #548: GFLOPs: 320.7724. Time: 104.6306 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #549: GFLOPs: 320.7986. Time: 104.6221 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #550: GFLOPs: 317.6625. Time: 105.6550 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #551: GFLOPs: 316.9046. Time: 105.9077 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #552: GFLOPs: 317.9929. Time: 105.5452 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #553: GFLOPs: 320.7537. Time: 104.6367 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #554: GFLOPs: 317.9374. Time: 105.5636 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #555: GFLOPs: 317.7532. Time: 105.6248 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #556: GFLOPs: 317.9861. Time: 105.5475 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #557: GFLOPs: 316.8803. Time: 105.9158 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #558: GFLOPs: 316.9023. Time: 105.9084 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #559: GFLOPs: 317.7334. Time: 105.6314 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #560: GFLOPs: 316.8878. Time: 105.9133 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #561: GFLOPs: 318.0358. Time: 105.5310 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #562: GFLOPs: 318.0091. Time: 105.5398 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #563: GFLOPs: 318.0608. Time: 105.5227 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #564: GFLOPs: 324.6881. Time: 103.3688 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #565: GFLOPs: 324.7000. Time: 103.3650 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #566: GFLOPs: 324.6289. Time: 103.3877 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #567: GFLOPs: 317.7334. Time: 105.6314 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #568: GFLOPs: 316.8415. Time: 105.9288 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #569: GFLOPs: 320.0863. Time: 104.8549 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #570: GFLOPs: 320.0884. Time: 104.8542 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #571: GFLOPs: 320.0740. Time: 104.8589 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #572: GFLOPs: 320.0884. Time: 104.8542 us. Best GFLOPs: 327.7769
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #573: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(512) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(2048) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(2048) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 512, 4, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 512, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 512, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #574: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(32) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(2048)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(256)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4096), (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(16), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(32) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(32)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_1_i1_1_fused * T.int64(1024) + i0_2_i1_2_fused * T.int64(32) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 32, 16, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70 = sch.split(loop=l68, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2024-04-30 08:16:53 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #575: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  313: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  312: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  311: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  310: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  309: tvm::transform::Pass::operator()(tvm::IRModule) const
  308: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  307: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  306: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  305: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  304: _ZN3tvm7runtime13PackedFun
  303: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  302: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  301: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  300: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  299: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  298: _ZZN3tvm3tir11StmtFunctorI
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorI
  294: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  293: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  292: _ZZN3tvm3tir11StmtFunctorI
  291: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  290: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  289: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  288: _ZZN3tvm3tir11StmtFunctorI
  287: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  286: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  285: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  284: _ZZN3tvm3tir11StmtFunctorI
  283: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  282: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  281: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  280: _ZZN3tvm3tir11StmtFunctorI
  279: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  278: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  277: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  276: _ZZN3tvm3tir11StmtFunctorI
  275: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  274: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  273: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  272: _ZZN3tvm3tir11StmtFunctorI
  271: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  270: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  269: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  268: _ZZN3tvm3tir11StmtFunctorI
  267: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  266: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  265: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  264: _ZZN3tvm3tir11StmtFunctorI
  263: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  262: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  261: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  260: _ZZN3tvm3tir11StmtFunctorI
  259: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  258: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  257: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  256: _ZZN3tvm3tir11StmtFunctorI
  255: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  254: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  253: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  252: _ZZN3tvm3tir11StmtFunctorI
  251: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  250: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  249: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  248: _ZZN3tvm3tir11StmtFunctorI
  247: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  246: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  245: _ZZN3tvm3tir11StmtFunctorI
  244: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  243: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  242: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  241: _ZZN3tvm3tir11StmtFunctorI
  240: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  239: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  238: _ZZN3tvm3tir11StmtFunctorI
  237: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  236: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  235: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  234: _ZZN3tvm3tir11StmtFunctorI
  233: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  232: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  231: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  230: _ZZN3tvm3tir11StmtFunctorI
  229: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  228: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  227: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  226: _ZZN3tvm3tir11StmtFunctorI
  225: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  224: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  223: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  222: _ZZN3tvm3tir11StmtFunctorI
  221: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  220: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  219: _ZZN3tvm3tir11StmtFunctorI
  218: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  217: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  216: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  215: _ZZN3tvm3tir11StmtFunctorI
  214: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  213: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  212: _ZZN3tvm3tir11StmtFunctorI
  211: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  210: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  209: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  208: _ZZN3tvm3tir11StmtFunctorI
  207: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  206: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  205: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  204: _ZZN3tvm3tir11StmtFunctorI
  203: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  200: _ZZN3tvm3tir11StmtFunctorI
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  197: _ZZN3tvm3tir11StmtFunctorI
  196: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  195: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  194: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  193: _ZZN3tvm3tir11StmtFunctorI
  192: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  191: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  190: _ZZN3tvm3tir11StmtFunctorI
  189: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  188: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  187: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  186: _ZZN3tvm3tir11StmtFunctorI
  185: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  182: _ZZN3tvm3tir11StmtFunctorI
  181: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  180: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  179: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  178: _ZZN3tvm3tir11StmtFunctorI
  177: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  176: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  175: _ZZN3tvm3tir11StmtFunctorI
  174: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorI
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorI
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorI
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorI
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorI
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorI
  155: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  154: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  153: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorI
  151: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  150: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  149: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  148: _ZZN3tvm3tir11StmtFunctorI
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorI
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorI
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorI
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorI
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS
  132: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  131: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  130: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  129: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  128: _ZZN3tvm3tir11StmtFunctorI
  127: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  126: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  125: _ZZN3tvm3tir11StmtFunctorI
  124: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  123: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  122: _ZZN3tvm3tir11StmtFunctorI
  121: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  120: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  119: _ZZN3tvm3tir11StmtFunctorI
  118: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  117: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  116: _ZZN3tvm3tir11StmtFunctorI
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorI
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorI
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorIFNS
  106: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  105: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  104: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  103: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  102: _ZZN3tvm3tir11StmtFunctorI
  101: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  100: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  99: _ZZN3tvm3tir11StmtFunctorI
  98: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  97: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  96: _ZZN3tvm3tir11StmtFunctorI
  95: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  94: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  93: _ZZN3tvm3tir11StmtFunctorI
  92: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  91: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  90: _ZZN3tvm3tir11StmtFunctorI
  89: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  88: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  87: _ZZN3tvm3tir11StmtFunctorI
  86: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  85: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  84: _ZZN3tvm3tir11StmtFunctorI
  83: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  82: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  81: _ZZN3tvm3tir11StmtFunctorI
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorIFNS
  77: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  76: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  75: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  74: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  73: _ZZN3tvm3tir11StmtFunctorI
  72: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  71: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  70: _ZZN3tvm3tir11StmtFunctorI
  69: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  68: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  67: _ZZN3tvm3tir11StmtFunctorI
  66: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  65: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  64: _ZZN3tvm3tir11StmtFunctorI
  63: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  62: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  61: _ZZN3tvm3tir11StmtFunctorI
  60: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  59: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  58: _ZZN3tvm3tir11StmtFunctorI
  57: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  56: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  55: _ZZN3tvm3tir11StmtFunctorI
  54: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  53: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  52: _ZZN3tvm3tir11StmtFunctorI
  51: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  50: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  49: _ZZN3tvm3tir11StmtFunctorIFNS
  48: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  47: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  45: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  44: _ZZN3tvm3tir11StmtFunctorI
  43: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  42: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorI
  40: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  39: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  38: _ZZN3tvm3tir11StmtFunctorI
  37: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  36: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  35: _ZZN3tvm3tir11StmtFunctorI
  34: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  33: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  32: _ZZN3tvm3tir11StmtFunctorI
  31: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  30: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  29: _ZZN3tvm3tir11StmtFunctorI
  28: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  27: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  26: _ZZN3tvm3tir11StmtFunctorI
  25: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  24: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  23: _ZZN3tvm3tir11StmtFunctorIFNS
  22: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  21: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  20: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  19: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  18: _ZZN3tvm3tir11StmtFunctorI
  17: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  16: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorI
  13: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  12: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  11: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  10: _ZZN3tvm3tir11StmtFunctorI
  9: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  8: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  7: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  6: _ZZN3tvm3tir11StmtFunctorIFNS
  5: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  4: _ZZN3tvm3tir11ExprFunctorI
  3: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  2: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  1: tvm::tir::APIType(tvm::runtime::DataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(2048), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(4)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(512) + i0_1_i1_1_fused * T.int64(256) + i0_2_i1_2_fused * T.int64(4) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 64, 4, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 1, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 09:01:48 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 09:01:48 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 09:01:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2022 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2428 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2833 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3239 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3645 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 4048 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:54 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-04-30 09:01:55 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 192 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:57 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 175 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:01:59 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 183 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:02:01 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:02:01 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1466  1.1258  1.1184  1.1025  1.0970  1.0647  1.0493  1.0441  1.0387  1.0148  1.0068  1.0061  1.0041  0.9997  0.9993  0.9858
[17 : 32]:	0.9843  0.9808  0.9783  0.9748  0.9746  0.9731  0.9716  0.9716  0.9706  0.9674  0.9633  0.9597  0.9577  0.9534  0.9472  0.9472
[33 : 48]:	0.9369  0.9367  0.9354  0.9354  0.9351  0.9304  0.9304  0.9301  0.9208  0.9208  0.9128  0.9128  0.9128  0.9099  0.9099  0.9099
[49 : 64]:	0.9095  0.9095  0.9045  0.9034  0.8998  0.8977  0.8971  0.8971  0.8960  0.8960  0.8959  0.8956  0.8946  0.8944  0.8944  0.8944
2024-04-30 09:02:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 09:02:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #576: GFLOPs: 288.8215. Time: 116.2054 us. Best GFLOPs: 327.7769
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #577: GFLOPs: 198.0044. Time: 169.5044 us. Best GFLOPs: 327.7769
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #578: GFLOPs: 198.0006. Time: 169.5076 us. Best GFLOPs: 327.7769
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #579: GFLOPs: 213.2615. Time: 157.3778 us. Best GFLOPs: 327.7769
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #580: GFLOPs: 198.0643. Time: 169.4532 us. Best GFLOPs: 327.7769
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #581: GFLOPs: 350.2436. Time: 95.8265 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #582: GFLOPs: 212.6736. Time: 157.8128 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #583: GFLOPs: 287.0236. Time: 116.9333 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #584: GFLOPs: 198.2546. Time: 169.2905 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #585: GFLOPs: 213.2560. Time: 157.3819 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #586: GFLOPs: 159.2568. Time: 210.7454 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #587: GFLOPs: 213.3001. Time: 157.3493 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #588: GFLOPs: 213.3473. Time: 157.3145 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #589: GFLOPs: 213.2820. Time: 157.3627 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #590: GFLOPs: 213.3289. Time: 157.3281 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #591: GFLOPs: 349.7910. Time: 95.9505 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #592: GFLOPs: 349.8404. Time: 95.9370 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #593: GFLOPs: 349.8121. Time: 95.9447 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #594: GFLOPs: 349.8873. Time: 95.9241 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #595: GFLOPs: 159.2415. Time: 210.7655 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #596: GFLOPs: 317.8464. Time: 105.5938 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #597: GFLOPs: 317.9464. Time: 105.5606 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #598: GFLOPs: 319.7260. Time: 104.9731 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #599: GFLOPs: 319.8303. Time: 104.9388 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #600: GFLOPs: 133.0242. Time: 252.3046 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #601: GFLOPs: 317.9212. Time: 105.5690 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #602: GFLOPs: 213.1460. Time: 157.4631 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #603: GFLOPs: 132.9617. Time: 252.4232 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #604: GFLOPs: 132.9676. Time: 252.4120 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #605: GFLOPs: 213.1454. Time: 157.4636 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #606: GFLOPs: 299.6943. Time: 111.9895 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #607: GFLOPs: 299.6519. Time: 112.0054 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #608: GFLOPs: 324.5852. Time: 103.4016 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #609: GFLOPs: 213.2680. Time: 157.3730 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #610: GFLOPs: 324.5947. Time: 103.3986 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #611: GFLOPs: 327.0115. Time: 102.6344 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #612: GFLOPs: 324.6112. Time: 103.3933 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #613: GFLOPs: 299.6708. Time: 111.9983 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #614: GFLOPs: 299.6938. Time: 111.9897 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #615: GFLOPs: 213.2403. Time: 157.3934 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #616: GFLOPs: 296.1050. Time: 113.3470 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #617: GFLOPs: 296.0615. Time: 113.3637 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #618: GFLOPs: 294.2621. Time: 114.0569 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #619: GFLOPs: 294.2808. Time: 114.0496 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #620: GFLOPs: 294.2190. Time: 114.0736 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #621: GFLOPs: 291.8242. Time: 115.0097 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #622: GFLOPs: 291.8605. Time: 114.9954 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #623: GFLOPs: 291.8223. Time: 115.0105 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #624: GFLOPs: 300.2379. Time: 111.7868 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #625: GFLOPs: 300.1764. Time: 111.8097 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #626: GFLOPs: 295.6117. Time: 113.5362 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #627: GFLOPs: 295.5947. Time: 113.5427 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #628: GFLOPs: 317.9387. Time: 105.5632 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #629: GFLOPs: 146.6278. Time: 228.8968 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #630: GFLOPs: 299.2451. Time: 112.1577 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #631: GFLOPs: 299.2546. Time: 112.1541 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #632: GFLOPs: 299.7037. Time: 111.9860 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #633: GFLOPs: 299.2794. Time: 112.1448 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #634: GFLOPs: 294.2636. Time: 114.0563 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #635: GFLOPs: 293.6361. Time: 114.3001 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #636: GFLOPs: 294.3204. Time: 114.0343 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #637: GFLOPs: 26.4706. Time: 1267.9194 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #638: GFLOPs: 39.5352. Time: 848.9307 us. Best GFLOPs: 350.2436
2024-04-30 09:03:33 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #639: GFLOPs: 42.9317. Time: 781.7680 us. Best GFLOPs: 350.2436
2024-04-30 09:59:23 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 09:59:23 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-30 09:59:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 1614 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2020 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2425 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 2832 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3232 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 3636 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:28 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2024-04-30 09:59:30 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 146 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:32 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 174 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:33 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:35 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0xa7d5308)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x860e168)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x860eba8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x478ad58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x44b36a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xc94a0e8)]: 172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x860eb78)]: 0 failure(s)
2024-04-30 09:59:36 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0670  1.0355  1.0355  1.0231  1.0187  1.0140  0.9907  0.9818  0.9751  0.9732  0.9568  0.9459  0.9399  0.9164  0.9164  0.9088
[17 : 32]:	0.9088  0.9086  0.9086  0.9086  0.9081  0.9057  0.9048  0.8964  0.8786  0.8743  0.8640  0.8631  0.8631  0.8623  0.8621  0.8621
[33 : 48]:	0.8481  0.8477  0.8436  0.8436  0.8431  0.8425  0.8425  0.8425  0.8396  0.8392  0.8390  0.8389  0.8389  0.8389  0.8387  0.8387
[49 : 64]:	0.8387  0.8387  0.8378  0.8378  0.8378  0.8376  0.8376  0.8350  0.8350  0.8350  0.8348  0.8348  0.8348  0.8346  0.8346  0.8344
2024-04-30 09:59:36 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 09:59:36 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #640: GFLOPs: 356.3975. Time: 94.1719 us. Best GFLOPs: 356.3975
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #641: GFLOPs: 351.6926. Time: 95.4317 us. Best GFLOPs: 356.3975
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #642: GFLOPs: 351.5468. Time: 95.4713 us. Best GFLOPs: 356.3975
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #643: GFLOPs: 351.5752. Time: 95.4636 us. Best GFLOPs: 356.3975
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #644: GFLOPs: 351.6496. Time: 95.4434 us. Best GFLOPs: 356.3975
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #645: GFLOPs: 357.6792. Time: 93.8344 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #646: GFLOPs: 350.8681. Time: 95.6560 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #647: GFLOPs: 239.0514. Time: 140.3992 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #648: GFLOPs: 350.8352. Time: 95.6649 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #649: GFLOPs: 351.0719. Time: 95.6004 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #650: GFLOPs: 318.7995. Time: 105.2781 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #651: GFLOPs: 323.1295. Time: 103.8674 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #652: GFLOPs: 318.7861. Time: 105.2826 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #653: GFLOPs: 318.7371. Time: 105.2988 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #654: GFLOPs: 318.7975. Time: 105.2788 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #655: GFLOPs: 317.9516. Time: 105.5589 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #656: GFLOPs: 319.9941. Time: 104.8851 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #657: GFLOPs: 317.9120. Time: 105.5720 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #658: GFLOPs: 317.9090. Time: 105.5731 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #659: GFLOPs: 317.8999. Time: 105.5761 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #660: GFLOPs: 292.2845. Time: 114.8286 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #661: GFLOPs: 318.7776. Time: 105.2854 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #662: GFLOPs: 292.2096. Time: 114.8580 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #663: GFLOPs: 318.7205. Time: 105.3043 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #664: GFLOPs: 198.7940. Time: 168.8312 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #665: GFLOPs: 198.7990. Time: 168.8269 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #666: GFLOPs: 300.0326. Time: 111.8632 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #667: GFLOPs: 300.0288. Time: 111.8647 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #668: GFLOPs: 300.0575. Time: 111.8540 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #669: GFLOPs: 300.0169. Time: 111.8691 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #670: GFLOPs: 300.0646. Time: 111.8513 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #671: GFLOPs: 300.0403. Time: 111.8604 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #672: GFLOPs: 300.0704. Time: 111.8492 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #673: GFLOPs: 296.4727. Time: 113.2065 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #674: GFLOPs: 300.0806. Time: 111.8454 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #675: GFLOPs: 300.0857. Time: 111.8435 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #676: GFLOPs: 273.2780. Time: 122.8150 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #677: GFLOPs: 299.6441. Time: 112.0083 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #678: GFLOPs: 299.6289. Time: 112.0140 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #679: GFLOPs: 299.6329. Time: 112.0125 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #680: GFLOPs: 299.6383. Time: 112.0104 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #681: GFLOPs: 299.6345. Time: 112.0119 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #682: GFLOPs: 299.6348. Time: 112.0118 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #683: GFLOPs: 292.8821. Time: 114.5943 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #684: GFLOPs: 293.5816. Time: 114.3213 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #685: GFLOPs: 292.8285. Time: 114.6153 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #686: GFLOPs: 292.8473. Time: 114.6079 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #687: GFLOPs: 292.8755. Time: 114.5969 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #688: GFLOPs: 292.8741. Time: 114.5974 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #689: GFLOPs: 292.8623. Time: 114.6021 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #690: GFLOPs: 300.5376. Time: 111.6753 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #691: GFLOPs: 300.5331. Time: 111.6770 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #692: GFLOPs: 300.5244. Time: 111.6802 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #693: GFLOPs: 300.5452. Time: 111.6725 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #694: GFLOPs: 300.5652. Time: 111.6650 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #695: GFLOPs: 292.8933. Time: 114.5899 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #696: GFLOPs: 293.6227. Time: 114.3053 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #697: GFLOPs: 292.8241. Time: 114.6170 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #698: GFLOPs: 292.8559. Time: 114.6046 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #699: GFLOPs: 292.9014. Time: 114.5868 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #700: GFLOPs: 292.8578. Time: 114.6038 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:131] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #701: GFLOPs: 58.1482. Time: 577.1910 us. Best GFLOPs: 357.6792
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #702: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3_init + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(4096), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(32) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(4096), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + i1_3 + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(256) + i0_1_i1_1_fused * T.int64(32) + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 8, 32, 1, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[4096, 1, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
l55 = sch.fuse(l30, preserve_unit_iters=True)
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l55, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v56 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v56)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l57, l58, l59, l60, l61 = sch.get_loops(block=b37)
l62, l63 = sch.split(loop=l61, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l63, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b73)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b74)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b76)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2024-04-30 10:01:05 [INFO] [task_scheduler.cc:121] [Task #17: fused_nn_dense_add_nn_relu_1] Trial #703: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(4096)), "float32"), p1: T.Buffer((T.int64(4096), T.int64(4096)), "float32"), p2: T.Buffer((T.int64(1), T.int64(4096)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(4096)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(4096)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4096), T.int64(4096)), scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i0 = T.axis.spatial(T.int64(1), i0_3_init + i0_4_init)
                            v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3_init * T.int64(2) + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i0, v_i1])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i0, v_i1] = T.float32(0)
                    for k_0 in range(T.int64(512)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(128) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(64) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(4096), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(256) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i0_3, i1_3, k_2, i0_4, i1_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i0 = T.axis.spatial(T.int64(1), i0_3 + i0_4)
                                v_i1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + i1_3 * T.int64(2) + i1_4)
                                v_k = T.axis.reduce(T.int64(4096), k_0 * T.int64(8) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i0, v_i1], p0_shared[v_i0, v_k], p1_shared[v_i1, v_k])
                                T.writes(T_matmul_NT_local[v_i0, v_i1])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i0, v_i1] = T_matmul_NT_local[v_i0, v_i1] + p0_shared[v_i0, v_k] * p1_shared[v_i1, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(4096), i0_0_i1_0_fused * T.int64(1024) + i0_1_i1_1_fused * T.int64(128) + i0_2_i1_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + p2[v0, v1], T.float32(0))
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11], preserve_unit_iters=True)
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 8, 64, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21], preserve_unit_iters=True)
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[512, 4, 2])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29], preserve_unit_iters=True)
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24, preserve_unit_iters=True)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True, index=-1)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True, index=-1)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43, preserve_unit_iters=True)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True, index=-1)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52, preserve_unit_iters=True)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
