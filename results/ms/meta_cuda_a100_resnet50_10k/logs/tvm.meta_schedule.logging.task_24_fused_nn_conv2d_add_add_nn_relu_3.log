2024-04-29 17:13:46 [INFO] [task_scheduler.cc:160] Initializing Task #24: "fused_nn_conv2d_add_add_nn_relu_3"
2024-04-29 17:13:46 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)))
        conv2d_nchw = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)))
        T_add_1 = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(7), T.int64(7)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(2048), T.int64(7), T.int64(7), T.int64(512), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1[v_ff, v_rc, v_ry, v_rx])
                T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2048), T.int64(7), T.int64(7)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2048), T.int64(7), T.int64(7)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3], p3[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add_1[v_ax0, v_ax1, v_ax2, v_ax3] = T_add[v_ax0, v_ax1, v_ax2, v_ax3] + p3[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2048), T.int64(7), T.int64(7)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2024-04-29 17:13:46 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-29 17:13:46 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(196), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                        for rc_0, ry_0, rx_0 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(25088)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), ax0_ax1_ax2_ax3_fused % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), ax0_ax1_ax2_ax3_fused % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(524288)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + ax0_ax1_ax2_ax3_fused // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(512))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(16) + ff_3 + ff_4)
                                    v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(512) + rc_1 * T.int64(4) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(16) + ax1)
                                v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + ax2)
                                v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 16, 16, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 128, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
2024-04-29 17:13:46 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(196), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(1), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(25088)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), ax0_ax1_ax2_ax3_fused % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), ax0_ax1_ax2_ax3_fused % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(524288)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + ax0_ax1_ax2_ax3_fused // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(512))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(16) + ff_3 + ff_4)
                                    v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(512), rc_1 * T.int64(4) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(16) + ax1)
                                v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + ax2)
                                v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 16, 16, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 128, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
2024-04-29 17:13:46 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(196), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                        for rc_0_ry_0_rx_0_fused in T.serial(T.int64(1), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(25088)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), ax0_ax1_ax2_ax3_fused % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), ax0_ax1_ax2_ax3_fused % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(524288)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + ax0_ax1_ax2_ax3_fused // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_ax3_fused % T.int64(512))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(16) + ff_3 + ff_4)
                                    v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                    v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                    v_rc = T.axis.reduce(T.int64(512), rc_1 * T.int64(4) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                    v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(16) + ax1)
                                v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + ax2)
                                v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 16, 16, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 128, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
2024-04-29 17:59:24 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 17:59:24 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-29 17:59:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 486 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 17:59:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 978 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 17:59:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1464 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 17:59:31 [INFO] [evolutionary_search.cc:723] Sampled 72 candidate(s)
2024-04-29 17:59:35 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 17:59:38 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 98 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 17:59:42 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 88 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 17:59:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 84 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 17:59:46 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9997  0.9988  0.9986  0.9974  0.9967  0.9962  0.9952  0.9946  0.9941  0.9925  0.9923  0.9897  0.9887  0.9880  0.9879  0.9879
[17 : 32]:	0.9872  0.9852  0.9841  0.9838  0.9816  0.9813  0.9808  0.9787  0.9786  0.9775  0.9773  0.9740  0.9727  0.9725  0.9722  0.9717
[33 : 48]:	0.9716  0.9696  0.9688  0.9685  0.9681  0.9681  0.9674  0.9655  0.9652  0.9646  0.9634  0.9633  0.9624  0.9614  0.9605  0.9602
[49 : 64]:	0.9596  0.9594  0.9567  0.9542  0.9533  0.9531  0.9511  0.9510  0.9504  0.9491  0.9483  0.9467  0.9465  0.9460  0.9454  0.9446
2024-04-29 17:59:46 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 17:59:46 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #1: GFLOPs: 1748.0017. Time: 58.9596 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #2: GFLOPs: 97.4974. Time: 1057.0698 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #3: GFLOPs: 457.7870. Time: 225.1298 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #4: GFLOPs: 185.8339. Time: 554.5892 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #5: GFLOPs: 734.5156. Time: 140.3122 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #6: GFLOPs: 779.4326. Time: 132.2263 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #7: GFLOPs: 42.1853. Time: 2443.0642 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #8: GFLOPs: 48.2237. Time: 2137.1533 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #9: GFLOPs: 534.2219. Time: 192.9189 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #10: GFLOPs: 1020.9143. Time: 100.9502 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #11: GFLOPs: 536.9945. Time: 191.9228 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #12: GFLOPs: 191.0396. Time: 539.4773 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #13: GFLOPs: 688.9860. Time: 149.5843 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #14: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(392), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 8, 32, 4, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #15: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(28), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(512) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 128, 2, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119 = sch.split(loop=l117, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b124)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #16: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(7), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused)
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(14))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(7), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused + ax2)
                            v3 = T.axis.spatial(T.int64(7), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 32, 32, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119 = sch.split(loop=l117, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b124)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #17: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(32) + rc_1 * T.int64(32) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[32, 2, 32, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[16, 1, 32])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #18: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(392))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 64, 1, 8])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120 = sch.split(loop=l118, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #19: GFLOPs: 72.0315. Time: 1430.7840 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #20: GFLOPs: 142.7585. Time: 721.9292 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #21: GFLOPs: 320.9147. Time: 321.1492 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #22: GFLOPs: 15.3279. Time: 6723.7889 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #23: GFLOPs: 439.4180. Time: 234.5409 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #24: GFLOPs: 1246.4081. Time: 82.6868 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #25: GFLOPs: 1499.6147. Time: 68.7253 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #26: GFLOPs: 436.4922. Time: 236.1131 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #27: GFLOPs: 25.4717. Time: 4046.1108 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #28: GFLOPs: 315.5706. Time: 326.5877 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #29: GFLOPs: 800.3399. Time: 128.7722 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #30: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(392), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 8, 32, 8, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #31: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(448), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(448), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1792) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1792) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1792) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(784))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(448), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1792) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1792) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(8192))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(7), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(16) + rc_1 * T.int64(16) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 64, 8, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[32, 1, 16])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 448, 4], preserve_unit_iters=True)
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 448, 4], preserve_unit_iters=True)
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b123)
l136, l137, l138, l139, l140, l141, l142, l143, l144 = sch.get_loops(block=b124)
l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l145, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l145, ann_key="pragma_unroll_explicit", ann_val=1)
l165, l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b126)
b172 = sch.get_block(name="conv2d_nchw", func_name="main")
l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192 = sch.get_loops(block=b172)
b193 = sch.decompose_reduction(block=b172, loop=l176)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #32: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(7))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(224))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(32) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 4, 32, 2, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[16, 16, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b123)
l136, l137, l138, l139, l140, l141, l142, l143, l144 = sch.get_loops(block=b124)
l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164 = sch.get_loops(block=b125)
l165, l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b126)
b172 = sch.get_block(name="conv2d_nchw", func_name="main")
l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192 = sch.get_loops(block=b172)
b193 = sch.decompose_reduction(block=b172, loop=l176)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #33: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1568), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 32, 32, 2, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119 = sch.split(loop=l117, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b124)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #34: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(256)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(32) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 4, 32, 2, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[16, 16, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119 = sch.split(loop=l117, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b123)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b124)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #35: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(98), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(98))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(49) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[16, 2, 32, 1, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #36: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(32) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(42)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(196), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 < T.int64(8192))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(32) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(64) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[16, 1, 4, 4, 8])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[8, 32, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119 = sch.split(loop=l117, factors=[None, 196], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b124)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #37: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(32) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(32) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[32, 1, 2, 32, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[128, 1, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109 = sch.split(loop=l107, factors=[None, 98], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b88)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 98, 2], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b119)
l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #38: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ff_3_init * T.int64(16) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(196))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ff_3 * T.int64(16) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 8, 4, 16])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[128, 4, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #39: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(7), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(224))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(32) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 64, 2, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[16, 32, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119 = sch.split(loop=l117, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b124)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #40: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(16) + ff_3_init * T.int64(16) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(16) + ff_3 * T.int64(16) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[16, 1, 8, 1, 16])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109 = sch.split(loop=l107, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b88)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b119)
l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #41: GFLOPs: 1625.4611. Time: 63.4045 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #42: GFLOPs: 650.0822. Time: 158.5361 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #43: GFLOPs: 1114.3675. Time: 92.4843 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #44: GFLOPs: 203.9307. Time: 505.3750 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #45: GFLOPs: 261.8096. Time: 393.6506 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #46: GFLOPs: 115.1297. Time: 895.1772 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #47: GFLOPs: 204.3548. Time: 504.3264 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #48: GFLOPs: 135.1598. Time: 762.5160 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #49: GFLOPs: 502.0165. Time: 205.2951 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #50: GFLOPs: 236.8286. Time: 435.1733 us. Best GFLOPs: 1748.0017
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #51: GFLOPs: 3186.6096. Time: 32.3421 us. Best GFLOPs: 3186.6096
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #52: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(16), T.int64(1), T.int64(7), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1024))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 16, 16, 4])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109 = sch.split(loop=l107, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b88)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b119)
l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #53: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(64) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[32, 2, 8, 1, 4])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[8, 32, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #54: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ff_3_init * T.int64(16) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(196))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ff_3 * T.int64(16) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 32, 2, 16])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[128, 4, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #55: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[16, 4, 32, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #56: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(7), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 128, 4, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #57: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(11)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1 < T.int64(512))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(64) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[256, 2, 1, 1, 4])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[8, 64, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 49, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115 = sch.get_loops(block=b88)
l116, l117 = sch.split(loop=l115, factors=[None, 49], preserve_unit_iters=True)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #58: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(8), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3_init * T.int64(8) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(196))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3 * T.int64(8) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(7), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(256) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 8])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[128, 4, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115 = sch.get_loops(block=b88)
l116, l117 = sch.split(loop=l115, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b119)
l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #59: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(7), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(7))
                                        v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(56))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(1024) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 256, 2, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 256, 4], preserve_unit_iters=True)
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120 = sch.split(loop=l118, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #60: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(128), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(196))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(8192))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(4) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 32, 32, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[128, 1, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119 = sch.split(loop=l117, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b124)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #61: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(7), ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1)
                                    v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 < T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(1024) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(2048) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1024))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1024, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 1024], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 1024, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #62: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(7))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(56))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1024))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[16, 2, 32, 1, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120 = sch.split(loop=l118, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #63: GFLOPs: 54.5105. Time: 1890.6711 us. Best GFLOPs: 3186.6096
2024-04-29 18:05:09 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #64: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(512), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(T.int64(512), rc_0)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 64, 4, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 18:19:09 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 18:19:09 [INFO] [evolutionary_search.cc:715] Picked top 36 candidate(s) from database
2024-04-29 18:19:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 456 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 18:19:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 914 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 18:19:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1375 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 18:19:16 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-04-29 18:19:20 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 147 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 18:19:26 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 143 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 18:19:31 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 18:19:37 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 142 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 18:19:38 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.6690  1.6582  1.6569  1.6461  1.6461  1.6284  1.6218  1.5951  1.5951  1.5861  1.4566  1.4491  1.4379  1.4326  1.4326  1.3995
[17 : 32]:	1.3970  1.3946  1.3892  1.3496  1.3457  1.3439  1.3424  1.3416  1.3412  1.3375  1.3365  1.3365  1.3317  1.3304  1.3299  1.3287
[33 : 48]:	1.3278  1.3240  1.3187  1.3155  1.3101  1.3088  1.3081  1.3075  1.3070  1.3067  1.2996  1.2969  1.2950  1.2944  1.2915  1.2915
[49 : 64]:	1.2915  1.2915  1.2901  1.2881  1.2860  1.2844  1.2833  1.2817  1.2803  1.2802  1.2742  1.2662  1.2650  1.2650  1.2650  1.2445
2024-04-29 18:19:38 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 18:19:38 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #65: GFLOPs: 2576.7528. Time: 39.9967 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #66: GFLOPs: 2709.2375. Time: 38.0408 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #67: GFLOPs: 2590.6357. Time: 39.7823 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #68: GFLOPs: 2510.3078. Time: 41.0553 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #69: GFLOPs: 2778.5013. Time: 37.0925 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #70: GFLOPs: 2664.8782. Time: 38.6740 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #71: GFLOPs: 2780.3311. Time: 37.0681 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #72: GFLOPs: 2623.4972. Time: 39.2840 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #73: GFLOPs: 2572.6189. Time: 40.0609 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #74: GFLOPs: 2672.2474. Time: 38.5674 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #75: GFLOPs: 1942.5930. Time: 53.0536 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #76: GFLOPs: 1615.5014. Time: 63.7954 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #77: GFLOPs: 1615.8596. Time: 63.7812 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #78: GFLOPs: 1906.8595. Time: 54.0478 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #79: GFLOPs: 1906.7688. Time: 54.0503 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #80: GFLOPs: 1811.4461. Time: 56.8946 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #81: GFLOPs: 1921.3555. Time: 53.6400 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #82: GFLOPs: 2433.0197. Time: 42.3595 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #83: GFLOPs: 1966.6580. Time: 52.4044 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #84: GFLOPs: 1907.8883. Time: 54.0186 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #85: GFLOPs: 2562.8545. Time: 40.2136 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #86: GFLOPs: 2009.2188. Time: 51.2943 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #87: GFLOPs: 2056.9206. Time: 50.1048 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #88: GFLOPs: 1731.6428. Time: 59.5166 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #89: GFLOPs: 1739.9282. Time: 59.2332 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #90: GFLOPs: 1775.4272. Time: 58.0488 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #91: GFLOPs: 1823.0617. Time: 56.5321 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #92: GFLOPs: 2043.9569. Time: 50.4225 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #93: GFLOPs: 2065.9779. Time: 49.8851 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #94: GFLOPs: 1817.9005. Time: 56.6926 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #95: GFLOPs: 2056.2587. Time: 50.1209 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #96: GFLOPs: 2066.8256. Time: 49.8646 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #97: GFLOPs: 925.0100. Time: 111.4166 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #98: GFLOPs: 2012.6978. Time: 51.2057 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #99: GFLOPs: 927.2379. Time: 111.1489 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #100: GFLOPs: 3074.3148. Time: 33.5234 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #101: GFLOPs: 1528.8523. Time: 67.4110 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #102: GFLOPs: 1918.1540. Time: 53.7295 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #103: GFLOPs: 1707.9425. Time: 60.3425 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #104: GFLOPs: 1462.8363. Time: 70.4532 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #105: GFLOPs: 1657.3531. Time: 62.1844 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #106: GFLOPs: 1881.9621. Time: 54.7628 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #107: GFLOPs: 2595.5498. Time: 39.7070 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #108: GFLOPs: 1981.0956. Time: 52.0225 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #109: GFLOPs: 1545.2557. Time: 66.6954 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #110: GFLOPs: 1798.2634. Time: 57.3117 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #111: GFLOPs: 1522.7066. Time: 67.6831 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #112: GFLOPs: 1547.6967. Time: 66.5902 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #113: GFLOPs: 1549.8959. Time: 66.4958 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #114: GFLOPs: 1489.8584. Time: 69.1754 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #115: GFLOPs: 1927.7455. Time: 53.4622 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #116: GFLOPs: 1548.0618. Time: 66.5745 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #117: GFLOPs: 1297.7960. Time: 79.4127 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #118: GFLOPs: 1934.9854. Time: 53.2622 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #119: GFLOPs: 875.3904. Time: 117.7320 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #120: GFLOPs: 1842.5391. Time: 55.9345 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #121: GFLOPs: 1593.9130. Time: 64.6594 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #122: GFLOPs: 1767.1122. Time: 58.3220 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #123: GFLOPs: 850.7872. Time: 121.1366 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #124: GFLOPs: 2309.9966. Time: 44.6154 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #125: GFLOPs: 1775.2218. Time: 58.0556 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #126: GFLOPs: 65.7857. Time: 1566.6240 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #127: GFLOPs: 185.7420. Time: 554.8637 us. Best GFLOPs: 3186.6096
2024-04-29 18:20:48 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #128: GFLOPs: 136.4963. Time: 755.0498 us. Best GFLOPs: 3186.6096
2024-04-29 19:00:46 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 19:00:47 [INFO] [evolutionary_search.cc:715] Picked top 100 candidate(s) from database
2024-04-29 19:00:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 398 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:00:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 793 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:00:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1189 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:00:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1582 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:00:55 [INFO] [evolutionary_search.cc:723] Sampled 66 candidate(s)
2024-04-29 19:00:59 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:01:05 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:01:10 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 136 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:01:15 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:01:17 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5551  1.5526  1.5387  1.5342  1.5181  1.5162  1.5159  1.5159  1.5158  1.5155  1.5142  1.5138  1.5039  1.5037  1.5035  1.4913
[17 : 32]:	1.4891  1.4891  1.4845  1.4809  1.4746  1.4722  1.4707  1.4692  1.4685  1.4685  1.4484  1.4456  1.4398  1.4398  1.4398  1.4367
[33 : 48]:	1.4353  1.4299  1.4288  1.4236  1.4203  1.4133  1.4108  1.4098  1.4093  1.3940  1.3898  1.3841  1.3777  1.3777  1.3766  1.3727
[49 : 64]:	1.3569  1.3558  1.3430  1.3418  1.3308  1.3297  1.3225  1.3151  1.3070  1.3070  1.3070  1.2989  1.2730  1.2576  1.2459  1.2379
2024-04-29 19:01:17 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 19:01:17 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #129: GFLOPs: 2221.3132. Time: 46.3967 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #130: GFLOPs: 2263.1043. Time: 45.5399 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #131: GFLOPs: 2265.7619. Time: 45.4865 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #132: GFLOPs: 2327.8755. Time: 44.2728 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #133: GFLOPs: 2632.4609. Time: 39.1503 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #134: GFLOPs: 2236.9540. Time: 46.0722 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #135: GFLOPs: 2665.2771. Time: 38.6682 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #136: GFLOPs: 2676.8590. Time: 38.5009 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #137: GFLOPs: 1762.9552. Time: 58.4595 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #138: GFLOPs: 1444.8136. Time: 71.3320 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #139: GFLOPs: 2229.7484. Time: 46.2211 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #140: GFLOPs: 1430.0269. Time: 72.0696 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #141: GFLOPs: 1460.0503. Time: 70.5876 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #142: GFLOPs: 1432.5230. Time: 71.9441 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #143: GFLOPs: 1446.0317. Time: 71.2720 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #144: GFLOPs: 2773.0814. Time: 37.1650 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #145: GFLOPs: 2597.7661. Time: 39.6731 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #146: GFLOPs: 2604.8984. Time: 39.5645 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #147: GFLOPs: 1310.7876. Time: 78.6256 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #148: GFLOPs: 1729.8440. Time: 59.5785 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #149: GFLOPs: 1702.2497. Time: 60.5443 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #150: GFLOPs: 2247.5995. Time: 45.8540 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #151: GFLOPs: 2757.0612. Time: 37.3809 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #152: GFLOPs: 1705.0436. Time: 60.4451 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #153: GFLOPs: 2598.3642. Time: 39.6640 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #154: GFLOPs: 2604.4438. Time: 39.5714 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #155: GFLOPs: 1414.1388. Time: 72.8793 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #156: GFLOPs: 1406.0189. Time: 73.3002 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #157: GFLOPs: 2807.3234. Time: 36.7117 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #158: GFLOPs: 2815.4653. Time: 36.6055 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #159: GFLOPs: 2438.7016. Time: 42.2608 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #160: GFLOPs: 2167.8005. Time: 47.5420 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #161: GFLOPs: 1645.2742. Time: 62.6409 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #162: GFLOPs: 1646.3421. Time: 62.6003 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #163: GFLOPs: 1315.2679. Time: 78.3578 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #164: GFLOPs: 2174.9775. Time: 47.3851 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #165: GFLOPs: 2755.2384. Time: 37.4057 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #166: GFLOPs: 1357.7049. Time: 75.9086 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #167: GFLOPs: 1424.7205. Time: 72.3381 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #168: GFLOPs: 2469.5320. Time: 41.7332 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #169: GFLOPs: 2652.9370. Time: 38.8481 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #170: GFLOPs: 1306.2414. Time: 78.8993 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #171: GFLOPs: 2510.9189. Time: 41.0453 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #172: GFLOPs: 2122.4650. Time: 48.5575 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #173: GFLOPs: 2049.7881. Time: 50.2791 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #174: GFLOPs: 2050.9280. Time: 50.2512 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #175: GFLOPs: 2161.3944. Time: 47.6829 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #176: GFLOPs: 1322.2381. Time: 77.9447 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #177: GFLOPs: 2485.8102. Time: 41.4599 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #178: GFLOPs: 1445.4558. Time: 71.3003 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #179: GFLOPs: 1424.0186. Time: 72.3737 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #180: GFLOPs: 1984.6634. Time: 51.9290 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #181: GFLOPs: 2406.9588. Time: 42.8181 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #182: GFLOPs: 1426.8698. Time: 72.2291 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #183: GFLOPs: 913.2330. Time: 112.8535 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #184: GFLOPs: 983.1284. Time: 104.8302 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #185: GFLOPs: 941.1546. Time: 109.5054 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #186: GFLOPs: 893.2527. Time: 115.3778 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #187: GFLOPs: 901.0894. Time: 114.3743 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #188: GFLOPs: 1930.3649. Time: 53.3896 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #189: GFLOPs: 1694.4385. Time: 60.8234 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #190: GFLOPs: 1119.6256. Time: 92.0500 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #191: GFLOPs: 1820.2838. Time: 56.6184 us. Best GFLOPs: 3186.6096
2024-04-29 19:02:32 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #192: GFLOPs: 16.1043. Time: 6399.6157 us. Best GFLOPs: 3186.6096
2024-04-29 19:39:58 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 19:39:59 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 19:40:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 389 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:40:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 787 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:40:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1180 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:40:05 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2024-04-29 19:40:09 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:40:14 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 98 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:40:19 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:40:24 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 19:40:26 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0817  1.0612  1.0609  1.0132  1.0106  1.0042  1.0010  0.9992  0.9987  0.9972  0.9964  0.9964  0.9961  0.9953  0.9953  0.9953
[17 : 32]:	0.9875  0.9851  0.9836  0.9753  0.9737  0.9649  0.9625  0.9574  0.9513  0.9494  0.9489  0.9485  0.9485  0.9449  0.9400  0.9383
[33 : 48]:	0.9365  0.9340  0.9301  0.9301  0.9278  0.9269  0.9248  0.9234  0.9229  0.9225  0.9219  0.9186  0.9164  0.9159  0.9148  0.9143
[49 : 64]:	0.9142  0.9126  0.9085  0.9073  0.9071  0.9064  0.9064  0.9055  0.9055  0.9048  0.9027  0.9027  0.9021  0.9018  0.9018  0.9010
2024-04-29 19:40:26 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 19:40:26 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #193: GFLOPs: 2424.3560. Time: 42.5109 us. Best GFLOPs: 3186.6096
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #194: GFLOPs: 3051.2848. Time: 33.7764 us. Best GFLOPs: 3186.6096
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #195: GFLOPs: 2826.4820. Time: 36.4628 us. Best GFLOPs: 3186.6096
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #196: GFLOPs: 1880.6035. Time: 54.8024 us. Best GFLOPs: 3186.6096
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #197: GFLOPs: 2800.9870. Time: 36.7947 us. Best GFLOPs: 3186.6096
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #198: GFLOPs: 3526.6062. Time: 29.2240 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #199: GFLOPs: 2115.9204. Time: 48.7076 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #200: GFLOPs: 2781.9353. Time: 37.0467 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #201: GFLOPs: 2562.7046. Time: 40.2159 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #202: GFLOPs: 2748.5348. Time: 37.4969 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #203: GFLOPs: 2995.1162. Time: 34.4099 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #204: GFLOPs: 2992.9308. Time: 34.4350 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #205: GFLOPs: 2677.7695. Time: 38.4878 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #206: GFLOPs: 2748.3870. Time: 37.4989 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #207: GFLOPs: 2744.3688. Time: 37.5538 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #208: GFLOPs: 2762.5932. Time: 37.3061 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #209: GFLOPs: 1747.2970. Time: 58.9834 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #210: GFLOPs: 1746.8668. Time: 58.9979 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #211: GFLOPs: 2921.9162. Time: 35.2719 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #212: GFLOPs: 1778.0065. Time: 57.9646 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #213: GFLOPs: 763.4263. Time: 134.9986 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #214: GFLOPs: 3002.8540. Time: 34.3212 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #215: GFLOPs: 3010.2416. Time: 34.2370 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #216: GFLOPs: 2983.5622. Time: 34.5431 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #217: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  404: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  403: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  402: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  401: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  400: tvm::transform::Pass::operator()(tvm::IRModule) const
  399: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  398: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  397: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  396: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  395: _ZN3tvm7runtime13PackedFun
  394: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  393: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  392: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  391: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  390: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  389: _ZZN3tvm3tir11StmtFunctorI
  388: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  387: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  386: _ZZN3tvm3tir11StmtFunctorI
  385: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  384: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  383: _ZZN3tvm3tir11StmtFunctorI
  382: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  381: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  380: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  379: _ZZN3tvm3tir11StmtFunctorI
  378: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  377: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  376: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  375: _ZZN3tvm3tir11StmtFunctorI
  374: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  373: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  372: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  371: _ZZN3tvm3tir11StmtFunctorI
  370: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  369: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  368: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  367: _ZZN3tvm3tir11StmtFunctorI
  366: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  365: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  364: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  363: _ZZN3tvm3tir11StmtFunctorI
  362: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  361: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  360: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  359: _ZZN3tvm3tir11StmtFunctorI
  358: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  357: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  356: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  355: _ZZN3tvm3tir11StmtFunctorI
  354: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  353: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  352: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  351: _ZZN3tvm3tir11StmtFunctorI
  350: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  349: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  348: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  347: _ZZN3tvm3tir11StmtFunctorI
  346: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  345: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  344: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  343: _ZZN3tvm3tir11StmtFunctorI
  342: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  341: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  340: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  339: _ZZN3tvm3tir11StmtFunctorI
  338: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  337: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  336: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  335: _ZZN3tvm3tir11StmtFunctorI
  334: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  333: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  332: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  331: _ZZN3tvm3tir11StmtFunctorI
  330: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  329: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  328: _ZZN3tvm3tir11StmtFunctorI
  327: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  326: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  325: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  324: _ZZN3tvm3tir11StmtFunctorI
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorI
  320: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  319: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  318: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  317: _ZZN3tvm3tir11StmtFunctorI
  316: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  315: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  314: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  313: _ZZN3tvm3tir11StmtFunctorI
  312: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  311: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  310: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  309: _ZZN3tvm3tir11StmtFunctorI
  308: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  307: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  306: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  305: _ZZN3tvm3tir11StmtFunctorI
  304: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  303: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  302: _ZZN3tvm3tir11StmtFunctorI
  301: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  300: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  299: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  298: _ZZN3tvm3tir11StmtFunctorI
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorI
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorI
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorI
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorI
  282: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  281: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  280: _ZZN3tvm3tir11StmtFunctorI
  279: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  278: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  277: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  276: _ZZN3tvm3tir11StmtFunctorI
  275: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  274: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  273: _ZZN3tvm3tir11StmtFunctorI
  272: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  271: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  270: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  269: _ZZN3tvm3tir11StmtFunctorI
  268: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  267: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  266: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  265: _ZZN3tvm3tir11StmtFunctorI
  264: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorI
  260: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  259: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  258: _ZZN3tvm3tir11StmtFunctorI
  257: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  256: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  255: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  254: _ZZN3tvm3tir11StmtFunctorI
  253: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  252: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  251: _ZZN3tvm3tir11StmtFunctorI
  250: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  249: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  248: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  247: _ZZN3tvm3tir11StmtFunctorI
  246: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  245: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  244: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  243: _ZZN3tvm3tir11StmtFunctorI
  242: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  241: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  240: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  239: _ZZN3tvm3tir11StmtFunctorI
  238: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  237: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  236: _ZZN3tvm3tir11StmtFunctorI
  235: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  234: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  233: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  232: _ZZN3tvm3tir11StmtFunctorI
  231: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  230: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  229: _ZZN3tvm3tir11StmtFunctorI
  228: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  227: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  226: _ZZN3tvm3tir11StmtFunctorI
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorI
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorI
  219: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  218: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  217: _ZZN3tvm3tir11StmtFunctorI
  216: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  215: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  214: _ZZN3tvm3tir11StmtFunctorI
  213: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  212: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  211: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  210: _ZZN3tvm3tir11StmtFunctorI
  209: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  208: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  207: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  206: _ZZN3tvm3tir11StmtFunctorI
  205: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  204: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorI
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  200: _ZZN3tvm3tir11StmtFunctorI
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  197: _ZZN3tvm3tir11StmtFunctorI
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorI
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorI
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorI
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS
  184: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  183: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  182: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  181: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  180: _ZZN3tvm3tir11StmtFunctorI
  179: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  178: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorI
  176: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  175: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorI
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorI
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorI
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorI
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorI
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorI
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorI
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS
  152: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  151: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  150: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  149: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  148: _ZZN3tvm3tir11StmtFunctorI
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorI
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorI
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorI
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorI
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorI
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorI
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorI
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorI
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorI
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorI
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorI
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorI
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorI
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorI
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorI
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorI
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorI
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorI
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorI
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorI
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorI
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorI
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorI
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorI
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorI
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorI
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorI
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorI
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorI
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorI
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorI
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorI
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorI
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorI
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorI
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorI
  21: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  20: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  19: _ZZN3tvm3tir11StmtFunctorIFNS
  18: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  17: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  16: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorI
  13: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  12: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  11: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  10: _ZZN3tvm3tir11StmtFunctorI
  9: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  8: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  7: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  6: _ZZN3tvm3tir11StmtFunctorIFNS
  5: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  4: _ZZN3tvm3tir11ExprFunctorI
  3: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  2: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  1: tvm::tir::APIType(tvm::runtime::DataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(98))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[32, 2, 32, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115 = sch.get_loops(block=b88)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b120)
l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b121)
l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l138, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l138, ann_key="pragma_unroll_explicit", ann_val=1)
l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
b163 = sch.get_block(name="conv2d_nchw", func_name="main")
l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b163)
b182 = sch.decompose_reduction(block=b163, loop=l167)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #218: GFLOPs: 3236.6681. Time: 31.8419 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #219: GFLOPs: 3096.0417. Time: 33.2882 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #220: GFLOPs: 3062.2366. Time: 33.6556 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #221: GFLOPs: 3079.4162. Time: 33.4679 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #222: GFLOPs: 1938.4934. Time: 53.1658 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #223: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "Objects/call.c", line 200, in PyVectorcall_Call
  File "Python/ceval.c", line 4963, in call_function
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  404: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}>(tvm::__mk_TVM22::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::allocator<char>, tvm::runtime::TVMArgs const&)
  403: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  402: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  401: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  400: tvm::transform::Pass::operator()(tvm::IRModule) const
  399: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  398: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  397: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  396: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  395: _ZN3tvm7runtime13PackedFun
  394: tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const
  393: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  392: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  391: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  390: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  389: _ZZN3tvm3tir11StmtFunctorI
  388: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  387: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  386: _ZZN3tvm3tir11StmtFunctorI
  385: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  384: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  383: _ZZN3tvm3tir11StmtFunctorI
  382: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  381: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  380: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  379: _ZZN3tvm3tir11StmtFunctorI
  378: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  377: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  376: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  375: _ZZN3tvm3tir11StmtFunctorI
  374: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  373: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  372: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  371: _ZZN3tvm3tir11StmtFunctorI
  370: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  369: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  368: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  367: _ZZN3tvm3tir11StmtFunctorI
  366: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  365: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  364: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  363: _ZZN3tvm3tir11StmtFunctorI
  362: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  361: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  360: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  359: _ZZN3tvm3tir11StmtFunctorI
  358: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  357: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  356: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  355: _ZZN3tvm3tir11StmtFunctorI
  354: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  353: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  352: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  351: _ZZN3tvm3tir11StmtFunctorI
  350: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  349: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  348: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  347: _ZZN3tvm3tir11StmtFunctorI
  346: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  345: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  344: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  343: _ZZN3tvm3tir11StmtFunctorI
  342: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  341: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  340: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  339: _ZZN3tvm3tir11StmtFunctorI
  338: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  337: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  336: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  335: _ZZN3tvm3tir11StmtFunctorI
  334: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  333: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  332: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  331: _ZZN3tvm3tir11StmtFunctorI
  330: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  329: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  328: _ZZN3tvm3tir11StmtFunctorI
  327: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  326: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  325: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  324: _ZZN3tvm3tir11StmtFunctorI
  323: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  322: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  321: _ZZN3tvm3tir11StmtFunctorI
  320: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  319: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  318: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  317: _ZZN3tvm3tir11StmtFunctorI
  316: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  315: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  314: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  313: _ZZN3tvm3tir11StmtFunctorI
  312: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  311: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  310: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  309: _ZZN3tvm3tir11StmtFunctorI
  308: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  307: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  306: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  305: _ZZN3tvm3tir11StmtFunctorI
  304: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  303: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  302: _ZZN3tvm3tir11StmtFunctorI
  301: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  300: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  299: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  298: _ZZN3tvm3tir11StmtFunctorI
  297: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  296: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  295: _ZZN3tvm3tir11StmtFunctorI
  294: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  293: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  292: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  291: _ZZN3tvm3tir11StmtFunctorI
  290: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  289: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  288: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  287: _ZZN3tvm3tir11StmtFunctorI
  286: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  285: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  284: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  283: _ZZN3tvm3tir11StmtFunctorI
  282: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  281: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  280: _ZZN3tvm3tir11StmtFunctorI
  279: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  278: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  277: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  276: _ZZN3tvm3tir11StmtFunctorI
  275: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  274: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  273: _ZZN3tvm3tir11StmtFunctorI
  272: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  271: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  270: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  269: _ZZN3tvm3tir11StmtFunctorI
  268: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  267: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  266: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  265: _ZZN3tvm3tir11StmtFunctorI
  264: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  263: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  262: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  261: _ZZN3tvm3tir11StmtFunctorI
  260: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  259: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  258: _ZZN3tvm3tir11StmtFunctorI
  257: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  256: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  255: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  254: _ZZN3tvm3tir11StmtFunctorI
  253: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  252: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  251: _ZZN3tvm3tir11StmtFunctorI
  250: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  249: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  248: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  247: _ZZN3tvm3tir11StmtFunctorI
  246: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  245: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  244: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  243: _ZZN3tvm3tir11StmtFunctorI
  242: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  241: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  240: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  239: _ZZN3tvm3tir11StmtFunctorI
  238: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  237: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  236: _ZZN3tvm3tir11StmtFunctorI
  235: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  234: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  233: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  232: _ZZN3tvm3tir11StmtFunctorI
  231: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  230: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  229: _ZZN3tvm3tir11StmtFunctorI
  228: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  227: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  226: _ZZN3tvm3tir11StmtFunctorI
  225: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  224: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  223: _ZZN3tvm3tir11StmtFunctorI
  222: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  221: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  220: _ZZN3tvm3tir11StmtFunctorI
  219: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  218: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  217: _ZZN3tvm3tir11StmtFunctorI
  216: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  215: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  214: _ZZN3tvm3tir11StmtFunctorI
  213: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  212: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  211: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  210: _ZZN3tvm3tir11StmtFunctorI
  209: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  208: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  207: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  206: _ZZN3tvm3tir11StmtFunctorI
  205: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  204: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorI
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  200: _ZZN3tvm3tir11StmtFunctorI
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  197: _ZZN3tvm3tir11StmtFunctorI
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  194: _ZZN3tvm3tir11StmtFunctorI
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  191: _ZZN3tvm3tir11StmtFunctorI
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  188: _ZZN3tvm3tir11StmtFunctorI
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  185: _ZZN3tvm3tir11StmtFunctorIFNS
  184: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  183: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  182: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  181: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  180: _ZZN3tvm3tir11StmtFunctorI
  179: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  178: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorI
  176: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  175: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorI
  173: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  172: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorI
  170: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  169: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorI
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  165: _ZZN3tvm3tir11StmtFunctorI
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  162: _ZZN3tvm3tir11StmtFunctorI
  161: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  160: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  159: _ZZN3tvm3tir11StmtFunctorI
  158: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  157: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  156: _ZZN3tvm3tir11StmtFunctorI
  155: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  154: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  153: _ZZN3tvm3tir11StmtFunctorIFNS
  152: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  151: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  150: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  149: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  148: _ZZN3tvm3tir11StmtFunctorI
  147: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  146: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  145: _ZZN3tvm3tir11StmtFunctorI
  144: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  143: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorI
  141: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  140: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorI
  138: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  137: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorI
  135: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  134: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorI
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  130: _ZZN3tvm3tir11StmtFunctorI
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  127: _ZZN3tvm3tir11StmtFunctorI
  126: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  125: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  124: _ZZN3tvm3tir11StmtFunctorI
  123: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  122: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  121: _ZZN3tvm3tir11StmtFunctorI
  120: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  119: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  118: _ZZN3tvm3tir11StmtFunctorIFNS
  117: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  116: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  115: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  114: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  113: _ZZN3tvm3tir11StmtFunctorI
  112: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  111: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorI
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  107: _ZZN3tvm3tir11StmtFunctorI
  106: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  105: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorI
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  101: _ZZN3tvm3tir11StmtFunctorI
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  98: _ZZN3tvm3tir11StmtFunctorI
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  95: _ZZN3tvm3tir11StmtFunctorI
  94: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  93: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorI
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  89: _ZZN3tvm3tir11StmtFunctorI
  88: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  87: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorI
  85: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  84: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS
  82: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  81: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  78: _ZZN3tvm3tir11StmtFunctorI
  77: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  76: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorI
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  72: _ZZN3tvm3tir11StmtFunctorI
  71: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  70: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorI
  68: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  67: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorI
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  63: _ZZN3tvm3tir11StmtFunctorI
  62: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  61: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorI
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  57: _ZZN3tvm3tir11StmtFunctorI
  56: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  55: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorI
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  51: _ZZN3tvm3tir11StmtFunctorI
  50: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  49: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  48: _ZZN3tvm3tir11StmtFunctorIFNS
  47: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  46: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  45: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  44: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorI
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  40: _ZZN3tvm3tir11StmtFunctorI
  39: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  38: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorI
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  34: _ZZN3tvm3tir11StmtFunctorI
  33: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  32: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorI
  30: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  29: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorI
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  25: _ZZN3tvm3tir11StmtFunctorI
  24: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  23: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorI
  21: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  20: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  19: _ZZN3tvm3tir11StmtFunctorIFNS
  18: tvm::runtime::Array<tvm::tir::Stmt, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type> tvm::tir::StmtMutator::Internal::MutateArray<tvm::tir::Stmt, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>(tvm::tir::StmtMutator*, std::enable_if<std::is_base_of<tvm::runtime::ObjectRef, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}>::value, void>::type const&, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  17: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::Object, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  16: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  15: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorI
  13: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  12: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  11: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  10: _ZZN3tvm3tir11StmtFunctorI
  9: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  8: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  7: tvm::tir::StmtFunctor<tvm::tir::Stmt (tvm::tir::Stmt const&)>::VisitStmt(tvm::tir::Stmt const&)
  6: _ZZN3tvm3tir11StmtFunctorIFNS
  5: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  4: _ZZN3tvm3tir11ExprFunctorI
  3: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  2: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  1: tvm::tir::APIType(tvm::runtime::DataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(98))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[32, 2, 32, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109 = sch.split(loop=l107, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b88)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b119)
l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #224: GFLOPs: 1774.2852. Time: 58.0862 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #225: GFLOPs: 760.5491. Time: 135.5093 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #226: GFLOPs: 2203.2623. Time: 46.7768 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #227: GFLOPs: 3158.4101. Time: 32.6308 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #228: GFLOPs: 3154.5141. Time: 32.6711 us. Best GFLOPs: 3526.6062
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #229: GFLOPs: 3966.1703. Time: 25.9851 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #230: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(16))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[256, 4, 1, 2, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 49], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 49, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #231: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(7))
                                        v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(28))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(4) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[32, 1, 64, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[128, 2, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115 = sch.get_loops(block=b88)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b120)
l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b121)
l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l138, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l138, ann_key="pragma_unroll_explicit", ann_val=1)
l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
b163 = sch.get_block(name="conv2d_nchw", func_name="main")
l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b163)
b182 = sch.decompose_reduction(block=b163, loop=l167)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #232: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[64, 4, 2, 1, 4])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 98, 4], preserve_unit_iters=True)
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 98, 4], preserve_unit_iters=True)
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b123)
l136, l137, l138, l139, l140, l141, l142, l143, l144 = sch.get_loops(block=b124)
l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l145, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l145, ann_key="pragma_unroll_explicit", ann_val=1)
l165, l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b126)
b172 = sch.get_block(name="conv2d_nchw", func_name="main")
l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192 = sch.get_loops(block=b172)
b193 = sch.decompose_reduction(block=b172, loop=l176)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #233: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(56))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + rc_1 * T.int64(8) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[16, 2, 64, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109 = sch.split(loop=l107, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b88)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b119)
l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #234: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(49), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(49) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(32) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[256, 1, 1, 2, 4])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[16, 8, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 49], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 49, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #235: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(196) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(64))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[256, 4, 2, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 4, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 98, 4], preserve_unit_iters=True)
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 98, 2], preserve_unit_iters=True)
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b123)
l136, l137, l138, l139, l140, l141, l142, l143, l144 = sch.get_loops(block=b124)
l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l145, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l145, ann_key="pragma_unroll_explicit", ann_val=1)
l165, l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b126)
b172 = sch.get_block(name="conv2d_nchw", func_name="main")
l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192 = sch.get_loops(block=b172)
b193 = sch.decompose_reduction(block=b172, loop=l176)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #236: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(392) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(98), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(98) + ax0_ax1_ax2_ax3_fused_1 < T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(8) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(49) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(49) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[256, 4, 2, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 4, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 98, 4], preserve_unit_iters=True)
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120 = sch.split(loop=l118, factors=[None, 98], preserve_unit_iters=True)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #237: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(7), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1 < T.int64(56))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[64, 1, 32, 1, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109 = sch.split(loop=l107, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b88)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b119)
l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #238: GFLOPs: 1141.9534. Time: 90.2502 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #239: GFLOPs: 2257.5145. Time: 45.6526 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #240: GFLOPs: 2904.1232. Time: 35.4880 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #241: GFLOPs: 3166.8027. Time: 32.5443 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #242: GFLOPs: 2307.9726. Time: 44.6546 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #243: GFLOPs: 1559.5271. Time: 66.0851 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #244: GFLOPs: 2258.3261. Time: 45.6362 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #245: GFLOPs: 2256.8849. Time: 45.6654 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #246: GFLOPs: 2140.2402. Time: 48.1542 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #247: GFLOPs: 2349.2965. Time: 43.8691 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #248: GFLOPs: 3646.3616. Time: 28.2642 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #249: GFLOPs: 2406.7465. Time: 42.8219 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #250: GFLOPs: 2537.7812. Time: 40.6109 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #251: GFLOPs: 2725.0687. Time: 37.8198 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #252: GFLOPs: 3371.3344. Time: 30.5699 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #253: GFLOPs: 3654.3654. Time: 28.2023 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #254: GFLOPs: 537.3181. Time: 191.8072 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #255: GFLOPs: 359.5595. Time: 286.6327 us. Best GFLOPs: 3966.1703
2024-04-29 19:41:40 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #256: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(14), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(32) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused)
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(49))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused)
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(32) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(1024) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 32, 8, 4])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109 = sch.split(loop=l107, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114 = sch.get_loops(block=b88)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b119)
l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b120)
l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
b161 = sch.get_block(name="conv2d_nchw", func_name="main")
l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b161)
b180 = sch.decompose_reduction(block=b161, loop=l165)
2024-04-29 20:23:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 20:24:00 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 20:24:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:24:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 778 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:24:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1171 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:24:05 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2024-04-29 20:24:10 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 149 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:24:14 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:24:19 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 185 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:24:24 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 167 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:24:26 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5565  1.5528  1.5237  1.3709  1.3637  1.3442  1.3424  1.3388  1.3384  1.3329  1.3293  1.3253  1.3250  1.3175  1.3134  1.3125
[17 : 32]:	1.3104  1.3063  1.3028  1.2989  1.2976  1.2972  1.2954  1.2885  1.2847  1.2817  1.2794  1.2289  1.2272  1.2220  1.2091  1.1999
[33 : 48]:	1.1834  1.1744  1.1664  1.1659  1.1609  1.1609  1.1452  1.1296  1.1230  1.1164  1.1023  1.0978  1.0958  1.0937  1.0893  1.0720
[49 : 64]:	1.0603  1.0545  1.0328  1.0277  1.0157  0.9981  0.9955  0.9904  0.9848  0.9700  0.9693  0.9691  0.9553  0.9480  0.9451  0.9451
2024-04-29 20:24:26 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 20:24:26 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #257: GFLOPs: 3397.2169. Time: 30.3370 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #258: GFLOPs: 3839.1548. Time: 26.8448 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #259: GFLOPs: 3035.2522. Time: 33.9548 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #260: GFLOPs: 1997.2137. Time: 51.6026 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #261: GFLOPs: 3176.2159. Time: 32.4479 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #262: GFLOPs: 2702.8593. Time: 38.1305 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #263: GFLOPs: 3451.6640. Time: 29.8585 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #264: GFLOPs: 3280.1980. Time: 31.4193 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #265: GFLOPs: 3313.5988. Time: 31.1026 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #266: GFLOPs: 3451.9105. Time: 29.8564 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #267: GFLOPs: 3768.8992. Time: 27.3453 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #268: GFLOPs: 3614.4233. Time: 28.5140 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #269: GFLOPs: 3041.8087. Time: 33.8817 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #270: GFLOPs: 3313.3547. Time: 31.1049 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #271: GFLOPs: 2692.6161. Time: 38.2756 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #272: GFLOPs: 2798.0012. Time: 36.8340 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #273: GFLOPs: 3313.4896. Time: 31.1036 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #274: GFLOPs: 3741.3699. Time: 27.5465 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #275: GFLOPs: 2800.2651. Time: 36.8042 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #276: GFLOPs: 3127.9681. Time: 32.9484 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #277: GFLOPs: 3660.6997. Time: 28.1535 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #278: GFLOPs: 3622.8660. Time: 28.4475 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #279: GFLOPs: 1953.2924. Time: 52.7630 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #280: GFLOPs: 2067.2640. Time: 49.8541 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #281: GFLOPs: 1564.3765. Time: 65.8802 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #282: GFLOPs: 3231.0986. Time: 31.8967 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #283: GFLOPs: 3231.5164. Time: 31.8926 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #284: GFLOPs: 1964.7857. Time: 52.4543 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #285: GFLOPs: 1374.7429. Time: 74.9678 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #286: GFLOPs: 2054.7078. Time: 50.1587 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #287: GFLOPs: 1752.2055. Time: 58.8182 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #288: GFLOPs: 1999.2933. Time: 51.5490 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #289: GFLOPs: 3081.9068. Time: 33.4408 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #290: GFLOPs: 3106.9399. Time: 33.1714 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #291: GFLOPs: 3132.0692. Time: 32.9052 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #292: GFLOPs: 3177.7221. Time: 32.4325 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #293: GFLOPs: 3083.3082. Time: 33.4256 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #294: GFLOPs: 3106.0322. Time: 33.1811 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #295: GFLOPs: 2552.4431. Time: 40.3776 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #296: GFLOPs: 1892.6494. Time: 54.4536 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #297: GFLOPs: 3084.1931. Time: 33.4160 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #298: GFLOPs: 3034.7028. Time: 33.9610 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #299: GFLOPs: 2057.6109. Time: 50.0879 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #300: GFLOPs: 2848.2616. Time: 36.1840 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #301: GFLOPs: 2168.4390. Time: 47.5280 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #302: GFLOPs: 2014.0643. Time: 51.1709 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #303: GFLOPs: 1953.5086. Time: 52.7571 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #304: GFLOPs: 2557.7646. Time: 40.2936 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #305: GFLOPs: 2493.4967. Time: 41.3321 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #306: GFLOPs: 1820.0645. Time: 56.6252 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #307: GFLOPs: 334.8308. Time: 307.8018 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #308: GFLOPs: 3234.2212. Time: 31.8659 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #309: GFLOPs: 2721.5479. Time: 37.8687 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #310: GFLOPs: 2775.2323. Time: 37.1362 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #311: GFLOPs: 1893.6261. Time: 54.4255 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #312: GFLOPs: 1953.6656. Time: 52.7529 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #313: GFLOPs: 2774.9404. Time: 37.1401 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #314: GFLOPs: 3149.5479. Time: 32.7226 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #315: GFLOPs: 2893.5139. Time: 35.6181 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #316: GFLOPs: 2239.3882. Time: 46.0222 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #317: GFLOPs: 2779.7089. Time: 37.0764 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #318: GFLOPs: 537.8997. Time: 191.5998 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #319: GFLOPs: 13.0216. Time: 7914.6535 us. Best GFLOPs: 3966.1703
2024-04-29 20:25:45 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #320: GFLOPs: 18.4073. Time: 5598.9477 us. Best GFLOPs: 3966.1703
2024-04-29 20:54:31 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 20:54:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 20:54:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:54:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 797 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:54:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:54:37 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2024-04-29 20:54:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 177 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:54:46 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 171 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:54:50 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 149 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:54:55 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 175 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 20:54:56 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0435  1.0008  0.9941  0.9834  0.9741  0.9734  0.9678  0.9678  0.9658  0.9629  0.9628  0.9622  0.9617  0.9608  0.9602  0.9602
[17 : 32]:	0.9597  0.9588  0.9579  0.9565  0.9563  0.9556  0.9552  0.9547  0.9512  0.9477  0.9472  0.9466  0.9420  0.9410  0.9398  0.9370
[33 : 48]:	0.9352  0.9352  0.9351  0.9313  0.9294  0.9293  0.9264  0.9263  0.9192  0.9189  0.9183  0.9177  0.9177  0.9177  0.9177  0.9169
[49 : 64]:	0.9169  0.9155  0.9145  0.9120  0.9096  0.9090  0.9085  0.9076  0.9055  0.9053  0.9036  0.9029  0.8992  0.8978  0.8967  0.8948
2024-04-29 20:54:57 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 20:54:57 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #321: GFLOPs: 892.2480. Time: 115.5077 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #322: GFLOPs: 3598.2748. Time: 28.6419 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #323: GFLOPs: 2327.3143. Time: 44.2834 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #324: GFLOPs: 2999.1669. Time: 34.3634 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #325: GFLOPs: 939.5200. Time: 109.6959 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #326: GFLOPs: 2926.9112. Time: 35.2117 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #327: GFLOPs: 2646.0524. Time: 38.9492 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #328: GFLOPs: 3588.3882. Time: 28.7208 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #329: GFLOPs: 3721.0284. Time: 27.6970 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #330: GFLOPs: 3452.4618. Time: 29.8516 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #331: GFLOPs: 3488.6834. Time: 29.5417 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #332: GFLOPs: 3496.1597. Time: 29.4785 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #333: GFLOPs: 3378.1560. Time: 30.5082 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #334: GFLOPs: 2961.3840. Time: 34.8018 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #335: GFLOPs: 3488.4610. Time: 29.5435 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #336: GFLOPs: 3504.1780. Time: 29.4110 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #337: GFLOPs: 2188.8034. Time: 47.0858 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #338: GFLOPs: 3687.7004. Time: 27.9474 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #339: GFLOPs: 2334.3444. Time: 44.1501 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #340: GFLOPs: 3818.8649. Time: 26.9875 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #341: GFLOPs: 3502.1651. Time: 29.4279 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #342: GFLOPs: 3687.9731. Time: 27.9453 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #343: GFLOPs: 3651.8245. Time: 28.2219 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #344: GFLOPs: 3471.5505. Time: 29.6875 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #345: GFLOPs: 2741.4924. Time: 37.5932 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #346: GFLOPs: 2180.8486. Time: 47.2575 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #347: GFLOPs: 3501.8171. Time: 29.4309 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #348: GFLOPs: 3688.2493. Time: 27.9432 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #349: GFLOPs: 3748.5158. Time: 27.4939 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #350: GFLOPs: 3735.5346. Time: 27.5895 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #351: GFLOPs: 3775.6975. Time: 27.2960 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #352: GFLOPs: 2335.6458. Time: 44.1255 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #353: GFLOPs: 2188.9257. Time: 47.0831 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #354: GFLOPs: 3735.7671. Time: 27.5878 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #355: GFLOPs: 3007.2430. Time: 34.2711 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #356: GFLOPs: 1989.8664. Time: 51.7932 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #357: GFLOPs: 3437.1776. Time: 29.9843 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #358: GFLOPs: 2251.8690. Time: 45.7671 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #359: GFLOPs: 3775.9377. Time: 27.2943 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #360: GFLOPs: 2770.0050. Time: 37.2063 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #361: GFLOPs: 2983.3293. Time: 34.5458 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #362: GFLOPs: 2278.7427. Time: 45.2274 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #363: GFLOPs: 3775.3289. Time: 27.2987 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #364: GFLOPs: 2709.2375. Time: 38.0408 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #365: GFLOPs: 2810.1057. Time: 36.6753 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #366: GFLOPs: 2709.0371. Time: 38.0436 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #367: GFLOPs: 2188.9184. Time: 47.0833 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #368: GFLOPs: 2168.9075. Time: 47.5177 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #369: GFLOPs: 2250.3924. Time: 45.7971 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #370: GFLOPs: 3453.0825. Time: 29.8462 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #371: GFLOPs: 3651.8798. Time: 28.2215 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #372: GFLOPs: 2831.5084. Time: 36.3981 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #373: GFLOPs: 3784.7887. Time: 27.2305 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #374: GFLOPs: 2168.4604. Time: 47.5275 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #375: GFLOPs: 2935.0988. Time: 35.1135 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #376: GFLOPs: 3953.8968. Time: 26.0658 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #377: GFLOPs: 1702.7832. Time: 60.5253 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #378: GFLOPs: 3503.8537. Time: 29.4138 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #379: GFLOPs: 3720.6618. Time: 27.6998 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #380: GFLOPs: 3495.7994. Time: 29.4815 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #381: GFLOPs: 2762.5474. Time: 37.3067 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #382: GFLOPs: 661.3751. Time: 155.8291 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #383: GFLOPs: 359.0523. Time: 287.0376 us. Best GFLOPs: 3966.1703
2024-04-29 20:56:16 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #384: GFLOPs: 169.4226. Time: 608.3101 us. Best GFLOPs: 3966.1703
2024-04-29 21:33:32 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 21:33:33 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 21:33:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 21:33:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 784 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 21:33:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1177 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 21:33:38 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2024-04-29 21:33:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 21:33:47 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 188 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 21:33:51 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 21:33:56 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 239 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 21:33:58 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0099  0.9948  0.9913  0.9682  0.9674  0.9650  0.9644  0.9625  0.9536  0.9506  0.9506  0.9498  0.9481  0.9475  0.9475  0.9464
[17 : 32]:	0.9452  0.9444  0.9429  0.9419  0.9418  0.9416  0.9414  0.9410  0.9408  0.9388  0.9381  0.9368  0.9360  0.9349  0.9341  0.9338
[33 : 48]:	0.9337  0.9327  0.9303  0.9298  0.9298  0.9294  0.9293  0.9285  0.9285  0.9274  0.9265  0.9261  0.9253  0.9253  0.9233  0.9231
[49 : 64]:	0.9223  0.9220  0.9215  0.9208  0.9200  0.9198  0.9193  0.9189  0.9188  0.9185  0.9181  0.9181  0.9171  0.9162  0.9157  0.9156
2024-04-29 21:33:58 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 21:33:58 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #385: GFLOPs: 2777.7752. Time: 37.1022 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #386: GFLOPs: 3939.1660. Time: 26.1633 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #387: GFLOPs: 2720.0896. Time: 37.8890 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #388: GFLOPs: 3172.0976. Time: 32.4900 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #389: GFLOPs: 2416.3895. Time: 42.6510 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #390: GFLOPs: 3110.7810. Time: 33.1304 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #391: GFLOPs: 3845.2113. Time: 26.8026 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #392: GFLOPs: 3933.1437. Time: 26.2033 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #393: GFLOPs: 3837.8069. Time: 26.8543 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #394: GFLOPs: 2720.8579. Time: 37.8783 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #395: GFLOPs: 3896.2164. Time: 26.4517 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #396: GFLOPs: 3756.8705. Time: 27.4328 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #397: GFLOPs: 3192.0536. Time: 32.2869 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #398: GFLOPs: 3837.5205. Time: 26.8563 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #399: GFLOPs: 2825.4882. Time: 36.4756 us. Best GFLOPs: 3966.1703
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #400: GFLOPs: 4173.7615. Time: 24.6927 us. Best GFLOPs: 4173.7615
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #401: GFLOPs: 4049.8271. Time: 25.4484 us. Best GFLOPs: 4173.7615
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #402: GFLOPs: 4060.4837. Time: 25.3816 us. Best GFLOPs: 4173.7615
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #403: GFLOPs: 4249.1325. Time: 24.2547 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #404: GFLOPs: 3499.1590. Time: 29.4532 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #405: GFLOPs: 3855.7313. Time: 26.7294 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #406: GFLOPs: 3587.4836. Time: 28.7281 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #407: GFLOPs: 3588.2436. Time: 28.7220 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #408: GFLOPs: 3852.2137. Time: 26.7538 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #409: GFLOPs: 3657.7914. Time: 28.1759 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #410: GFLOPs: 3191.3560. Time: 32.2940 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #411: GFLOPs: 3782.3687. Time: 27.2479 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #412: GFLOPs: 3782.4294. Time: 27.2474 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #413: GFLOPs: 3782.5537. Time: 27.2465 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #414: GFLOPs: 2746.5484. Time: 37.5240 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #415: GFLOPs: 3574.3341. Time: 28.8338 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #416: GFLOPs: 3847.3996. Time: 26.7873 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #417: GFLOPs: 3653.7914. Time: 28.2067 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #418: GFLOPs: 3815.9619. Time: 27.0080 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #419: GFLOPs: 2535.6656. Time: 40.6448 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #420: GFLOPs: 2415.3414. Time: 42.6695 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #421: GFLOPs: 3723.5765. Time: 27.6781 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #422: GFLOPs: 3810.5630. Time: 27.0463 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #423: GFLOPs: 3843.1507. Time: 26.8169 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #424: GFLOPs: 3491.6429. Time: 29.5166 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #425: GFLOPs: 3491.6332. Time: 29.5167 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #426: GFLOPs: 2225.0474. Time: 46.3188 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #427: GFLOPs: 3928.0500. Time: 26.2373 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #428: GFLOPs: 3615.4480. Time: 28.5059 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #429: GFLOPs: 3509.7448. Time: 29.3644 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #430: GFLOPs: 3509.8380. Time: 29.3636 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #431: GFLOPs: 3841.3100. Time: 26.8298 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #432: GFLOPs: 3782.7216. Time: 27.2453 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #433: GFLOPs: 3657.1356. Time: 28.1809 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #434: GFLOPs: 3654.0689. Time: 28.2046 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #435: GFLOPs: 3657.1764. Time: 28.1806 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #436: GFLOPs: 3852.0864. Time: 26.7547 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #437: GFLOPs: 2420.3304. Time: 42.5816 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #438: GFLOPs: 3623.5700. Time: 28.4420 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #439: GFLOPs: 3782.7967. Time: 27.2448 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #440: GFLOPs: 2420.6135. Time: 42.5766 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #441: GFLOPs: 3657.6942. Time: 28.1766 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #442: GFLOPs: 2787.6190. Time: 36.9712 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #443: GFLOPs: 3809.2509. Time: 27.0556 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #444: GFLOPs: 2651.2127. Time: 38.8733 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #445: GFLOPs: 3723.8460. Time: 27.6761 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #446: GFLOPs: 773.4294. Time: 133.2526 us. Best GFLOPs: 4249.1325
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #447: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  2: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.0]
  1: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init * T.int64(7) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(256), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(49))
                                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(49) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(7))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 < T.int64(98))
                                    T.reads(p0[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(7), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 * T.int64(7) + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0 * T.int64(2) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_0 + rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(7), T.int64(7)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(1024) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(512) + nn_2_ff_2_yy_2_xx_2_fused * T.int64(8) + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 64, 8, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2024-04-29 21:35:17 [INFO] [task_scheduler.cc:121] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #448: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/mnt/home/gverma/ceph/opt/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  5: _ZN3tvm7runtime13PackedFun
  4: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  3: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  2: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  0: _ZN3tvm7runtime6deta
  File "/mnt/home/gverma/ceph/opt/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), "float32"), p2: T.Buffer((T.int64(1), T.int64(2048), T.int64(1), T.int64(1)), "float32"), p3: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        conv2d_nchw_local = T.alloc_buffer((T.int64(1), T.int64(2048), T.int64(7), T.int64(7)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(2048), T.int64(512), T.int64(1), T.int64(1)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(7), yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0_ry_0_rx_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(49))
                                        v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(49) // T.int64(7))
                                        v3 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(7))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(392))
                                        T.reads(p0[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = p0[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        v2 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v3 = T.axis.spatial(T.int64(1), T.int64(0))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(1024))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(7), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(7), yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(512), rc_0_ry_0_rx_0_fused * T.int64(8) + rc_1 * T.int64(4) + rc_2)
                                v_ry = T.axis.reduce(T.int64(1), ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(1), rx_1 + rx_2)
                                T.reads(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(7), T.int64(1)):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(2048), nn_0_ff_0_yy_0_xx_0_fused * T.int64(128) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(7), ax2)
                            v3 = T.axis.spatial(T.int64(7), nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], p2[v0, v1, T.int64(0), T.int64(0)], p3[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + p2[v0, v1, T.int64(0), T.int64(0)] + p3[v0, v1, v2, v3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17], preserve_unit_iters=True)
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[16, 1, 32, 4, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27], preserve_unit_iters=True)
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37], preserve_unit_iters=True)
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47], preserve_unit_iters=True)
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55], preserve_unit_iters=True)
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True, index=-1)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85, preserve_unit_iters=True)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True, index=-1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98, preserve_unit_iters=True)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
l101 = sch.fuse(l56, l62, l68, preserve_unit_iters=True)
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l101, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107 = sch.get_loops(block=b75)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115 = sch.get_loops(block=b88)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b120)
l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b121)
l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l138, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l138, ann_key="pragma_unroll_explicit", ann_val=1)
l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
b163 = sch.get_block(name="conv2d_nchw", func_name="main")
l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b163)
b182 = sch.decompose_reduction(block=b163, loop=l167)
2024-04-29 22:35:46 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-29 22:35:47 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2024-04-29 22:35:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 387 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 22:35:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 780 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 22:35:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 1172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 22:35:53 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2024-04-29 22:35:57 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 264 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 22:36:03 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 226 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 22:36:08 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 237 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 22:36:13 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5407498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x7d972b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x93ebe98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x7f21488)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x7a93ab8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0xd0ba318)]: 213 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x7d98548)]: 0 failure(s)
2024-04-29 22:36:15 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9533  0.9385  0.9339  0.9312  0.9274  0.9261  0.9252  0.9239  0.9234  0.9193  0.9190  0.9174  0.9170  0.9166  0.9163  0.9162
[17 : 32]:	0.9162  0.9160  0.9159  0.9147  0.9142  0.9142  0.9139  0.9134  0.9128  0.9128  0.9120  0.9117  0.9109  0.9105  0.9104  0.9102
[33 : 48]:	0.9097  0.9085  0.9056  0.9016  0.9013  0.9013  0.9013  0.9011  0.8993  0.8963  0.8963  0.8963  0.8963  0.8959  0.8945  0.8942
[49 : 64]:	0.8937  0.8930  0.8917  0.8914  0.8884  0.8872  0.8865  0.8787  0.8785  0.8778  0.8764  0.8763  0.8757  0.8756  0.8755  0.8754
2024-04-29 22:36:15 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-29 22:36:15 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #449: GFLOPs: 3930.5595. Time: 26.2206 us. Best GFLOPs: 4249.1325
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #450: GFLOPs: 4204.5074. Time: 24.5121 us. Best GFLOPs: 4249.1325
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #451: GFLOPs: 3783.7915. Time: 27.2376 us. Best GFLOPs: 4249.1325
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #452: GFLOPs: 3816.1447. Time: 27.0067 us. Best GFLOPs: 4249.1325
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #453: GFLOPs: 4146.5454. Time: 24.8548 us. Best GFLOPs: 4249.1325
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #454: GFLOPs: 3849.1048. Time: 26.7754 us. Best GFLOPs: 4249.1325
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #455: GFLOPs: 4266.7829. Time: 24.1544 us. Best GFLOPs: 4266.7829
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #456: GFLOPs: 3848.6465. Time: 26.7786 us. Best GFLOPs: 4266.7829
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #457: GFLOPs: 4149.5342. Time: 24.8369 us. Best GFLOPs: 4266.7829
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #458: GFLOPs: 4272.1540. Time: 24.1240 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #459: GFLOPs: 4150.4618. Time: 24.8313 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #460: GFLOPs: 4132.5869. Time: 24.9387 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #461: GFLOPs: 4045.7290. Time: 25.4741 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #462: GFLOPs: 4089.3458. Time: 25.2024 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #463: GFLOPs: 4043.9540. Time: 25.4853 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #464: GFLOPs: 3817.9873. Time: 26.9937 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #465: GFLOPs: 4012.0987. Time: 25.6877 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #466: GFLOPs: 4212.8000. Time: 24.4639 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #467: GFLOPs: 4022.2134. Time: 25.6231 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #468: GFLOPs: 3813.7423. Time: 27.0237 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #469: GFLOPs: 3759.1782. Time: 27.4160 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #470: GFLOPs: 3750.4481. Time: 27.4798 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #471: GFLOPs: 3703.0622. Time: 27.8314 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #472: GFLOPs: 3797.0266. Time: 27.1427 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #473: GFLOPs: 3794.3646. Time: 27.1617 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #474: GFLOPs: 3794.6373. Time: 27.1598 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #475: GFLOPs: 3888.1765. Time: 26.5064 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #476: GFLOPs: 3749.9035. Time: 27.4838 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #477: GFLOPs: 3758.4647. Time: 27.4212 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #478: GFLOPs: 3869.3470. Time: 26.6354 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #479: GFLOPs: 3809.6045. Time: 27.0531 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #480: GFLOPs: 3954.0977. Time: 26.0645 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #481: GFLOPs: 3887.7622. Time: 26.5092 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #482: GFLOPs: 4064.1636. Time: 25.3586 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #483: GFLOPs: 4249.1259. Time: 24.2548 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #484: GFLOPs: 3813.4545. Time: 27.0258 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #485: GFLOPs: 4038.1713. Time: 25.5218 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #486: GFLOPs: 4078.5295. Time: 25.2693 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #487: GFLOPs: 4038.3576. Time: 25.5206 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #488: GFLOPs: 4023.2119. Time: 25.6167 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #489: GFLOPs: 4037.7447. Time: 25.5245 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #490: GFLOPs: 3823.5426. Time: 26.9545 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #491: GFLOPs: 3880.6408. Time: 26.5579 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #492: GFLOPs: 3912.1132. Time: 26.3442 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #493: GFLOPs: 3893.3387. Time: 26.4712 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #494: GFLOPs: 3783.3010. Time: 27.2412 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #495: GFLOPs: 4132.4783. Time: 24.9394 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #496: GFLOPs: 3825.8328. Time: 26.9383 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #497: GFLOPs: 3104.5653. Time: 33.1968 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #498: GFLOPs: 3006.1063. Time: 34.2841 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #499: GFLOPs: 3762.6232. Time: 27.3909 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #500: GFLOPs: 3535.3047. Time: 29.1521 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #501: GFLOPs: 3669.2227. Time: 28.0881 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #502: GFLOPs: 4011.1924. Time: 25.6935 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #503: GFLOPs: 3585.7478. Time: 28.7420 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #504: GFLOPs: 3688.2923. Time: 27.9429 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #505: GFLOPs: 3669.1909. Time: 28.0883 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #506: GFLOPs: 4090.0208. Time: 25.1983 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #507: GFLOPs: 4182.2084. Time: 24.6428 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #508: GFLOPs: 3621.7992. Time: 28.4559 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #509: GFLOPs: 3759.0089. Time: 27.4172 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #510: GFLOPs: 255.9635. Time: 402.6414 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #511: GFLOPs: 913.4490. Time: 112.8268 us. Best GFLOPs: 4272.1540
2024-04-29 22:37:35 [INFO] [task_scheduler.cc:131] [Task #24: fused_nn_conv2d_add_add_nn_relu_3] Trial #512: GFLOPs: 42.6660. Time: 2415.5428 us. Best GFLOPs: 4272.1540
